{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any')\n",
    "    \n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "    \n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 74, 919, 4, 4, 39, 228, 20, 33, 748]\n",
      "라벨:  0\n",
      "1번째 리뷰 문장 길이:  10\n",
      "2번째 리뷰 문장 길이:  17\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])  # 1번째 리뷰데이터\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨\n",
    "print('1번째 리뷰 문장 길이: ', len(X_train[0]))\n",
    "print('2번째 리뷰 문장 길이: ', len(X_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.969376315021577\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843535456326455\n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_sequences maxlen :  41\n",
      "전체 문장의 93.43%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(round(np.sum(num_tokens < max_tokens) / len(num_tokens)*100, 2) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146182, 41)\n"
     ]
    }
   ],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  41.0\n",
      "문장길이 최대 :  41\n",
      "문장길이 표준편차 :  0.0\n"
     ]
    }
   ],
   "source": [
    "# 문장길이의 평균값, 최대값, 표준편차를 다시 계산해본다.\n",
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126182, 41)\n",
      "(126182,)\n",
      "(20000, 41)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 20000건 분리\n",
    "x_val = X_train[:20000]   \n",
    "y_val = y_train[:20000]\n",
    "\n",
    "# validation set을 제외한 나머지\n",
    "partial_X_train = X_train[20000:]  \n",
    "partial_y_train = y_train[20000:]\n",
    "\n",
    "print(partial_X_train.shape)\n",
    "print(partial_y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(index_to_word)\n",
    "word_vector_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 163,761\n",
      "Trainable params: 163,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "Cnn_model = keras.Sequential()\n",
    "Cnn_model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "Cnn_model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "Cnn_model.add(keras.layers.MaxPooling1D(5))\n",
    "Cnn_model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "Cnn_model.add(keras.layers.GlobalMaxPooling1D())\n",
    "Cnn_model.add(keras.layers.Dense(8, activation='relu'))\n",
    "Cnn_model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "Cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,881\n",
      "Trainable params: 160,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "\n",
    "LSTM_model = keras.Sequential(name=\"LSTM\")\n",
    "LSTM_model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "LSTM_model.add(keras.layers.LSTM(8, dropout=0.7))\n",
    "LSTM_model.add(keras.layers.Dense(8, activation='relu'))\n",
    "LSTM_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GMP\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,145\n",
      "Trainable params: 160,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "GMP_model = keras.Sequential(name=\"GMP\")\n",
    "GMP_model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "GMP_model.add(keras.layers.GlobalMaxPooling1D())\n",
    "GMP_model.add(keras.layers.Dense(8, activation='relu'))\n",
    "GMP_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "GMP_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CNN', 'LSTM', 'GMP']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lst = [CNN_model.name, LSTM_model.name, GMP_model.name]\n",
    "model_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Start fitting CNN ...\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 11s 44ms/step - loss: 0.3994 - accuracy: 0.8106 - val_loss: 0.3228 - val_accuracy: 0.8591\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.2709 - accuracy: 0.8882 - val_loss: 0.3197 - val_accuracy: 0.8637\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.1800 - accuracy: 0.9311 - val_loss: 0.3580 - val_accuracy: 0.8585\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.0971 - accuracy: 0.9660 - val_loss: 0.4739 - val_accuracy: 0.8531\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.0569 - accuracy: 0.9815 - val_loss: 0.5348 - val_accuracy: 0.8461\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.0409 - accuracy: 0.9869 - val_loss: 0.6814 - val_accuracy: 0.8464\n",
      "Epoch 7/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.0335 - accuracy: 0.9888 - val_loss: 0.7235 - val_accuracy: 0.8445\n",
      "Epoch 8/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.0307 - accuracy: 0.9897 - val_loss: 0.7996 - val_accuracy: 0.8447\n",
      "Epoch 9/20\n",
      "247/247 [==============================] - 9s 34ms/step - loss: 0.0324 - accuracy: 0.9889 - val_loss: 0.7779 - val_accuracy: 0.8483\n",
      "Epoch 10/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.0285 - accuracy: 0.9903 - val_loss: 0.8203 - val_accuracy: 0.8484\n",
      "Epoch 11/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.0223 - accuracy: 0.9920 - val_loss: 0.9583 - val_accuracy: 0.8491\n",
      "Epoch 12/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.0185 - accuracy: 0.9932 - val_loss: 0.9868 - val_accuracy: 0.8465\n",
      "Epoch 13/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.0158 - accuracy: 0.9940 - val_loss: 1.0924 - val_accuracy: 0.8507\n",
      "Epoch 14/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.0172 - accuracy: 0.9934 - val_loss: 1.0807 - val_accuracy: 0.8469\n",
      "Epoch 15/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.0174 - accuracy: 0.9934 - val_loss: 1.0278 - val_accuracy: 0.8447\n",
      "Epoch 16/20\n",
      "247/247 [==============================] - 9s 34ms/step - loss: 0.0145 - accuracy: 0.9942 - val_loss: 1.1042 - val_accuracy: 0.8460\n",
      "Epoch 17/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.0129 - accuracy: 0.9948 - val_loss: 1.0893 - val_accuracy: 0.8464\n",
      "Epoch 18/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.0133 - accuracy: 0.9945 - val_loss: 1.1258 - val_accuracy: 0.8457\n",
      "Epoch 19/20\n",
      "247/247 [==============================] - 9s 35ms/step - loss: 0.0103 - accuracy: 0.9956 - val_loss: 1.2692 - val_accuracy: 0.8504\n",
      "Epoch 20/20\n",
      "247/247 [==============================] - 8s 34ms/step - loss: 0.0110 - accuracy: 0.9954 - val_loss: 1.3422 - val_accuracy: 0.8442\n",
      "Start evaluating CNN ...\n",
      "1537/1537 - 3s - loss: 1.3727 - accuracy: 0.8407\n",
      "----------------------------------------\n",
      "Start fitting LSTM ...\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.5506 - accuracy: 0.7309 - val_loss: 0.3902 - val_accuracy: 0.8380\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3911 - accuracy: 0.8298 - val_loss: 0.3521 - val_accuracy: 0.8494\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3633 - accuracy: 0.8440 - val_loss: 0.3456 - val_accuracy: 0.8508\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3528 - accuracy: 0.8479 - val_loss: 0.3444 - val_accuracy: 0.8528\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3434 - accuracy: 0.8541 - val_loss: 0.3442 - val_accuracy: 0.8508\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3381 - accuracy: 0.8559 - val_loss: 0.3421 - val_accuracy: 0.8532\n",
      "Epoch 7/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3336 - accuracy: 0.8580 - val_loss: 0.3426 - val_accuracy: 0.8529\n",
      "Epoch 8/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3310 - accuracy: 0.8589 - val_loss: 0.3434 - val_accuracy: 0.8533\n",
      "Epoch 9/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3270 - accuracy: 0.8611 - val_loss: 0.3433 - val_accuracy: 0.8528\n",
      "Epoch 10/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3256 - accuracy: 0.8613 - val_loss: 0.3419 - val_accuracy: 0.8536\n",
      "Epoch 11/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3215 - accuracy: 0.8630 - val_loss: 0.3598 - val_accuracy: 0.8433\n",
      "Epoch 12/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3210 - accuracy: 0.8634 - val_loss: 0.3419 - val_accuracy: 0.8520\n",
      "Epoch 13/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3177 - accuracy: 0.8654 - val_loss: 0.3419 - val_accuracy: 0.8523\n",
      "Epoch 14/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3169 - accuracy: 0.8644 - val_loss: 0.3431 - val_accuracy: 0.8539\n",
      "Epoch 15/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3154 - accuracy: 0.8659 - val_loss: 0.3391 - val_accuracy: 0.8536\n",
      "Epoch 16/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3132 - accuracy: 0.8665 - val_loss: 0.3405 - val_accuracy: 0.8529\n",
      "Epoch 17/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3105 - accuracy: 0.8687 - val_loss: 0.3438 - val_accuracy: 0.8518\n",
      "Epoch 18/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3098 - accuracy: 0.8674 - val_loss: 0.3400 - val_accuracy: 0.8514\n",
      "Epoch 19/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3080 - accuracy: 0.8681 - val_loss: 0.3380 - val_accuracy: 0.8544\n",
      "Epoch 20/20\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.3056 - accuracy: 0.8695 - val_loss: 0.3369 - val_accuracy: 0.8558\n",
      "Start evaluating LSTM ...\n",
      "1537/1537 - 2s - loss: 0.3437 - accuracy: 0.8505\n",
      "----------------------------------------\n",
      "Start fitting GMP ...\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 1s 3ms/step - loss: 0.6019 - accuracy: 0.7063 - val_loss: 0.4646 - val_accuracy: 0.8159\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.3910 - accuracy: 0.8385 - val_loss: 0.3670 - val_accuracy: 0.8403\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.3304 - accuracy: 0.8613 - val_loss: 0.3542 - val_accuracy: 0.8456\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.3021 - accuracy: 0.8754 - val_loss: 0.3539 - val_accuracy: 0.8464\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.2819 - accuracy: 0.8849 - val_loss: 0.3574 - val_accuracy: 0.8480\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.2655 - accuracy: 0.8928 - val_loss: 0.3647 - val_accuracy: 0.8447\n",
      "Epoch 7/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.2518 - accuracy: 0.8998 - val_loss: 0.3725 - val_accuracy: 0.8453\n",
      "Epoch 8/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.2398 - accuracy: 0.9050 - val_loss: 0.3818 - val_accuracy: 0.8431\n",
      "Epoch 9/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.2289 - accuracy: 0.9104 - val_loss: 0.3923 - val_accuracy: 0.8415\n",
      "Epoch 10/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.2192 - accuracy: 0.9158 - val_loss: 0.4030 - val_accuracy: 0.8410\n",
      "Epoch 11/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.2106 - accuracy: 0.9194 - val_loss: 0.4151 - val_accuracy: 0.8389\n",
      "Epoch 12/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.2024 - accuracy: 0.9233 - val_loss: 0.4270 - val_accuracy: 0.8378\n",
      "Epoch 13/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.1951 - accuracy: 0.9269 - val_loss: 0.4384 - val_accuracy: 0.8367\n",
      "Epoch 14/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.1884 - accuracy: 0.9295 - val_loss: 0.4521 - val_accuracy: 0.8345\n",
      "Epoch 15/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.1825 - accuracy: 0.9328 - val_loss: 0.4656 - val_accuracy: 0.8324\n",
      "Epoch 16/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.1765 - accuracy: 0.9353 - val_loss: 0.4765 - val_accuracy: 0.8315\n",
      "Epoch 17/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.1714 - accuracy: 0.9373 - val_loss: 0.4891 - val_accuracy: 0.8304\n",
      "Epoch 18/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.1666 - accuracy: 0.9400 - val_loss: 0.5004 - val_accuracy: 0.8289\n",
      "Epoch 19/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.1621 - accuracy: 0.9416 - val_loss: 0.5106 - val_accuracy: 0.8288\n",
      "Epoch 20/20\n",
      "247/247 [==============================] - 1s 2ms/step - loss: 0.1580 - accuracy: 0.9436 - val_loss: 0.5215 - val_accuracy: 0.8281\n",
      "Start evaluating GMP ...\n",
      "1537/1537 - 1s - loss: 0.5280 - accuracy: 0.8246\n"
     ]
    }
   ],
   "source": [
    "model_result = {}\n",
    "\n",
    "for model_name in model_lst:\n",
    "    \n",
    "    if model_name == \"CNN\":\n",
    "        model = CNN_model\n",
    "    elif model_name == \"LSTM\":\n",
    "        model = LSTM_model\n",
    "    else :\n",
    "        model = GMP_model\n",
    "    \n",
    "    print('-'*40)\n",
    "    print(\"Start fitting {} ...\".format(model_name))\n",
    "    model.compile(optimizer='Adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    epochs=20\n",
    "\n",
    "    history = model.fit(partial_X_train,\n",
    "                        partial_y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=512,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        verbose=1)\n",
    "    \n",
    "    \n",
    "    print(\"Start evaluating {} ...\".format(model_name))\n",
    "    results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "    model_result[model_name] = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM \t 0.8505197763442993\n",
      "CNN \t 0.8406534194946289\n",
      "GMP \t 0.824623167514801\n"
     ]
    }
   ],
   "source": [
    "for name, [_, acc] in sorted(model_result.items(), key=lambda x : x[1][1], reverse=True) :\n",
    "    print(name,'\\t',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 1000)        10000000  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               578048    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 8)                 1032      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 10,579,089\n",
      "Trainable params: 10,579,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 1000  # 워드 벡터의 차원수\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "# CNN을 추가했을 때\n",
    "# model.add(keras.layers.Conv1D(8, 7, activation='relu'))\n",
    "# model.add(keras.layers.MaxPooling1D())\n",
    "# LSTM 레이어를 두개로 학습했을 때\n",
    "# model.add(keras.layers.LSTM(256, dropout=0.7, return_sequences=True))\n",
    "model.add(keras.layers.LSTM(128, dropout=0.7))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 이진분류\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "model_check = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "986/986 [==============================] - ETA: 0s - loss: 0.3914 - accuracy: 0.8200\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85390, saving model to model.h5\n",
      "986/986 [==============================] - 74s 75ms/step - loss: 0.3914 - accuracy: 0.8200 - val_loss: 0.3303 - val_accuracy: 0.8539\n",
      "Epoch 2/20\n",
      "986/986 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.8667\n",
      "Epoch 00002: val_accuracy improved from 0.85390 to 0.86160, saving model to model.h5\n",
      "986/986 [==============================] - 73s 74ms/step - loss: 0.3094 - accuracy: 0.8667 - val_loss: 0.3187 - val_accuracy: 0.8616\n",
      "Epoch 3/20\n",
      "986/986 [==============================] - ETA: 0s - loss: 0.2742 - accuracy: 0.8831\n",
      "Epoch 00003: val_accuracy improved from 0.86160 to 0.86645, saving model to model.h5\n",
      "986/986 [==============================] - 73s 74ms/step - loss: 0.2742 - accuracy: 0.8831 - val_loss: 0.3108 - val_accuracy: 0.8665\n",
      "Epoch 4/20\n",
      "986/986 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.8943\n",
      "Epoch 00004: val_accuracy improved from 0.86645 to 0.86745, saving model to model.h5\n",
      "986/986 [==============================] - 73s 74ms/step - loss: 0.2490 - accuracy: 0.8943 - val_loss: 0.3158 - val_accuracy: 0.8674\n",
      "Epoch 5/20\n",
      "986/986 [==============================] - ETA: 0s - loss: 0.2279 - accuracy: 0.9057\n",
      "Epoch 00005: val_accuracy did not improve from 0.86745\n",
      "986/986 [==============================] - 73s 74ms/step - loss: 0.2279 - accuracy: 0.9057 - val_loss: 0.3287 - val_accuracy: 0.8668\n",
      "Epoch 6/20\n",
      "986/986 [==============================] - ETA: 0s - loss: 0.2089 - accuracy: 0.9140\n",
      "Epoch 00006: val_accuracy did not improve from 0.86745\n",
      "986/986 [==============================] - 73s 74ms/step - loss: 0.2089 - accuracy: 0.9140 - val_loss: 0.3357 - val_accuracy: 0.8663\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1,\n",
    "                   callbacks=[early_stopping, model_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 3s - loss: 0.3409 - accuracy: 0.8617\n",
      "[0.3408955931663513, 0.8616676926612854]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# 빨간 실선으로 표시\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# 파란 실선으로 표시\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장\n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim)) # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀작성\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록\n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['기쁨']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('슬퍼요', 0.5188561081886292),\n",
       " ('색다르', 0.515166699886322),\n",
       " ('신들린', 0.5050404071807861),\n",
       " ('이영애', 0.5040378570556641),\n",
       " ('완소', 0.502723753452301),\n",
       " ('멋져요', 0.5004808306694031),\n",
       " ('love', 0.495347797870636),\n",
       " ('Good', 0.49451780319213867),\n",
       " ('귓가', 0.49305373430252075),\n",
       " ('웰메이드', 0.49173200130462646)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"기쁨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
