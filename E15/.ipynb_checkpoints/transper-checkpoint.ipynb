{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 178009\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42034</th>\n",
       "      <td>I have a Japanese car.</td>\n",
       "      <td>J'ai une voiture japonaise.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163051</th>\n",
       "      <td>If you don't water the plants, they will wither.</td>\n",
       "      <td>Si tu n'arroses pas les plantes, elles vont se...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82029</th>\n",
       "      <td>Are you done with the paper?</td>\n",
       "      <td>En avez-vous fini avec le papier ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94128</th>\n",
       "      <td>This time you won't be alone.</td>\n",
       "      <td>Cette fois, tu ne seras pas seule.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122609</th>\n",
       "      <td>I'm not the only one with a child.</td>\n",
       "      <td>Je ne suis pas la seule avec un enfant.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     eng  \\\n",
       "42034                             I have a Japanese car.   \n",
       "163051  If you don't water the plants, they will wither.   \n",
       "82029                       Are you done with the paper?   \n",
       "94128                      This time you won't be alone.   \n",
       "122609                I'm not the only one with a child.   \n",
       "\n",
       "                                                      fra  \\\n",
       "42034                         J'ai une voiture japonaise.   \n",
       "163051  Si tu n'arroses pas les plantes, elles vont se...   \n",
       "82029                  En avez-vous fini avec le papier ?   \n",
       "94128                  Cette fois, tu ne seras pas seule.   \n",
       "122609            Je ne suis pas la seule avec un enfant.   \n",
       "\n",
       "                                                       cc  \n",
       "42034   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "163051  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "82029   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "94128   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "122609  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47780</th>\n",
       "      <td>He was sick of his job.</td>\n",
       "      <td>Il en avait assez de son travail.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11653</th>\n",
       "      <td>I'm not perfect.</td>\n",
       "      <td>Je ne suis pas parfait.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>We're here.</td>\n",
       "      <td>Nous sommes là.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>Who talked?</td>\n",
       "      <td>Qui a parlé ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15049</th>\n",
       "      <td>I miss you a lot.</td>\n",
       "      <td>Vous me manquez beaucoup.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           eng                                fra\n",
       "47780  He was sick of his job.  Il en avait assez de son travail.\n",
       "11653         I'm not perfect.            Je ne suis pas parfait.\n",
       "1822               We're here.                    Nous sommes là.\n",
       "1850               Who talked?                      Qui a parlé ?\n",
       "15049        I miss you a lot.          Vous me manquez beaucoup."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:50000] # 5만개 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27315</th>\n",
       "      <td>What are you doing?</td>\n",
       "      <td>\\t Qu'es-tu en train de faire ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29001</th>\n",
       "      <td>He is about my size.</td>\n",
       "      <td>\\t Il fait à peu près ma taille. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16553</th>\n",
       "      <td>The soup is cold.</td>\n",
       "      <td>\\t Le potage est froid. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>I need time.</td>\n",
       "      <td>\\t J'ai besoin de temps. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47871</th>\n",
       "      <td>Here's the tricky part.</td>\n",
       "      <td>\\t Voilà le rôle difficile. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           eng                                  fra\n",
       "27315      What are you doing?   \\t Qu'es-tu en train de faire ? \\n\n",
       "29001     He is about my size.  \\t Il fait à peu près ma taille. \\n\n",
       "16553        The soup is cold.           \\t Le potage est froid. \\n\n",
       "2329              I need time.          \\t J'ai besoin de temps. \\n\n",
       "47871  Here's the tricky part.       \\t Voilà le rôle difficile. \\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 3, 8], [10, 5, 8], [10, 5, 8]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # 50000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 1, 19, 4, 1, 33, 1, 12],\n",
       " [11, 1, 3, 4, 13, 7, 5, 1, 33, 1, 12],\n",
       " [11, 1, 3, 4, 13, 7, 5, 14, 1, 12]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 50000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 51\n",
      "프랑스어 단어장의 크기 : 73\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 23\n",
      "프랑스어 시퀀스의 최대 길이 76\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n",
      "영어 단어장의 크기 : 51\n",
      "프랑스어 단어장의 크기 : 73\n",
      "영어 시퀀스의 최대 길이 23\n",
      "프랑스어 시퀀스의 최대 길이 76\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 1, 19, 4, 1, 33, 1], [11, 1, 3, 4, 13, 7, 5, 1, 33, 1], [11, 1, 3, 4, 13, 7, 5, 14, 1]]\n",
      "[[1, 19, 4, 1, 33, 1, 12], [1, 3, 4, 13, 7, 5, 1, 33, 1, 12], [1, 3, 4, 13, 7, 5, 14, 1, 12]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 23)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  3  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 23, 51)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (50000, 23, 51)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print('⏳')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "encoder_inputs = Input(shape=(None, eng_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "# 디코더로 전달할 hidden state, cell state를 리턴. encoder_outputs은 여기서는 불필요.\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# hidden state와 cell state를 다음 time step으로 전달하기 위해서 별도 저장.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "decoder_inputs = Input(shape=(None, fra_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 51)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 315392      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 672,073\n",
      "Trainable params: 672,073\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.9279 - val_loss: 0.7902\n",
      "Epoch 2/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.5716 - val_loss: 0.6541\n",
      "Epoch 3/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.4767 - val_loss: 0.5688\n",
      "Epoch 4/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.4171 - val_loss: 0.5173\n",
      "Epoch 5/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.3780 - val_loss: 0.4812\n",
      "Epoch 6/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.3502 - val_loss: 0.4555\n",
      "Epoch 7/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.3293 - val_loss: 0.4427\n",
      "Epoch 8/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.3124 - val_loss: 0.4253\n",
      "Epoch 9/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2982 - val_loss: 0.4181\n",
      "Epoch 10/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2864 - val_loss: 0.4030\n",
      "Epoch 11/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2761 - val_loss: 0.3950\n",
      "Epoch 12/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2672 - val_loss: 0.3886\n",
      "Epoch 13/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2593 - val_loss: 0.3831\n",
      "Epoch 14/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2523 - val_loss: 0.3788\n",
      "Epoch 15/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2458 - val_loss: 0.3718\n",
      "Epoch 16/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2399 - val_loss: 0.3838\n",
      "Epoch 17/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2346 - val_loss: 0.3675\n",
      "Epoch 18/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2296 - val_loss: 0.3690\n",
      "Epoch 19/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2248 - val_loss: 0.3688\n",
      "Epoch 20/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2204 - val_loss: 0.3691\n",
      "Epoch 21/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2162 - val_loss: 0.3634\n",
      "Epoch 22/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2123 - val_loss: 0.3691\n",
      "Epoch 23/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2086 - val_loss: 0.3673\n",
      "Epoch 24/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2050 - val_loss: 0.3656\n",
      "Epoch 25/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.2018 - val_loss: 0.3640\n",
      "Epoch 26/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1985 - val_loss: 0.3649\n",
      "Epoch 27/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1955 - val_loss: 0.3800\n",
      "Epoch 28/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1925 - val_loss: 0.3715\n",
      "Epoch 29/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1897 - val_loss: 0.3685\n",
      "Epoch 30/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1869 - val_loss: 0.3697\n",
      "Epoch 31/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1843 - val_loss: 0.3700\n",
      "Epoch 32/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1817 - val_loss: 0.3737\n",
      "Epoch 33/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1793 - val_loss: 0.3737\n",
      "Epoch 34/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1769 - val_loss: 0.3726\n",
      "Epoch 35/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1745 - val_loss: 0.3754\n",
      "Epoch 36/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1724 - val_loss: 0.3783\n",
      "Epoch 37/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1701 - val_loss: 0.3817\n",
      "Epoch 38/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1682 - val_loss: 0.3840\n",
      "Epoch 39/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1661 - val_loss: 0.3841\n",
      "Epoch 40/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1641 - val_loss: 0.3889\n",
      "Epoch 41/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1621 - val_loss: 0.3870\n",
      "Epoch 42/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1603 - val_loss: 0.3924\n",
      "Epoch 43/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1583 - val_loss: 0.3928\n",
      "Epoch 44/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1566 - val_loss: 0.3959\n",
      "Epoch 45/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1549 - val_loss: 0.3959\n",
      "Epoch 46/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1532 - val_loss: 0.3967\n",
      "Epoch 47/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1515 - val_loss: 0.4044\n",
      "Epoch 48/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1499 - val_loss: 0.3996\n",
      "Epoch 49/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1485 - val_loss: 0.4113\n",
      "Epoch 50/50\n",
      "368/368 [==============================] - 5s 14ms/step - loss: 0.1469 - val_loss: 0.4068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f54a03c7e10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 51)]        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 315392    \n",
      "=================================================================\n",
      "Total params: 315,392\n",
      "Trainable params: 315,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 356,681\n",
      "Trainable params: 356,681\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장:  Cours ! \n",
      "번역기가 번역한 문장:  courez ! \n",
      "-----------------------------------\n",
      "입력 문장: I left.\n",
      "정답 문장:  Je suis partie. \n",
      "번역기가 번역한 문장:  j'ai pain. \n",
      "-----------------------------------\n",
      "입력 문장: Call us.\n",
      "정답 문장:  Appelez-nous ! \n",
      "번역기가 번역한 문장:  appelle tom. \n",
      "-----------------------------------\n",
      "입력 문장: How nice!\n",
      "정답 문장:  Comme c'est gentil ! \n",
      "번역기가 번역한 문장:  comme c'est chouette ! \n",
      "-----------------------------------\n",
      "입력 문장: Turn left.\n",
      "정답 문장:  Tourne à gauche. \n",
      "번역기가 번역한 문장:  écoutez ça ! \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
