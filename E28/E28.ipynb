{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from scipy.interpolate import interp1d\n",
    "from inspect import signature\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 32, 32, 1)\n",
      "(10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_data = (train_data - 127.5) / 127.5\n",
    "test_data = (test_data - 127.5) / 127.5\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Fashion MNIST padding to 32 X 32\n",
    "train_data_32 = np.zeros((train_data.shape[0], 32, 32)).astype('float32')\n",
    "test_data_32 = np.zeros((test_data.shape[0], 32, 32)).astype('float32')     \n",
    "train_data_32[:, 2:30, 2:30] = train_data\n",
    "test_data_32[:, 2:30, 2:30] = test_data\n",
    "\n",
    "# 1channel data reshape\n",
    "train_data = train_data_32.reshape(train_data_32.shape[0], 32, 32, 1).astype('float32')\n",
    "test_data = test_data_32.reshape(test_data_32.shape[0], 32, 32, 1).astype('float32')\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_data[i].reshape(32, 32), cmap='gray')\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 8:   # Bag:8\n",
    "            new_t_labels.append([0])  # Bag을 이상치로 처리\n",
    "        else:\n",
    "            new_t_labels.append([1])  # 그 외의 경우는 정상치\n",
    "             \n",
    "    return np.array(new_t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(train_labels)\n",
    "bol_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "for data, label in zip(train_data, bol_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 32, 32, 1) (54000, 1)\n",
      "(6000, 32, 32, 1) (6000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 32, 32, 1)\n",
      "(16000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 1)\n",
      "(16000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in test_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                          kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                                   kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(512) # 1\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512) # 2\n",
    "        self.decoder_3 = Conv_T_block(256) # 4\n",
    "        self.decoder_2 = Conv_T_block(128) # 8\n",
    "        self.decoder_1 = Conv_T_block(64) # 16\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2, padding='same', use_bias=False, # 32\n",
    "                                                   kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "                \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # gen\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(100) # 1\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides=1, padding='same',\n",
    "                                          use_bias=False, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # dis\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_output_channel=1)  # Generator가 32X32X1 짜리 이미지를 생성해야 합니다. \n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, input_data, gen_data, latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + \\\n",
    "                     w_context * context_loss + \\\n",
    "                     w_encoder * encoder_loss\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 설정\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getenv('HOME'),'aiffel/ganomaly_skip_no_norm/ckpt')\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 100, \t Total Gen Loss : 18.723709106445312, \t Total Dis Loss : 0.6497347950935364\n",
      "Steps : 200, \t Total Gen Loss : 19.8875789642334, \t Total Dis Loss : 0.39845162630081177\n",
      "Steps : 300, \t Total Gen Loss : 21.561674118041992, \t Total Dis Loss : 0.2611566185951233\n",
      "Steps : 400, \t Total Gen Loss : 23.28426742553711, \t Total Dis Loss : 0.1994418203830719\n",
      "Steps : 500, \t Total Gen Loss : 21.678173065185547, \t Total Dis Loss : 0.13184580206871033\n",
      "Steps : 600, \t Total Gen Loss : 22.235544204711914, \t Total Dis Loss : 0.17549456655979156\n",
      "Steps : 700, \t Total Gen Loss : 21.76806640625, \t Total Dis Loss : 0.2830055356025696\n",
      "Steps : 800, \t Total Gen Loss : 21.419429779052734, \t Total Dis Loss : 0.24169188737869263\n",
      "Steps : 900, \t Total Gen Loss : 22.980276107788086, \t Total Dis Loss : 0.04420815408229828\n",
      "Steps : 1000, \t Total Gen Loss : 22.995620727539062, \t Total Dis Loss : 0.04437936469912529\n",
      "Steps : 1100, \t Total Gen Loss : 24.805429458618164, \t Total Dis Loss : 0.023579470813274384\n",
      "Steps : 1200, \t Total Gen Loss : 21.63918685913086, \t Total Dis Loss : 0.07877057790756226\n",
      "Steps : 1300, \t Total Gen Loss : 22.8270206451416, \t Total Dis Loss : 0.02035108022391796\n",
      "Steps : 1400, \t Total Gen Loss : 23.992856979370117, \t Total Dis Loss : 0.0229342058300972\n",
      "Steps : 1500, \t Total Gen Loss : 22.573190689086914, \t Total Dis Loss : 0.034100066870450974\n",
      "Steps : 1600, \t Total Gen Loss : 24.73590850830078, \t Total Dis Loss : 0.36882832646369934\n",
      "Steps : 1700, \t Total Gen Loss : 21.513626098632812, \t Total Dis Loss : 0.03951224312186241\n",
      "Steps : 1800, \t Total Gen Loss : 23.72904396057129, \t Total Dis Loss : 0.023027360439300537\n",
      "Steps : 1900, \t Total Gen Loss : 23.425439834594727, \t Total Dis Loss : 0.0196197722107172\n",
      "Steps : 2000, \t Total Gen Loss : 23.86089324951172, \t Total Dis Loss : 0.016088904812932014\n",
      "Steps : 2100, \t Total Gen Loss : 24.269365310668945, \t Total Dis Loss : 0.018937792629003525\n",
      "Steps : 2200, \t Total Gen Loss : 26.50604248046875, \t Total Dis Loss : 0.04422249644994736\n",
      "Steps : 2300, \t Total Gen Loss : 26.015825271606445, \t Total Dis Loss : 0.08519770205020905\n",
      "Steps : 2400, \t Total Gen Loss : 23.82772445678711, \t Total Dis Loss : 0.48530659079551697\n",
      "Steps : 2500, \t Total Gen Loss : 26.653228759765625, \t Total Dis Loss : 0.007893532514572144\n",
      "Steps : 2600, \t Total Gen Loss : 26.412460327148438, \t Total Dis Loss : 0.008387197740375996\n",
      "Steps : 2700, \t Total Gen Loss : 23.463626861572266, \t Total Dis Loss : 0.044297948479652405\n",
      "Steps : 2800, \t Total Gen Loss : 24.337154388427734, \t Total Dis Loss : 0.04740704968571663\n",
      "Steps : 2900, \t Total Gen Loss : 24.570674896240234, \t Total Dis Loss : 0.020875612273812294\n",
      "Steps : 3000, \t Total Gen Loss : 23.548032760620117, \t Total Dis Loss : 0.027122486382722855\n",
      "Steps : 3100, \t Total Gen Loss : 24.670839309692383, \t Total Dis Loss : 0.013612699694931507\n",
      "Steps : 3200, \t Total Gen Loss : 26.408674240112305, \t Total Dis Loss : 0.011016030795872211\n",
      "Steps : 3300, \t Total Gen Loss : 26.741069793701172, \t Total Dis Loss : 0.02302178367972374\n",
      "Steps : 3400, \t Total Gen Loss : 24.4692440032959, \t Total Dis Loss : 0.022273197770118713\n",
      "Steps : 3500, \t Total Gen Loss : 26.068021774291992, \t Total Dis Loss : 0.008132796734571457\n",
      "Steps : 3600, \t Total Gen Loss : 28.238719940185547, \t Total Dis Loss : 0.005020139738917351\n",
      "Steps : 3700, \t Total Gen Loss : 25.116928100585938, \t Total Dis Loss : 0.011299031786620617\n",
      "Steps : 3800, \t Total Gen Loss : 25.320423126220703, \t Total Dis Loss : 0.009256192483007908\n",
      "Steps : 3900, \t Total Gen Loss : 25.21657943725586, \t Total Dis Loss : 0.014753947034478188\n",
      "Steps : 4000, \t Total Gen Loss : 27.76640510559082, \t Total Dis Loss : 0.0037186387926340103\n",
      "Steps : 4100, \t Total Gen Loss : 25.030864715576172, \t Total Dis Loss : 0.012496435083448887\n",
      "Steps : 4200, \t Total Gen Loss : 25.917207717895508, \t Total Dis Loss : 0.004410388879477978\n",
      "Steps : 4300, \t Total Gen Loss : 25.28089714050293, \t Total Dis Loss : 0.009067879989743233\n",
      "Steps : 4400, \t Total Gen Loss : 26.024168014526367, \t Total Dis Loss : 0.004850416909903288\n",
      "Steps : 4500, \t Total Gen Loss : 25.661123275756836, \t Total Dis Loss : 0.0020541728008538485\n",
      "Steps : 4600, \t Total Gen Loss : 25.73509407043457, \t Total Dis Loss : 0.005477071274071932\n",
      "Steps : 4700, \t Total Gen Loss : 26.80308723449707, \t Total Dis Loss : 0.024173520505428314\n",
      "Steps : 4800, \t Total Gen Loss : 25.73406219482422, \t Total Dis Loss : 0.01944250985980034\n",
      "Steps : 4900, \t Total Gen Loss : 25.950031280517578, \t Total Dis Loss : 0.009525341913104057\n",
      "Steps : 5000, \t Total Gen Loss : 22.495708465576172, \t Total Dis Loss : 0.02675708755850792\n",
      "Steps : 5100, \t Total Gen Loss : 26.426115036010742, \t Total Dis Loss : 0.005160333588719368\n",
      "Steps : 5200, \t Total Gen Loss : 22.745441436767578, \t Total Dis Loss : 0.023212460801005363\n",
      "Steps : 5300, \t Total Gen Loss : 25.828340530395508, \t Total Dis Loss : 0.0053632184863090515\n",
      "Steps : 5400, \t Total Gen Loss : 24.670011520385742, \t Total Dis Loss : 0.025540152564644814\n",
      "Steps : 5500, \t Total Gen Loss : 24.76211929321289, \t Total Dis Loss : 0.0022944174706935883\n",
      "Steps : 5600, \t Total Gen Loss : 25.288352966308594, \t Total Dis Loss : 0.010703607462346554\n",
      "Steps : 5700, \t Total Gen Loss : 27.096363067626953, \t Total Dis Loss : 0.0020909772720187902\n",
      "Steps : 5800, \t Total Gen Loss : 25.27816390991211, \t Total Dis Loss : 0.06552082300186157\n",
      "Steps : 5900, \t Total Gen Loss : 25.295047760009766, \t Total Dis Loss : 0.007393830455839634\n",
      "Steps : 6000, \t Total Gen Loss : 25.43181610107422, \t Total Dis Loss : 0.0030047520995140076\n",
      "Steps : 6100, \t Total Gen Loss : 27.03196907043457, \t Total Dis Loss : 0.004128715954720974\n",
      "Steps : 6200, \t Total Gen Loss : 26.372678756713867, \t Total Dis Loss : 0.05883385241031647\n",
      "Steps : 6300, \t Total Gen Loss : 23.87896728515625, \t Total Dis Loss : 0.8500231504440308\n",
      "Steps : 6400, \t Total Gen Loss : 26.920787811279297, \t Total Dis Loss : 0.010313357226550579\n",
      "Steps : 6500, \t Total Gen Loss : 26.93449592590332, \t Total Dis Loss : 0.013671998865902424\n",
      "Steps : 6600, \t Total Gen Loss : 26.79740333557129, \t Total Dis Loss : 0.022052932530641556\n",
      "Steps : 6700, \t Total Gen Loss : 25.270864486694336, \t Total Dis Loss : 0.015267986804246902\n",
      "Time for epoch 1 is 350.10318756103516 sec\n",
      "Steps : 6800, \t Total Gen Loss : 27.829875946044922, \t Total Dis Loss : 0.004337837919592857\n",
      "Steps : 6900, \t Total Gen Loss : 26.116905212402344, \t Total Dis Loss : 0.006871373858302832\n",
      "Steps : 7000, \t Total Gen Loss : 25.273590087890625, \t Total Dis Loss : 0.007485006004571915\n",
      "Steps : 7100, \t Total Gen Loss : 23.77198600769043, \t Total Dis Loss : 0.006807316560298204\n",
      "Steps : 7200, \t Total Gen Loss : 27.89702033996582, \t Total Dis Loss : 0.014644716866314411\n",
      "Steps : 7300, \t Total Gen Loss : 26.917699813842773, \t Total Dis Loss : 0.007533702999353409\n",
      "Steps : 7400, \t Total Gen Loss : 28.824176788330078, \t Total Dis Loss : 0.006086536683142185\n",
      "Steps : 7500, \t Total Gen Loss : 27.03953742980957, \t Total Dis Loss : 0.01606098748743534\n",
      "Steps : 7600, \t Total Gen Loss : 23.453758239746094, \t Total Dis Loss : 0.4356578588485718\n",
      "Steps : 7700, \t Total Gen Loss : 24.83198356628418, \t Total Dis Loss : 0.020487036556005478\n",
      "Steps : 7800, \t Total Gen Loss : 26.771045684814453, \t Total Dis Loss : 0.0038407272659242153\n",
      "Steps : 7900, \t Total Gen Loss : 27.168140411376953, \t Total Dis Loss : 0.006309224292635918\n",
      "Steps : 8000, \t Total Gen Loss : 29.112213134765625, \t Total Dis Loss : 0.017629891633987427\n",
      "Steps : 8100, \t Total Gen Loss : 29.89798927307129, \t Total Dis Loss : 0.010677730664610863\n",
      "Steps : 8200, \t Total Gen Loss : 27.763870239257812, \t Total Dis Loss : 0.004464758560061455\n",
      "Steps : 8300, \t Total Gen Loss : 27.632490158081055, \t Total Dis Loss : 0.011059342883527279\n",
      "Steps : 8400, \t Total Gen Loss : 30.250198364257812, \t Total Dis Loss : 0.001412979676388204\n",
      "Steps : 8500, \t Total Gen Loss : 27.773529052734375, \t Total Dis Loss : 0.0065168216824531555\n",
      "Steps : 8600, \t Total Gen Loss : 26.358613967895508, \t Total Dis Loss : 0.011792280711233616\n",
      "Steps : 8700, \t Total Gen Loss : 27.27898597717285, \t Total Dis Loss : 0.0015924128238111734\n",
      "Steps : 8800, \t Total Gen Loss : 26.651424407958984, \t Total Dis Loss : 0.007702837698161602\n",
      "Steps : 8900, \t Total Gen Loss : 28.789173126220703, \t Total Dis Loss : 0.013183061964809895\n",
      "Steps : 9000, \t Total Gen Loss : 29.236408233642578, \t Total Dis Loss : 0.002617673482745886\n",
      "Steps : 9100, \t Total Gen Loss : 28.444843292236328, \t Total Dis Loss : 0.007797472178936005\n",
      "Steps : 9200, \t Total Gen Loss : 28.219987869262695, \t Total Dis Loss : 0.003966419957578182\n",
      "Steps : 9300, \t Total Gen Loss : 27.613481521606445, \t Total Dis Loss : 0.005292165093123913\n",
      "Steps : 9400, \t Total Gen Loss : 25.732948303222656, \t Total Dis Loss : 0.006811627186834812\n",
      "Steps : 9500, \t Total Gen Loss : 26.325279235839844, \t Total Dis Loss : 0.00235206657089293\n",
      "Steps : 9600, \t Total Gen Loss : 27.99307632446289, \t Total Dis Loss : 0.005172735080122948\n",
      "Steps : 9700, \t Total Gen Loss : 25.387632369995117, \t Total Dis Loss : 0.7885902523994446\n",
      "Steps : 9800, \t Total Gen Loss : 26.295602798461914, \t Total Dis Loss : 0.004665588028728962\n",
      "Steps : 9900, \t Total Gen Loss : 26.07811737060547, \t Total Dis Loss : 0.02932656742632389\n",
      "Steps : 10000, \t Total Gen Loss : 26.991588592529297, \t Total Dis Loss : 0.002643610816448927\n",
      "Steps : 10100, \t Total Gen Loss : 25.73359489440918, \t Total Dis Loss : 0.008592386730015278\n",
      "Steps : 10200, \t Total Gen Loss : 25.757871627807617, \t Total Dis Loss : 0.0034619015641510487\n",
      "Steps : 10300, \t Total Gen Loss : 23.205171585083008, \t Total Dis Loss : 0.004191972315311432\n",
      "Steps : 10400, \t Total Gen Loss : 27.696914672851562, \t Total Dis Loss : 0.0012377131497487426\n",
      "Steps : 10500, \t Total Gen Loss : 26.69023323059082, \t Total Dis Loss : 0.0023416478652507067\n",
      "Steps : 10600, \t Total Gen Loss : 25.293718338012695, \t Total Dis Loss : 0.026992658153176308\n",
      "Steps : 10700, \t Total Gen Loss : 26.75676155090332, \t Total Dis Loss : 0.004218670539557934\n",
      "Steps : 10800, \t Total Gen Loss : 25.551403045654297, \t Total Dis Loss : 0.008020954206585884\n",
      "Steps : 10900, \t Total Gen Loss : 28.880565643310547, \t Total Dis Loss : 0.0033844178542494774\n",
      "Steps : 11000, \t Total Gen Loss : 26.784751892089844, \t Total Dis Loss : 0.0018649967387318611\n",
      "Steps : 11100, \t Total Gen Loss : 26.0148983001709, \t Total Dis Loss : 0.0014258348383009434\n",
      "Steps : 11200, \t Total Gen Loss : 25.701343536376953, \t Total Dis Loss : 0.002102137776091695\n",
      "Steps : 11300, \t Total Gen Loss : 27.41376304626465, \t Total Dis Loss : 0.01045142114162445\n",
      "Steps : 11400, \t Total Gen Loss : 28.043012619018555, \t Total Dis Loss : 0.0017709907842800021\n",
      "Steps : 11500, \t Total Gen Loss : 26.84989356994629, \t Total Dis Loss : 0.014412011951208115\n",
      "Steps : 11600, \t Total Gen Loss : 25.08524513244629, \t Total Dis Loss : 0.0019307730253785849\n",
      "Steps : 11700, \t Total Gen Loss : 26.163644790649414, \t Total Dis Loss : 0.0030578041914850473\n",
      "Steps : 11800, \t Total Gen Loss : 25.774337768554688, \t Total Dis Loss : 0.008744584396481514\n",
      "Steps : 11900, \t Total Gen Loss : 26.786911010742188, \t Total Dis Loss : 0.005660367198288441\n",
      "Steps : 12000, \t Total Gen Loss : 26.37192726135254, \t Total Dis Loss : 0.0011855947086587548\n",
      "Steps : 12100, \t Total Gen Loss : 27.38410186767578, \t Total Dis Loss : 0.0020759725011885166\n",
      "Steps : 12200, \t Total Gen Loss : 27.57624053955078, \t Total Dis Loss : 0.0036558215506374836\n",
      "Steps : 12300, \t Total Gen Loss : 26.128093719482422, \t Total Dis Loss : 0.003747488372027874\n",
      "Steps : 12400, \t Total Gen Loss : 26.84097671508789, \t Total Dis Loss : 0.0012626242823898792\n",
      "Steps : 12500, \t Total Gen Loss : 28.76712417602539, \t Total Dis Loss : 0.0011922178091481328\n",
      "Steps : 12600, \t Total Gen Loss : 26.732942581176758, \t Total Dis Loss : 0.004100291524082422\n",
      "Steps : 12700, \t Total Gen Loss : 24.731109619140625, \t Total Dis Loss : 0.3193918466567993\n",
      "Steps : 12800, \t Total Gen Loss : 27.191160202026367, \t Total Dis Loss : 0.0015789908356964588\n",
      "Steps : 12900, \t Total Gen Loss : 28.307756423950195, \t Total Dis Loss : 0.0009830754715949297\n",
      "Steps : 13000, \t Total Gen Loss : 28.43299102783203, \t Total Dis Loss : 0.0011599150020629168\n",
      "Steps : 13100, \t Total Gen Loss : 27.873401641845703, \t Total Dis Loss : 0.0016087614931166172\n",
      "Steps : 13200, \t Total Gen Loss : 28.90884017944336, \t Total Dis Loss : 0.0004136816714890301\n",
      "Steps : 13300, \t Total Gen Loss : 24.869441986083984, \t Total Dis Loss : 0.006364996079355478\n",
      "Steps : 13400, \t Total Gen Loss : 26.910205841064453, \t Total Dis Loss : 0.00807766243815422\n",
      "Steps : 13500, \t Total Gen Loss : 28.47231674194336, \t Total Dis Loss : 0.0011090440675616264\n",
      "Time for epoch 2 is 347.262255191803 sec\n",
      "Steps : 13600, \t Total Gen Loss : 26.5246524810791, \t Total Dis Loss : 0.006170556880533695\n",
      "Steps : 13700, \t Total Gen Loss : 25.431516647338867, \t Total Dis Loss : 0.004680870566517115\n",
      "Steps : 13800, \t Total Gen Loss : 27.984888076782227, \t Total Dis Loss : 0.004823640920221806\n",
      "Steps : 13900, \t Total Gen Loss : 28.414226531982422, \t Total Dis Loss : 0.0023178390692919493\n",
      "Steps : 14000, \t Total Gen Loss : 28.898414611816406, \t Total Dis Loss : 0.0016822937177494168\n",
      "Steps : 14100, \t Total Gen Loss : 28.37594985961914, \t Total Dis Loss : 0.001313564134761691\n",
      "Steps : 14200, \t Total Gen Loss : 28.949983596801758, \t Total Dis Loss : 0.0013198100496083498\n",
      "Steps : 14300, \t Total Gen Loss : 28.331085205078125, \t Total Dis Loss : 0.0012432931689545512\n",
      "Steps : 14400, \t Total Gen Loss : 25.706192016601562, \t Total Dis Loss : 0.0006782246637158096\n",
      "Steps : 14500, \t Total Gen Loss : 28.254491806030273, \t Total Dis Loss : 0.0014563651056960225\n",
      "Steps : 14600, \t Total Gen Loss : 29.77427101135254, \t Total Dis Loss : 0.005751611664891243\n",
      "Steps : 14700, \t Total Gen Loss : 25.909833908081055, \t Total Dis Loss : 0.0020234009716659784\n",
      "Steps : 14800, \t Total Gen Loss : 25.47907257080078, \t Total Dis Loss : 0.00190535222645849\n",
      "Steps : 14900, \t Total Gen Loss : 26.408891677856445, \t Total Dis Loss : 0.002480710856616497\n",
      "Steps : 15000, \t Total Gen Loss : 28.178125381469727, \t Total Dis Loss : 0.0008252932457253337\n",
      "Steps : 15100, \t Total Gen Loss : 27.02997589111328, \t Total Dis Loss : 0.0026305464562028646\n",
      "Steps : 15200, \t Total Gen Loss : 24.36739730834961, \t Total Dis Loss : 0.008369375951588154\n",
      "Steps : 15300, \t Total Gen Loss : 26.539772033691406, \t Total Dis Loss : 0.009236112236976624\n",
      "Steps : 15400, \t Total Gen Loss : 27.389291763305664, \t Total Dis Loss : 0.0022943359799683094\n",
      "Steps : 15500, \t Total Gen Loss : 28.006813049316406, \t Total Dis Loss : 0.0013592897448688745\n",
      "Steps : 15600, \t Total Gen Loss : 26.75870704650879, \t Total Dis Loss : 0.001845589722506702\n",
      "Steps : 15700, \t Total Gen Loss : 26.978389739990234, \t Total Dis Loss : 0.001577866729348898\n",
      "Steps : 15800, \t Total Gen Loss : 28.837623596191406, \t Total Dis Loss : 0.0023407090920954943\n",
      "Steps : 15900, \t Total Gen Loss : 28.198163986206055, \t Total Dis Loss : 0.0026105898432433605\n",
      "Steps : 16000, \t Total Gen Loss : 31.401609420776367, \t Total Dis Loss : 0.001553786569274962\n",
      "Steps : 16100, \t Total Gen Loss : 30.296110153198242, \t Total Dis Loss : 0.06571419537067413\n",
      "Steps : 16200, \t Total Gen Loss : 28.985851287841797, \t Total Dis Loss : 0.0031434656120836735\n",
      "Steps : 16300, \t Total Gen Loss : 28.605079650878906, \t Total Dis Loss : 0.004364702384918928\n",
      "Steps : 16400, \t Total Gen Loss : 27.79831886291504, \t Total Dis Loss : 0.0007708788616582751\n",
      "Steps : 16500, \t Total Gen Loss : 26.68806266784668, \t Total Dis Loss : 0.004491494037210941\n",
      "Steps : 16600, \t Total Gen Loss : 28.41400146484375, \t Total Dis Loss : 0.0020096730440855026\n",
      "Steps : 16700, \t Total Gen Loss : 25.48421859741211, \t Total Dis Loss : 0.004331827629357576\n",
      "Steps : 16800, \t Total Gen Loss : 27.85067367553711, \t Total Dis Loss : 0.004934077616780996\n",
      "Steps : 16900, \t Total Gen Loss : 30.027786254882812, \t Total Dis Loss : 0.0027742264792323112\n",
      "Steps : 17000, \t Total Gen Loss : 28.310192108154297, \t Total Dis Loss : 0.04261031001806259\n",
      "Steps : 17100, \t Total Gen Loss : 28.7874698638916, \t Total Dis Loss : 0.008427757769823074\n",
      "Steps : 17200, \t Total Gen Loss : 26.748489379882812, \t Total Dis Loss : 0.0042863767594099045\n",
      "Steps : 17300, \t Total Gen Loss : 27.15760612487793, \t Total Dis Loss : 0.004183280281722546\n",
      "Steps : 17400, \t Total Gen Loss : 29.08879280090332, \t Total Dis Loss : 0.005362106021493673\n",
      "Steps : 17500, \t Total Gen Loss : 23.76028060913086, \t Total Dis Loss : 0.01065043918788433\n",
      "Steps : 17600, \t Total Gen Loss : 25.768268585205078, \t Total Dis Loss : 0.025818148627877235\n",
      "Steps : 17700, \t Total Gen Loss : 27.96185874938965, \t Total Dis Loss : 0.002610889496281743\n",
      "Steps : 17800, \t Total Gen Loss : 28.288007736206055, \t Total Dis Loss : 0.003667948069050908\n",
      "Steps : 17900, \t Total Gen Loss : 27.880373001098633, \t Total Dis Loss : 0.0023955469951033592\n",
      "Steps : 18000, \t Total Gen Loss : 27.742822647094727, \t Total Dis Loss : 0.008271774277091026\n",
      "Steps : 18100, \t Total Gen Loss : 26.300491333007812, \t Total Dis Loss : 0.007029466796666384\n",
      "Steps : 18200, \t Total Gen Loss : 24.97597885131836, \t Total Dis Loss : 0.00951590109616518\n",
      "Steps : 18300, \t Total Gen Loss : 26.002355575561523, \t Total Dis Loss : 0.11843288689851761\n",
      "Steps : 18400, \t Total Gen Loss : 29.173221588134766, \t Total Dis Loss : 0.0036761853843927383\n",
      "Steps : 18500, \t Total Gen Loss : 26.950551986694336, \t Total Dis Loss : 0.0010887623066082597\n",
      "Steps : 18600, \t Total Gen Loss : 28.30641746520996, \t Total Dis Loss : 0.0011441740207374096\n",
      "Steps : 18700, \t Total Gen Loss : 26.046003341674805, \t Total Dis Loss : 0.0011277602752670646\n",
      "Steps : 18800, \t Total Gen Loss : 27.001680374145508, \t Total Dis Loss : 0.0015623379731550813\n",
      "Steps : 18900, \t Total Gen Loss : 26.184146881103516, \t Total Dis Loss : 0.000883165281265974\n",
      "Steps : 19000, \t Total Gen Loss : 20.466259002685547, \t Total Dis Loss : 4.087769508361816\n",
      "Steps : 19100, \t Total Gen Loss : 26.081092834472656, \t Total Dis Loss : 0.0024995477870106697\n",
      "Steps : 19200, \t Total Gen Loss : 26.3145694732666, \t Total Dis Loss : 0.0017605560133233666\n",
      "Steps : 19300, \t Total Gen Loss : 26.772714614868164, \t Total Dis Loss : 0.0030926261097192764\n",
      "Steps : 19400, \t Total Gen Loss : 27.53667640686035, \t Total Dis Loss : 0.0025767695624381304\n",
      "Steps : 19500, \t Total Gen Loss : 29.464876174926758, \t Total Dis Loss : 0.0007207090384326875\n",
      "Steps : 19600, \t Total Gen Loss : 29.260509490966797, \t Total Dis Loss : 0.0017729856772348285\n",
      "Steps : 19700, \t Total Gen Loss : 26.786941528320312, \t Total Dis Loss : 0.003750370815396309\n",
      "Steps : 19800, \t Total Gen Loss : 25.74142074584961, \t Total Dis Loss : 0.05916764959692955\n",
      "Steps : 19900, \t Total Gen Loss : 26.616172790527344, \t Total Dis Loss : 0.0015755451750010252\n",
      "Steps : 20000, \t Total Gen Loss : 27.717205047607422, \t Total Dis Loss : 0.0013517244951799512\n",
      "Steps : 20100, \t Total Gen Loss : 26.876800537109375, \t Total Dis Loss : 0.001475030672736466\n",
      "Steps : 20200, \t Total Gen Loss : 27.608882904052734, \t Total Dis Loss : 0.0007699462003074586\n",
      "Time for epoch 3 is 338.10966300964355 sec\n",
      "Steps : 20300, \t Total Gen Loss : 26.850116729736328, \t Total Dis Loss : 0.0006024960312061012\n",
      "Steps : 20400, \t Total Gen Loss : 27.613128662109375, \t Total Dis Loss : 0.0006444182363338768\n",
      "Steps : 20500, \t Total Gen Loss : 28.866554260253906, \t Total Dis Loss : 0.0022730412892997265\n",
      "Steps : 20600, \t Total Gen Loss : 26.916723251342773, \t Total Dis Loss : 0.094530388712883\n",
      "Steps : 20700, \t Total Gen Loss : 27.49496841430664, \t Total Dis Loss : 0.0026150615885853767\n",
      "Steps : 20800, \t Total Gen Loss : 27.329113006591797, \t Total Dis Loss : 0.001229556743055582\n",
      "Steps : 20900, \t Total Gen Loss : 28.958520889282227, \t Total Dis Loss : 0.0006215856410562992\n",
      "Steps : 21000, \t Total Gen Loss : 30.436845779418945, \t Total Dis Loss : 0.00027743761893361807\n",
      "Steps : 21100, \t Total Gen Loss : 27.943843841552734, \t Total Dis Loss : 0.000626724970061332\n",
      "Steps : 21200, \t Total Gen Loss : 28.222396850585938, \t Total Dis Loss : 0.0006767965969629586\n",
      "Steps : 21300, \t Total Gen Loss : 31.197649002075195, \t Total Dis Loss : 0.00046882868628017604\n",
      "Steps : 21400, \t Total Gen Loss : 24.310134887695312, \t Total Dis Loss : 0.0058482978492975235\n",
      "Steps : 21500, \t Total Gen Loss : 28.573699951171875, \t Total Dis Loss : 0.0013678651303052902\n",
      "Steps : 21600, \t Total Gen Loss : 26.614789962768555, \t Total Dis Loss : 0.0015202725771814585\n",
      "Steps : 21700, \t Total Gen Loss : 25.42887306213379, \t Total Dis Loss : 0.014757632277905941\n",
      "Steps : 21800, \t Total Gen Loss : 28.56631088256836, \t Total Dis Loss : 0.000886010006070137\n",
      "Steps : 21900, \t Total Gen Loss : 28.245311737060547, \t Total Dis Loss : 0.004908205475658178\n",
      "Steps : 22000, \t Total Gen Loss : 26.244426727294922, \t Total Dis Loss : 0.0019360363949090242\n",
      "Steps : 22100, \t Total Gen Loss : 27.769712448120117, \t Total Dis Loss : 0.0013547621201723814\n",
      "Steps : 22200, \t Total Gen Loss : 28.375411987304688, \t Total Dis Loss : 0.0014693913981318474\n",
      "Steps : 22300, \t Total Gen Loss : 29.60980796813965, \t Total Dis Loss : 0.000939274556003511\n",
      "Steps : 22400, \t Total Gen Loss : 28.493976593017578, \t Total Dis Loss : 0.001640873495489359\n",
      "Steps : 22500, \t Total Gen Loss : 28.715137481689453, \t Total Dis Loss : 0.0008932011551223695\n",
      "Steps : 22600, \t Total Gen Loss : 29.255468368530273, \t Total Dis Loss : 0.000755095505155623\n",
      "Steps : 22700, \t Total Gen Loss : 29.077512741088867, \t Total Dis Loss : 0.0005708847311325371\n",
      "Steps : 22800, \t Total Gen Loss : 28.440587997436523, \t Total Dis Loss : 0.0011905988212674856\n",
      "Steps : 22900, \t Total Gen Loss : 31.375986099243164, \t Total Dis Loss : 0.00025846948847174644\n",
      "Steps : 23000, \t Total Gen Loss : 29.713796615600586, \t Total Dis Loss : 0.0006688645808026195\n",
      "Steps : 23100, \t Total Gen Loss : 27.364912033081055, \t Total Dis Loss : 0.001945141819305718\n",
      "Steps : 23200, \t Total Gen Loss : 28.293319702148438, \t Total Dis Loss : 0.0008245626231655478\n",
      "Steps : 23300, \t Total Gen Loss : 31.925331115722656, \t Total Dis Loss : 0.001561709912493825\n",
      "Steps : 23400, \t Total Gen Loss : 28.394481658935547, \t Total Dis Loss : 0.0035867432598024607\n",
      "Steps : 23500, \t Total Gen Loss : 32.53800582885742, \t Total Dis Loss : 0.000798664812464267\n",
      "Steps : 23600, \t Total Gen Loss : 30.244861602783203, \t Total Dis Loss : 0.0020597982220351696\n",
      "Steps : 23700, \t Total Gen Loss : 30.148540496826172, \t Total Dis Loss : 0.00118793617002666\n",
      "Steps : 23800, \t Total Gen Loss : 30.2486572265625, \t Total Dis Loss : 0.0007641202537342906\n",
      "Steps : 23900, \t Total Gen Loss : 29.10304832458496, \t Total Dis Loss : 0.002547099255025387\n",
      "Steps : 24000, \t Total Gen Loss : 27.31985092163086, \t Total Dis Loss : 0.0007364906487055123\n",
      "Steps : 24100, \t Total Gen Loss : 27.407197952270508, \t Total Dis Loss : 0.0008685457287356257\n",
      "Steps : 24200, \t Total Gen Loss : 30.315284729003906, \t Total Dis Loss : 0.00033700777566991746\n",
      "Steps : 24300, \t Total Gen Loss : 30.607391357421875, \t Total Dis Loss : 0.00019568364950828254\n",
      "Steps : 24400, \t Total Gen Loss : 27.871946334838867, \t Total Dis Loss : 0.00041290849912911654\n",
      "Steps : 24500, \t Total Gen Loss : 27.58633804321289, \t Total Dis Loss : 0.002712423447519541\n",
      "Steps : 24600, \t Total Gen Loss : 28.88395118713379, \t Total Dis Loss : 0.00025313819060102105\n",
      "Steps : 24700, \t Total Gen Loss : 28.280437469482422, \t Total Dis Loss : 0.002035447396337986\n",
      "Steps : 24800, \t Total Gen Loss : 27.98174285888672, \t Total Dis Loss : 0.000674496463034302\n",
      "Steps : 24900, \t Total Gen Loss : 29.034029006958008, \t Total Dis Loss : 0.0005708973621949553\n",
      "Steps : 25000, \t Total Gen Loss : 28.29534912109375, \t Total Dis Loss : 0.0020734411664307117\n",
      "Steps : 25100, \t Total Gen Loss : 29.610876083374023, \t Total Dis Loss : 0.0004616822116076946\n",
      "Steps : 25200, \t Total Gen Loss : 28.203950881958008, \t Total Dis Loss : 0.0011233529075980186\n",
      "Steps : 25300, \t Total Gen Loss : 27.931312561035156, \t Total Dis Loss : 0.0007398580200970173\n",
      "Steps : 25400, \t Total Gen Loss : 27.970338821411133, \t Total Dis Loss : 0.005554721225053072\n",
      "Steps : 25500, \t Total Gen Loss : 29.1131591796875, \t Total Dis Loss : 0.0005967637407593429\n",
      "Steps : 25600, \t Total Gen Loss : 27.410297393798828, \t Total Dis Loss : 0.001990122254937887\n",
      "Steps : 25700, \t Total Gen Loss : 25.93320083618164, \t Total Dis Loss : 0.0014552883803844452\n",
      "Steps : 25800, \t Total Gen Loss : 28.564722061157227, \t Total Dis Loss : 0.0008451822213828564\n",
      "Steps : 25900, \t Total Gen Loss : 29.498226165771484, \t Total Dis Loss : 0.0009947770740836859\n",
      "Steps : 26000, \t Total Gen Loss : 27.773061752319336, \t Total Dis Loss : 0.0008215505513362586\n",
      "Steps : 26100, \t Total Gen Loss : 27.797536849975586, \t Total Dis Loss : 0.004022432956844568\n",
      "Steps : 26200, \t Total Gen Loss : 28.743589401245117, \t Total Dis Loss : 0.002395777963101864\n",
      "Steps : 26300, \t Total Gen Loss : 27.71219825744629, \t Total Dis Loss : 0.0008250863174907863\n",
      "Steps : 26400, \t Total Gen Loss : 28.497892379760742, \t Total Dis Loss : 0.0007021308410912752\n",
      "Steps : 26500, \t Total Gen Loss : 27.99358558654785, \t Total Dis Loss : 0.0007675053202547133\n",
      "Steps : 26600, \t Total Gen Loss : 28.552385330200195, \t Total Dis Loss : 0.0011932452907785773\n",
      "Steps : 26700, \t Total Gen Loss : 26.999788284301758, \t Total Dis Loss : 0.0010021643247455359\n",
      "Steps : 26800, \t Total Gen Loss : 27.296546936035156, \t Total Dis Loss : 0.0018144850619137287\n",
      "Steps : 26900, \t Total Gen Loss : 25.951173782348633, \t Total Dis Loss : 0.001982271671295166\n",
      "Steps : 27000, \t Total Gen Loss : 28.757579803466797, \t Total Dis Loss : 0.0005605008918792009\n",
      "Time for epoch 4 is 341.50665736198425 sec\n",
      "Steps : 27100, \t Total Gen Loss : 29.993316650390625, \t Total Dis Loss : 0.0005346290417946875\n",
      "Steps : 27200, \t Total Gen Loss : 28.8196964263916, \t Total Dis Loss : 0.07139670848846436\n",
      "Steps : 27300, \t Total Gen Loss : 29.368473052978516, \t Total Dis Loss : 0.0030160113237798214\n",
      "Steps : 27400, \t Total Gen Loss : 31.225189208984375, \t Total Dis Loss : 0.0007263453444465995\n",
      "Steps : 27500, \t Total Gen Loss : 29.31364631652832, \t Total Dis Loss : 0.009978143498301506\n",
      "Steps : 27600, \t Total Gen Loss : 36.7612419128418, \t Total Dis Loss : 0.008684583008289337\n",
      "Steps : 27700, \t Total Gen Loss : 29.841947555541992, \t Total Dis Loss : 0.0030329073779284954\n",
      "Steps : 27800, \t Total Gen Loss : 29.84417152404785, \t Total Dis Loss : 0.01175796426832676\n",
      "Steps : 27900, \t Total Gen Loss : 28.550708770751953, \t Total Dis Loss : 0.003460387699306011\n",
      "Steps : 28000, \t Total Gen Loss : 28.756635665893555, \t Total Dis Loss : 0.0009384885779581964\n",
      "Steps : 28100, \t Total Gen Loss : 29.586891174316406, \t Total Dis Loss : 0.004928078968077898\n",
      "Steps : 28200, \t Total Gen Loss : 31.173940658569336, \t Total Dis Loss : 0.0044062319211661816\n",
      "Steps : 28300, \t Total Gen Loss : 29.844575881958008, \t Total Dis Loss : 0.002731452463194728\n",
      "Steps : 28400, \t Total Gen Loss : 30.32710075378418, \t Total Dis Loss : 0.0023277464788407087\n",
      "Steps : 28500, \t Total Gen Loss : 29.44944190979004, \t Total Dis Loss : 0.0008631132077425718\n",
      "Steps : 28600, \t Total Gen Loss : 29.24806785583496, \t Total Dis Loss : 0.0015190769918262959\n",
      "Steps : 28700, \t Total Gen Loss : 30.015953063964844, \t Total Dis Loss : 0.0005613375687971711\n",
      "Steps : 28800, \t Total Gen Loss : 32.345191955566406, \t Total Dis Loss : 0.0005329300183802843\n",
      "Steps : 28900, \t Total Gen Loss : 30.286863327026367, \t Total Dis Loss : 0.00235047098249197\n",
      "Steps : 29000, \t Total Gen Loss : 28.597509384155273, \t Total Dis Loss : 0.0016931764548644423\n",
      "Steps : 29100, \t Total Gen Loss : 28.33714485168457, \t Total Dis Loss : 0.0009310737950727344\n",
      "Steps : 29200, \t Total Gen Loss : 26.926761627197266, \t Total Dis Loss : 0.17140528559684753\n",
      "Steps : 29300, \t Total Gen Loss : 26.1324462890625, \t Total Dis Loss : 0.002449560910463333\n",
      "Steps : 29400, \t Total Gen Loss : 29.281997680664062, \t Total Dis Loss : 0.00037558400072157383\n",
      "Steps : 29500, \t Total Gen Loss : 29.509031295776367, \t Total Dis Loss : 0.0014554773224517703\n",
      "Steps : 29600, \t Total Gen Loss : 28.086986541748047, \t Total Dis Loss : 0.004732079803943634\n",
      "Steps : 29700, \t Total Gen Loss : 28.02911949157715, \t Total Dis Loss : 0.00210033287294209\n",
      "Steps : 29800, \t Total Gen Loss : 28.44488525390625, \t Total Dis Loss : 0.0010924346279352903\n",
      "Steps : 29900, \t Total Gen Loss : 29.074548721313477, \t Total Dis Loss : 0.0006740344688296318\n",
      "Steps : 30000, \t Total Gen Loss : 25.075237274169922, \t Total Dis Loss : 0.001969123026356101\n",
      "Steps : 30100, \t Total Gen Loss : 26.808351516723633, \t Total Dis Loss : 0.0008873951737768948\n",
      "Steps : 30200, \t Total Gen Loss : 30.99152374267578, \t Total Dis Loss : 0.0018431083299219608\n",
      "Steps : 30300, \t Total Gen Loss : 29.31845474243164, \t Total Dis Loss : 8.254306158050895e-05\n",
      "Steps : 30400, \t Total Gen Loss : 28.22602081298828, \t Total Dis Loss : 0.0006167229730635881\n",
      "Steps : 30500, \t Total Gen Loss : 26.966320037841797, \t Total Dis Loss : 0.004338695202022791\n",
      "Steps : 30600, \t Total Gen Loss : 27.476085662841797, \t Total Dis Loss : 0.0016640751855447888\n",
      "Steps : 30700, \t Total Gen Loss : 27.494417190551758, \t Total Dis Loss : 0.006067512556910515\n",
      "Steps : 30800, \t Total Gen Loss : 29.637283325195312, \t Total Dis Loss : 0.00034115355811081827\n",
      "Steps : 30900, \t Total Gen Loss : 28.0393123626709, \t Total Dis Loss : 0.0005751472199335694\n",
      "Steps : 31000, \t Total Gen Loss : 27.971220016479492, \t Total Dis Loss : 0.00077341083670035\n",
      "Steps : 31100, \t Total Gen Loss : 27.587684631347656, \t Total Dis Loss : 0.000722753640729934\n",
      "Steps : 31200, \t Total Gen Loss : 27.988500595092773, \t Total Dis Loss : 0.0009757594089023769\n",
      "Steps : 31300, \t Total Gen Loss : 27.920429229736328, \t Total Dis Loss : 0.0004085585824213922\n",
      "Steps : 31400, \t Total Gen Loss : 26.926319122314453, \t Total Dis Loss : 0.001316449255682528\n",
      "Steps : 31500, \t Total Gen Loss : 26.86149787902832, \t Total Dis Loss : 0.0035819250624626875\n",
      "Steps : 31600, \t Total Gen Loss : 29.087142944335938, \t Total Dis Loss : 0.00126889836974442\n",
      "Steps : 31700, \t Total Gen Loss : 26.735626220703125, \t Total Dis Loss : 0.002384460996836424\n",
      "Steps : 31800, \t Total Gen Loss : 31.04677391052246, \t Total Dis Loss : 0.0006635713507421315\n",
      "Steps : 31900, \t Total Gen Loss : 28.659404754638672, \t Total Dis Loss : 0.001190842129290104\n",
      "Steps : 32000, \t Total Gen Loss : 30.32272720336914, \t Total Dis Loss : 0.000959315337240696\n",
      "Steps : 32100, \t Total Gen Loss : 29.658172607421875, \t Total Dis Loss : 0.0005274275317788124\n",
      "Steps : 32200, \t Total Gen Loss : 30.32645606994629, \t Total Dis Loss : 0.0011400317307561636\n",
      "Steps : 32300, \t Total Gen Loss : 30.362384796142578, \t Total Dis Loss : 0.000270867720246315\n",
      "Steps : 32400, \t Total Gen Loss : 27.146560668945312, \t Total Dis Loss : 0.001156406244263053\n",
      "Steps : 32500, \t Total Gen Loss : 30.34697914123535, \t Total Dis Loss : 0.0005803729873150587\n",
      "Steps : 32600, \t Total Gen Loss : 28.36359977722168, \t Total Dis Loss : 0.02169841155409813\n",
      "Steps : 32700, \t Total Gen Loss : 28.939884185791016, \t Total Dis Loss : 0.003844648599624634\n",
      "Steps : 32800, \t Total Gen Loss : 28.69725799560547, \t Total Dis Loss : 0.001933349296450615\n",
      "Steps : 32900, \t Total Gen Loss : 28.820955276489258, \t Total Dis Loss : 0.0005724255461245775\n",
      "Steps : 33000, \t Total Gen Loss : 27.95684242248535, \t Total Dis Loss : 0.0003915630513802171\n",
      "Steps : 33100, \t Total Gen Loss : 28.7773380279541, \t Total Dis Loss : 0.0002501128474250436\n",
      "Steps : 33200, \t Total Gen Loss : 32.17630386352539, \t Total Dis Loss : 0.0006680293590761721\n",
      "Steps : 33300, \t Total Gen Loss : 29.32550048828125, \t Total Dis Loss : 0.001680765300989151\n",
      "Steps : 33400, \t Total Gen Loss : 30.200082778930664, \t Total Dis Loss : 0.00032145134173333645\n",
      "Steps : 33500, \t Total Gen Loss : 27.432249069213867, \t Total Dis Loss : 0.0035688444040715694\n",
      "Steps : 33600, \t Total Gen Loss : 26.55706024169922, \t Total Dis Loss : 0.0006297979853115976\n",
      "Steps : 33700, \t Total Gen Loss : 26.49068260192871, \t Total Dis Loss : 0.0018143242923542857\n",
      "Time for epoch 5 is 346.42449712753296 sec\n",
      "Steps : 33800, \t Total Gen Loss : 26.77149200439453, \t Total Dis Loss : 0.0009852431248873472\n",
      "Steps : 33900, \t Total Gen Loss : 25.790016174316406, \t Total Dis Loss : 0.002289807191118598\n",
      "Steps : 34000, \t Total Gen Loss : 30.824871063232422, \t Total Dis Loss : 0.0002882442786358297\n",
      "Steps : 34100, \t Total Gen Loss : 28.208805084228516, \t Total Dis Loss : 0.0020487511064857244\n",
      "Steps : 34200, \t Total Gen Loss : 31.838760375976562, \t Total Dis Loss : 0.0004158882366027683\n",
      "Steps : 34300, \t Total Gen Loss : 27.7249755859375, \t Total Dis Loss : 0.002248358214274049\n",
      "Steps : 34400, \t Total Gen Loss : 26.299108505249023, \t Total Dis Loss : 0.0018313450273126364\n",
      "Steps : 34500, \t Total Gen Loss : 27.547061920166016, \t Total Dis Loss : 0.005330223124474287\n",
      "Steps : 34600, \t Total Gen Loss : 27.1758975982666, \t Total Dis Loss : 0.0012761480174958706\n",
      "Steps : 34700, \t Total Gen Loss : 27.100191116333008, \t Total Dis Loss : 0.0015852648066356778\n",
      "Steps : 34800, \t Total Gen Loss : 27.281959533691406, \t Total Dis Loss : 0.002589805517345667\n",
      "Steps : 34900, \t Total Gen Loss : 25.72536849975586, \t Total Dis Loss : 0.0031776572577655315\n",
      "Steps : 35000, \t Total Gen Loss : 26.0950870513916, \t Total Dis Loss : 0.003738861996680498\n",
      "Steps : 35100, \t Total Gen Loss : 25.371517181396484, \t Total Dis Loss : 0.001059053698554635\n",
      "Steps : 35200, \t Total Gen Loss : 26.863426208496094, \t Total Dis Loss : 0.0005678097950294614\n",
      "Steps : 35300, \t Total Gen Loss : 28.420316696166992, \t Total Dis Loss : 0.0004059547500219196\n",
      "Steps : 35400, \t Total Gen Loss : 29.35274314880371, \t Total Dis Loss : 0.00044532291940413415\n",
      "Steps : 35500, \t Total Gen Loss : 29.91922378540039, \t Total Dis Loss : 0.00034568863338790834\n",
      "Steps : 35600, \t Total Gen Loss : 27.50295639038086, \t Total Dis Loss : 0.0008725123479962349\n",
      "Steps : 35700, \t Total Gen Loss : 30.704050064086914, \t Total Dis Loss : 0.00017128803301602602\n",
      "Steps : 35800, \t Total Gen Loss : 29.234888076782227, \t Total Dis Loss : 0.00038212351500988007\n",
      "Steps : 35900, \t Total Gen Loss : 30.596153259277344, \t Total Dis Loss : 0.0002546818577684462\n",
      "Steps : 36000, \t Total Gen Loss : 34.154502868652344, \t Total Dis Loss : 0.00015082080790307373\n",
      "Steps : 36100, \t Total Gen Loss : 29.138402938842773, \t Total Dis Loss : 0.0024707564152777195\n",
      "Steps : 36200, \t Total Gen Loss : 29.811670303344727, \t Total Dis Loss : 0.0006440390716306865\n",
      "Steps : 36300, \t Total Gen Loss : 31.54804229736328, \t Total Dis Loss : 0.00036137946881353855\n",
      "Steps : 36400, \t Total Gen Loss : 31.15686798095703, \t Total Dis Loss : 0.0002736909664236009\n",
      "Steps : 36500, \t Total Gen Loss : 28.714900970458984, \t Total Dis Loss : 0.0012619660701602697\n",
      "Steps : 36600, \t Total Gen Loss : 26.95950698852539, \t Total Dis Loss : 0.0018334172200411558\n",
      "Steps : 36700, \t Total Gen Loss : 27.284523010253906, \t Total Dis Loss : 0.0008307622047141194\n",
      "Steps : 36800, \t Total Gen Loss : 30.253768920898438, \t Total Dis Loss : 0.004314655438065529\n",
      "Steps : 36900, \t Total Gen Loss : 27.28103256225586, \t Total Dis Loss : 0.001852571964263916\n",
      "Steps : 37000, \t Total Gen Loss : 27.853469848632812, \t Total Dis Loss : 0.0011290691327303648\n",
      "Steps : 37100, \t Total Gen Loss : 30.020023345947266, \t Total Dis Loss : 0.00048796963528729975\n",
      "Steps : 37200, \t Total Gen Loss : 30.821680068969727, \t Total Dis Loss : 0.00023768196115270257\n",
      "Steps : 37300, \t Total Gen Loss : 28.135984420776367, \t Total Dis Loss : 0.0008046825532801449\n",
      "Steps : 37400, \t Total Gen Loss : 27.397022247314453, \t Total Dis Loss : 0.00032846161047928035\n",
      "Steps : 37500, \t Total Gen Loss : 28.901269912719727, \t Total Dis Loss : 0.0006797433597967029\n",
      "Steps : 37600, \t Total Gen Loss : 25.958271026611328, \t Total Dis Loss : 0.002335551893338561\n",
      "Steps : 37700, \t Total Gen Loss : 26.66883659362793, \t Total Dis Loss : 0.0007372479303739965\n",
      "Steps : 37800, \t Total Gen Loss : 29.212209701538086, \t Total Dis Loss : 0.00031926596420817077\n",
      "Steps : 37900, \t Total Gen Loss : 28.251962661743164, \t Total Dis Loss : 0.0003646488767117262\n",
      "Steps : 38000, \t Total Gen Loss : 29.12596893310547, \t Total Dis Loss : 0.0027670576237142086\n",
      "Steps : 38100, \t Total Gen Loss : 26.942495346069336, \t Total Dis Loss : 0.0025511092972010374\n",
      "Steps : 38200, \t Total Gen Loss : 27.347864151000977, \t Total Dis Loss : 0.00041861203499138355\n",
      "Steps : 38300, \t Total Gen Loss : 29.144073486328125, \t Total Dis Loss : 0.0004655525553971529\n",
      "Steps : 38400, \t Total Gen Loss : 29.4677734375, \t Total Dis Loss : 0.00047965458361431956\n",
      "Steps : 38500, \t Total Gen Loss : 28.589092254638672, \t Total Dis Loss : 0.000783113413490355\n",
      "Steps : 38600, \t Total Gen Loss : 29.108335494995117, \t Total Dis Loss : 0.00027589412638917565\n",
      "Steps : 38700, \t Total Gen Loss : 30.682432174682617, \t Total Dis Loss : 0.0003299483214505017\n",
      "Steps : 38800, \t Total Gen Loss : 30.98257064819336, \t Total Dis Loss : 0.0001412492129020393\n",
      "Steps : 38900, \t Total Gen Loss : 28.62899398803711, \t Total Dis Loss : 0.12829245626926422\n",
      "Steps : 39000, \t Total Gen Loss : 32.09210968017578, \t Total Dis Loss : 0.0002808223362080753\n",
      "Steps : 39100, \t Total Gen Loss : 27.596834182739258, \t Total Dis Loss : 0.0007500638021156192\n",
      "Steps : 39200, \t Total Gen Loss : 28.473604202270508, \t Total Dis Loss : 0.005979529581964016\n",
      "Steps : 39300, \t Total Gen Loss : 26.855989456176758, \t Total Dis Loss : 0.0012867026962339878\n",
      "Steps : 39400, \t Total Gen Loss : 27.57066535949707, \t Total Dis Loss : 0.0011458845110610127\n",
      "Steps : 39500, \t Total Gen Loss : 28.055015563964844, \t Total Dis Loss : 0.0014453600160777569\n",
      "Steps : 39600, \t Total Gen Loss : 28.135347366333008, \t Total Dis Loss : 0.0014348074328154325\n",
      "Steps : 39700, \t Total Gen Loss : 29.881803512573242, \t Total Dis Loss : 0.0004602187837008387\n",
      "Steps : 39800, \t Total Gen Loss : 29.554048538208008, \t Total Dis Loss : 0.0003791721537709236\n",
      "Steps : 39900, \t Total Gen Loss : 29.587026596069336, \t Total Dis Loss : 0.00021941991872154176\n",
      "Steps : 40000, \t Total Gen Loss : 28.456680297851562, \t Total Dis Loss : 0.0001625672884983942\n",
      "Steps : 40100, \t Total Gen Loss : 30.881181716918945, \t Total Dis Loss : 0.00012255088950041682\n",
      "Steps : 40200, \t Total Gen Loss : 29.148107528686523, \t Total Dis Loss : 0.0008085502195172012\n",
      "Steps : 40300, \t Total Gen Loss : 29.040273666381836, \t Total Dis Loss : 0.0005231182440184057\n",
      "Steps : 40400, \t Total Gen Loss : 31.00411033630371, \t Total Dis Loss : 0.0003042088937945664\n",
      "Steps : 40500, \t Total Gen Loss : 31.298402786254883, \t Total Dis Loss : 0.0007222816930152476\n",
      "Time for epoch 6 is 343.879718542099 sec\n",
      "Steps : 40600, \t Total Gen Loss : 29.888771057128906, \t Total Dis Loss : 0.0002645117638166994\n",
      "Steps : 40700, \t Total Gen Loss : 26.44814109802246, \t Total Dis Loss : 0.0006112666451372206\n",
      "Steps : 40800, \t Total Gen Loss : 27.08445167541504, \t Total Dis Loss : 0.0017262656474485993\n",
      "Steps : 40900, \t Total Gen Loss : 29.813207626342773, \t Total Dis Loss : 0.0006002287263981998\n",
      "Steps : 41000, \t Total Gen Loss : 28.13734245300293, \t Total Dis Loss : 0.005969837307929993\n",
      "Steps : 41100, \t Total Gen Loss : 29.011898040771484, \t Total Dis Loss : 0.0003805170417763293\n",
      "Steps : 41200, \t Total Gen Loss : 27.879098892211914, \t Total Dis Loss : 0.0021789169404655695\n",
      "Steps : 41300, \t Total Gen Loss : 29.8930721282959, \t Total Dis Loss : 0.000279457017313689\n",
      "Steps : 41400, \t Total Gen Loss : 28.52096939086914, \t Total Dis Loss : 0.0002504830772522837\n",
      "Steps : 41500, \t Total Gen Loss : 27.16645050048828, \t Total Dis Loss : 0.004902995191514492\n",
      "Steps : 41600, \t Total Gen Loss : 28.84663200378418, \t Total Dis Loss : 0.0009227710543200374\n",
      "Steps : 41700, \t Total Gen Loss : 27.93356704711914, \t Total Dis Loss : 0.00037461897591128945\n",
      "Steps : 41800, \t Total Gen Loss : 27.423233032226562, \t Total Dis Loss : 0.0014280314790084958\n",
      "Steps : 41900, \t Total Gen Loss : 28.203596115112305, \t Total Dis Loss : 0.0008159267599694431\n",
      "Steps : 42000, \t Total Gen Loss : 27.53856086730957, \t Total Dis Loss : 0.0009625944658182561\n",
      "Steps : 42100, \t Total Gen Loss : 29.02614402770996, \t Total Dis Loss : 0.00032632675720378757\n",
      "Steps : 42200, \t Total Gen Loss : 29.573078155517578, \t Total Dis Loss : 0.00023203201999422163\n",
      "Steps : 42300, \t Total Gen Loss : 28.932029724121094, \t Total Dis Loss : 0.0003018885909114033\n",
      "Steps : 42400, \t Total Gen Loss : 30.594608306884766, \t Total Dis Loss : 0.00029444831307046115\n",
      "Steps : 42500, \t Total Gen Loss : 30.54004669189453, \t Total Dis Loss : 0.0001807025691960007\n",
      "Steps : 42600, \t Total Gen Loss : 30.62429428100586, \t Total Dis Loss : 0.00021731502783950418\n",
      "Steps : 42700, \t Total Gen Loss : 29.83871078491211, \t Total Dis Loss : 0.00018593104323372245\n",
      "Steps : 42800, \t Total Gen Loss : 31.09490966796875, \t Total Dis Loss : 0.0031480470206588507\n",
      "Steps : 42900, \t Total Gen Loss : 32.15856170654297, \t Total Dis Loss : 0.004513893276453018\n",
      "Steps : 43000, \t Total Gen Loss : 30.563304901123047, \t Total Dis Loss : 0.09361237287521362\n",
      "Steps : 43100, \t Total Gen Loss : 30.433290481567383, \t Total Dis Loss : 0.0007911655702628195\n",
      "Steps : 43200, \t Total Gen Loss : 31.46344757080078, \t Total Dis Loss : 0.00026932210312224925\n",
      "Steps : 43300, \t Total Gen Loss : 27.80434799194336, \t Total Dis Loss : 0.14265745878219604\n",
      "Steps : 43400, \t Total Gen Loss : 25.919940948486328, \t Total Dis Loss : 0.0009064365294761956\n",
      "Steps : 43500, \t Total Gen Loss : 29.535179138183594, \t Total Dis Loss : 0.000736852758564055\n",
      "Steps : 43600, \t Total Gen Loss : 30.40979766845703, \t Total Dis Loss : 0.000610350223723799\n",
      "Steps : 43700, \t Total Gen Loss : 31.68301010131836, \t Total Dis Loss : 0.00033036182867363095\n",
      "Steps : 43800, \t Total Gen Loss : 27.810041427612305, \t Total Dis Loss : 0.0010323742171749473\n",
      "Steps : 43900, \t Total Gen Loss : 26.288509368896484, \t Total Dis Loss : 0.0009446326293982565\n",
      "Steps : 44000, \t Total Gen Loss : 29.580842971801758, \t Total Dis Loss : 0.00034684661659412086\n",
      "Steps : 44100, \t Total Gen Loss : 29.201993942260742, \t Total Dis Loss : 0.000316073652356863\n",
      "Steps : 44200, \t Total Gen Loss : 29.098081588745117, \t Total Dis Loss : 0.0003215688921045512\n",
      "Steps : 44300, \t Total Gen Loss : 29.281200408935547, \t Total Dis Loss : 0.00026608025655150414\n",
      "Steps : 44400, \t Total Gen Loss : 30.838464736938477, \t Total Dis Loss : 0.0001880182680906728\n",
      "Steps : 44500, \t Total Gen Loss : 28.3734073638916, \t Total Dis Loss : 0.0008462148252874613\n",
      "Steps : 44600, \t Total Gen Loss : 30.396738052368164, \t Total Dis Loss : 0.0012857873225584626\n",
      "Steps : 44700, \t Total Gen Loss : 27.38404083251953, \t Total Dis Loss : 0.0007819067104719579\n",
      "Steps : 44800, \t Total Gen Loss : 27.087718963623047, \t Total Dis Loss : 0.0005044240970164537\n",
      "Steps : 44900, \t Total Gen Loss : 26.565393447875977, \t Total Dis Loss : 0.0020073233172297478\n",
      "Steps : 45000, \t Total Gen Loss : 29.815128326416016, \t Total Dis Loss : 0.00034482585033401847\n",
      "Steps : 45100, \t Total Gen Loss : 28.558996200561523, \t Total Dis Loss : 0.002605622634291649\n",
      "Steps : 45200, \t Total Gen Loss : 30.48255729675293, \t Total Dis Loss : 0.00023416579642798752\n",
      "Steps : 45300, \t Total Gen Loss : 28.563495635986328, \t Total Dis Loss : 0.008675242774188519\n",
      "Steps : 45400, \t Total Gen Loss : 29.38585090637207, \t Total Dis Loss : 0.003501882776618004\n",
      "Steps : 45500, \t Total Gen Loss : 30.804941177368164, \t Total Dis Loss : 0.0014171028742566705\n",
      "Steps : 45600, \t Total Gen Loss : 28.589082717895508, \t Total Dis Loss : 0.0010868519311770797\n",
      "Steps : 45700, \t Total Gen Loss : 27.522552490234375, \t Total Dis Loss : 0.0004625306755769998\n",
      "Steps : 45800, \t Total Gen Loss : 28.989383697509766, \t Total Dis Loss : 0.0004921897198073566\n",
      "Steps : 45900, \t Total Gen Loss : 25.483694076538086, \t Total Dis Loss : 0.0024380183313041925\n",
      "Steps : 46000, \t Total Gen Loss : 27.98641586303711, \t Total Dis Loss : 0.0008198644500225782\n",
      "Steps : 46100, \t Total Gen Loss : 28.262887954711914, \t Total Dis Loss : 0.000498287205118686\n",
      "Steps : 46200, \t Total Gen Loss : 28.90743064880371, \t Total Dis Loss : 0.0005147330812178552\n",
      "Steps : 46300, \t Total Gen Loss : 32.904762268066406, \t Total Dis Loss : 0.0007434376748278737\n",
      "Steps : 46400, \t Total Gen Loss : 33.37109375, \t Total Dis Loss : 0.02409234642982483\n",
      "Steps : 46500, \t Total Gen Loss : 28.224729537963867, \t Total Dis Loss : 0.0030392531771212816\n",
      "Steps : 46600, \t Total Gen Loss : 29.372737884521484, \t Total Dis Loss : 0.0016475910088047385\n",
      "Steps : 46700, \t Total Gen Loss : 29.50814437866211, \t Total Dis Loss : 0.0010614102939143777\n",
      "Steps : 46800, \t Total Gen Loss : 30.56073760986328, \t Total Dis Loss : 0.00016171150491572917\n",
      "Steps : 46900, \t Total Gen Loss : 28.757022857666016, \t Total Dis Loss : 0.0007679612608626485\n",
      "Steps : 47000, \t Total Gen Loss : 30.339719772338867, \t Total Dis Loss : 0.0006930599338375032\n",
      "Steps : 47100, \t Total Gen Loss : 28.917591094970703, \t Total Dis Loss : 0.00054606341291219\n",
      "Steps : 47200, \t Total Gen Loss : 31.018178939819336, \t Total Dis Loss : 0.0002788670244626701\n",
      "Time for epoch 7 is 345.65828132629395 sec\n",
      "Steps : 47300, \t Total Gen Loss : 29.992673873901367, \t Total Dis Loss : 0.002664222614839673\n",
      "Steps : 47400, \t Total Gen Loss : 28.768905639648438, \t Total Dis Loss : 0.004268775228410959\n",
      "Steps : 47500, \t Total Gen Loss : 31.164155960083008, \t Total Dis Loss : 0.0007475356687791646\n",
      "Steps : 47600, \t Total Gen Loss : 28.378021240234375, \t Total Dis Loss : 0.0012863778974860907\n",
      "Steps : 47700, \t Total Gen Loss : 28.453330993652344, \t Total Dis Loss : 0.0014864132972434163\n",
      "Steps : 47800, \t Total Gen Loss : 31.713886260986328, \t Total Dis Loss : 0.0006902636378072202\n",
      "Steps : 47900, \t Total Gen Loss : 30.1251163482666, \t Total Dis Loss : 0.005243957974016666\n",
      "Steps : 48000, \t Total Gen Loss : 28.591583251953125, \t Total Dis Loss : 0.0012219183845445514\n",
      "Steps : 48100, \t Total Gen Loss : 27.489269256591797, \t Total Dis Loss : 0.0013442935887724161\n",
      "Steps : 48200, \t Total Gen Loss : 27.054637908935547, \t Total Dis Loss : 0.0028701163828372955\n",
      "Steps : 48300, \t Total Gen Loss : 27.889341354370117, \t Total Dis Loss : 0.0011476732324808836\n",
      "Steps : 48400, \t Total Gen Loss : 27.832700729370117, \t Total Dis Loss : 0.01154717430472374\n",
      "Steps : 48500, \t Total Gen Loss : 28.879270553588867, \t Total Dis Loss : 0.0007262030267156661\n",
      "Steps : 48600, \t Total Gen Loss : 29.68029022216797, \t Total Dis Loss : 0.003078423673287034\n",
      "Steps : 48700, \t Total Gen Loss : 29.334842681884766, \t Total Dis Loss : 0.0007951376028358936\n",
      "Steps : 48800, \t Total Gen Loss : 27.534992218017578, \t Total Dis Loss : 0.0011210959637537599\n",
      "Steps : 48900, \t Total Gen Loss : 29.291879653930664, \t Total Dis Loss : 0.0006923817563802004\n",
      "Steps : 49000, \t Total Gen Loss : 27.667724609375, \t Total Dis Loss : 0.0008493651985190809\n",
      "Steps : 49100, \t Total Gen Loss : 27.310176849365234, \t Total Dis Loss : 0.0012126032961532474\n",
      "Steps : 49200, \t Total Gen Loss : 28.549449920654297, \t Total Dis Loss : 0.0010977881029248238\n",
      "Steps : 49300, \t Total Gen Loss : 28.872135162353516, \t Total Dis Loss : 0.0009720897069200873\n",
      "Steps : 49400, \t Total Gen Loss : 29.97488021850586, \t Total Dis Loss : 0.0006425335886888206\n",
      "Steps : 49500, \t Total Gen Loss : 27.779396057128906, \t Total Dis Loss : 0.0013788733631372452\n",
      "Steps : 49600, \t Total Gen Loss : 28.81048011779785, \t Total Dis Loss : 0.0018613954307511449\n",
      "Steps : 49700, \t Total Gen Loss : 29.721614837646484, \t Total Dis Loss : 0.0008188469219021499\n",
      "Steps : 49800, \t Total Gen Loss : 27.492952346801758, \t Total Dis Loss : 0.0006289341254159808\n",
      "Steps : 49900, \t Total Gen Loss : 27.14529800415039, \t Total Dis Loss : 0.001756326062604785\n",
      "Steps : 50000, \t Total Gen Loss : 30.317825317382812, \t Total Dis Loss : 0.00025179600925184786\n",
      "Steps : 50100, \t Total Gen Loss : 27.01768684387207, \t Total Dis Loss : 0.0007171396282501519\n",
      "Steps : 50200, \t Total Gen Loss : 26.65044403076172, \t Total Dis Loss : 0.0013762902235612273\n",
      "Steps : 50300, \t Total Gen Loss : 29.324325561523438, \t Total Dis Loss : 0.0003834619128610939\n",
      "Steps : 50400, \t Total Gen Loss : 25.440929412841797, \t Total Dis Loss : 0.001954152947291732\n",
      "Steps : 50500, \t Total Gen Loss : 28.789011001586914, \t Total Dis Loss : 0.000740338524337858\n",
      "Steps : 50600, \t Total Gen Loss : 32.09266662597656, \t Total Dis Loss : 0.0002140953001799062\n",
      "Steps : 50700, \t Total Gen Loss : 28.815448760986328, \t Total Dis Loss : 0.0014147697947919369\n",
      "Steps : 50800, \t Total Gen Loss : 28.53385353088379, \t Total Dis Loss : 0.00025051317061297596\n",
      "Steps : 50900, \t Total Gen Loss : 27.913509368896484, \t Total Dis Loss : 0.000528316420968622\n",
      "Steps : 51000, \t Total Gen Loss : 28.37335205078125, \t Total Dis Loss : 0.0010553888278082013\n",
      "Steps : 51100, \t Total Gen Loss : 29.31098747253418, \t Total Dis Loss : 0.0017265189671888947\n",
      "Steps : 51200, \t Total Gen Loss : 30.10707664489746, \t Total Dis Loss : 0.0005800723447464406\n",
      "Steps : 51300, \t Total Gen Loss : 28.7091007232666, \t Total Dis Loss : 0.005824616178870201\n",
      "Steps : 51400, \t Total Gen Loss : 29.238449096679688, \t Total Dis Loss : 0.0017540024127811193\n",
      "Steps : 51500, \t Total Gen Loss : 27.72575569152832, \t Total Dis Loss : 0.001779837068170309\n",
      "Steps : 51600, \t Total Gen Loss : 27.608863830566406, \t Total Dis Loss : 0.0007053750450722873\n",
      "Steps : 51700, \t Total Gen Loss : 26.2117862701416, \t Total Dis Loss : 0.003057220485061407\n",
      "Steps : 51800, \t Total Gen Loss : 29.106483459472656, \t Total Dis Loss : 0.0006270421436056495\n",
      "Steps : 51900, \t Total Gen Loss : 27.198657989501953, \t Total Dis Loss : 0.0004387756926007569\n",
      "Steps : 52000, \t Total Gen Loss : 27.489587783813477, \t Total Dis Loss : 0.0006112229311838746\n",
      "Steps : 52100, \t Total Gen Loss : 25.69234275817871, \t Total Dis Loss : 0.0004701611178461462\n",
      "Steps : 52200, \t Total Gen Loss : 29.592578887939453, \t Total Dis Loss : 0.000361399695975706\n",
      "Steps : 52300, \t Total Gen Loss : 29.656450271606445, \t Total Dis Loss : 0.0005507493042387068\n",
      "Steps : 52400, \t Total Gen Loss : 29.506807327270508, \t Total Dis Loss : 0.0004929615533910692\n",
      "Steps : 52500, \t Total Gen Loss : 27.72027015686035, \t Total Dis Loss : 0.0003630854480434209\n",
      "Steps : 52600, \t Total Gen Loss : 28.15692901611328, \t Total Dis Loss : 0.0007280840072780848\n",
      "Steps : 52700, \t Total Gen Loss : 26.31672477722168, \t Total Dis Loss : 0.30465877056121826\n",
      "Steps : 52800, \t Total Gen Loss : 29.082304000854492, \t Total Dis Loss : 0.0011820899089798331\n",
      "Steps : 52900, \t Total Gen Loss : 26.382007598876953, \t Total Dis Loss : 0.00099362269975245\n",
      "Steps : 53000, \t Total Gen Loss : 28.996395111083984, \t Total Dis Loss : 0.00039789354195818305\n",
      "Steps : 53100, \t Total Gen Loss : 30.06196403503418, \t Total Dis Loss : 0.0002962997823487967\n",
      "Steps : 53200, \t Total Gen Loss : 27.010562896728516, \t Total Dis Loss : 0.004108042921870947\n",
      "Steps : 53300, \t Total Gen Loss : 28.239320755004883, \t Total Dis Loss : 0.0004680130514316261\n",
      "Steps : 53400, \t Total Gen Loss : 29.647769927978516, \t Total Dis Loss : 0.00039300648495554924\n",
      "Steps : 53500, \t Total Gen Loss : 28.452285766601562, \t Total Dis Loss : 0.00046376592945307493\n",
      "Steps : 53600, \t Total Gen Loss : 27.877500534057617, \t Total Dis Loss : 0.0002745655656326562\n",
      "Steps : 53700, \t Total Gen Loss : 26.734006881713867, \t Total Dis Loss : 0.007123172283172607\n",
      "Steps : 53800, \t Total Gen Loss : 28.406606674194336, \t Total Dis Loss : 0.001992692705243826\n",
      "Steps : 53900, \t Total Gen Loss : 27.422752380371094, \t Total Dis Loss : 0.002875041216611862\n",
      "Steps : 54000, \t Total Gen Loss : 23.492111206054688, \t Total Dis Loss : 1.3700478076934814\n",
      "Time for epoch 8 is 344.34919834136963 sec\n",
      "Steps : 54100, \t Total Gen Loss : 27.382633209228516, \t Total Dis Loss : 0.0007740859291516244\n",
      "Steps : 54200, \t Total Gen Loss : 29.169334411621094, \t Total Dis Loss : 0.00027656348538585007\n",
      "Steps : 54300, \t Total Gen Loss : 27.927722930908203, \t Total Dis Loss : 0.000678465177770704\n",
      "Steps : 54400, \t Total Gen Loss : 28.608196258544922, \t Total Dis Loss : 0.00035868232953362167\n",
      "Steps : 54500, \t Total Gen Loss : 31.30551528930664, \t Total Dis Loss : 0.00020464662520680577\n",
      "Steps : 54600, \t Total Gen Loss : 32.1674919128418, \t Total Dis Loss : 0.00043755685328505933\n",
      "Steps : 54700, \t Total Gen Loss : 26.54104232788086, \t Total Dis Loss : 0.0009017347474582493\n",
      "Steps : 54800, \t Total Gen Loss : 29.012718200683594, \t Total Dis Loss : 0.03976796567440033\n",
      "Steps : 54900, \t Total Gen Loss : 28.266490936279297, \t Total Dis Loss : 0.0017696507275104523\n",
      "Steps : 55000, \t Total Gen Loss : 29.217350006103516, \t Total Dis Loss : 0.04336686432361603\n",
      "Steps : 55100, \t Total Gen Loss : 32.365577697753906, \t Total Dis Loss : 0.0006221753428690135\n",
      "Steps : 55200, \t Total Gen Loss : 31.063440322875977, \t Total Dis Loss : 0.0004939897335134447\n",
      "Steps : 55300, \t Total Gen Loss : 31.166385650634766, \t Total Dis Loss : 0.0013409213861450553\n",
      "Steps : 55400, \t Total Gen Loss : 29.863496780395508, \t Total Dis Loss : 0.0007996401400305331\n",
      "Steps : 55500, \t Total Gen Loss : 28.813331604003906, \t Total Dis Loss : 0.0010290599893778563\n",
      "Steps : 55600, \t Total Gen Loss : 32.1778564453125, \t Total Dis Loss : 0.001909601385705173\n",
      "Steps : 55700, \t Total Gen Loss : 28.930187225341797, \t Total Dis Loss : 0.006631354335695505\n",
      "Steps : 55800, \t Total Gen Loss : 34.66474914550781, \t Total Dis Loss : 0.003986209165304899\n",
      "Steps : 55900, \t Total Gen Loss : 33.343990325927734, \t Total Dis Loss : 0.00023131057969294488\n",
      "Steps : 56000, \t Total Gen Loss : 35.67890548706055, \t Total Dis Loss : 0.0014322965871542692\n",
      "Steps : 56100, \t Total Gen Loss : 31.810550689697266, \t Total Dis Loss : 0.0017580229323357344\n",
      "Steps : 56200, \t Total Gen Loss : 34.54285430908203, \t Total Dis Loss : 0.0002012614713748917\n",
      "Steps : 56300, \t Total Gen Loss : 31.146297454833984, \t Total Dis Loss : 0.0010778892319649458\n",
      "Steps : 56400, \t Total Gen Loss : 32.96284866333008, \t Total Dis Loss : 0.000548656505998224\n",
      "Steps : 56500, \t Total Gen Loss : 32.3039665222168, \t Total Dis Loss : 0.0015079495497047901\n",
      "Steps : 56600, \t Total Gen Loss : 32.146705627441406, \t Total Dis Loss : 0.00034455262357369065\n",
      "Steps : 56700, \t Total Gen Loss : 31.964017868041992, \t Total Dis Loss : 0.0008678854792378843\n",
      "Steps : 56800, \t Total Gen Loss : 31.113597869873047, \t Total Dis Loss : 0.0011622082674875855\n",
      "Steps : 56900, \t Total Gen Loss : 30.768877029418945, \t Total Dis Loss : 0.0006444380851462483\n",
      "Steps : 57000, \t Total Gen Loss : 29.91251564025879, \t Total Dis Loss : 0.011307671666145325\n",
      "Steps : 57100, \t Total Gen Loss : 30.179399490356445, \t Total Dis Loss : 0.000606889370828867\n",
      "Steps : 57200, \t Total Gen Loss : 30.429492950439453, \t Total Dis Loss : 0.0005819726502522826\n",
      "Steps : 57300, \t Total Gen Loss : 29.824434280395508, \t Total Dis Loss : 0.0028230163734406233\n",
      "Steps : 57400, \t Total Gen Loss : 28.38023567199707, \t Total Dis Loss : 0.00048492802307009697\n",
      "Steps : 57500, \t Total Gen Loss : 31.437267303466797, \t Total Dis Loss : 0.00045841297833248973\n",
      "Steps : 57600, \t Total Gen Loss : 28.3467960357666, \t Total Dis Loss : 0.0006333638448268175\n",
      "Steps : 57700, \t Total Gen Loss : 30.51045036315918, \t Total Dis Loss : 0.0005463238339871168\n",
      "Steps : 57800, \t Total Gen Loss : 29.276653289794922, \t Total Dis Loss : 0.000931450049392879\n",
      "Steps : 57900, \t Total Gen Loss : 30.600242614746094, \t Total Dis Loss : 0.00037672024336643517\n",
      "Steps : 58000, \t Total Gen Loss : 32.85337829589844, \t Total Dis Loss : 0.005850303918123245\n",
      "Steps : 58100, \t Total Gen Loss : 28.87410545349121, \t Total Dis Loss : 0.08059646934270859\n",
      "Steps : 58200, \t Total Gen Loss : 31.3365478515625, \t Total Dis Loss : 0.0010081883519887924\n",
      "Steps : 58300, \t Total Gen Loss : 32.810943603515625, \t Total Dis Loss : 0.001667236560024321\n",
      "Steps : 58400, \t Total Gen Loss : 29.739316940307617, \t Total Dis Loss : 0.001383794005960226\n",
      "Steps : 58500, \t Total Gen Loss : 30.16358184814453, \t Total Dis Loss : 0.16553832590579987\n",
      "Steps : 58600, \t Total Gen Loss : 31.224645614624023, \t Total Dis Loss : 0.00088504672748968\n",
      "Steps : 58700, \t Total Gen Loss : 30.699705123901367, \t Total Dis Loss : 0.0005421903333626688\n",
      "Steps : 58800, \t Total Gen Loss : 35.18376922607422, \t Total Dis Loss : 0.0002455090289004147\n",
      "Steps : 58900, \t Total Gen Loss : 30.50583839416504, \t Total Dis Loss : 0.000557691790163517\n",
      "Steps : 59000, \t Total Gen Loss : 29.833906173706055, \t Total Dis Loss : 0.0004221115377731621\n",
      "Steps : 59100, \t Total Gen Loss : 29.111953735351562, \t Total Dis Loss : 0.0005795853212475777\n",
      "Steps : 59200, \t Total Gen Loss : 28.12294578552246, \t Total Dis Loss : 0.0008167690830305219\n",
      "Steps : 59300, \t Total Gen Loss : 28.677644729614258, \t Total Dis Loss : 0.00042139191646128893\n",
      "Steps : 59400, \t Total Gen Loss : 29.81279182434082, \t Total Dis Loss : 0.0002447670267429203\n",
      "Steps : 59500, \t Total Gen Loss : 28.201881408691406, \t Total Dis Loss : 0.00039629824459552765\n",
      "Steps : 59600, \t Total Gen Loss : 31.233064651489258, \t Total Dis Loss : 0.0003337104863021523\n",
      "Steps : 59700, \t Total Gen Loss : 32.09944152832031, \t Total Dis Loss : 0.00015860964776948094\n",
      "Steps : 59800, \t Total Gen Loss : 29.376361846923828, \t Total Dis Loss : 0.0021974965929985046\n",
      "Steps : 59900, \t Total Gen Loss : 31.49420928955078, \t Total Dis Loss : 0.0008286446100100875\n",
      "Steps : 60000, \t Total Gen Loss : 30.99938201904297, \t Total Dis Loss : 0.0003857376577798277\n",
      "Steps : 60100, \t Total Gen Loss : 30.230955123901367, \t Total Dis Loss : 0.0013392395339906216\n",
      "Steps : 60200, \t Total Gen Loss : 28.801347732543945, \t Total Dis Loss : 0.0005355831235647202\n",
      "Steps : 60300, \t Total Gen Loss : 29.225658416748047, \t Total Dis Loss : 0.0006391414790414274\n",
      "Steps : 60400, \t Total Gen Loss : 29.145732879638672, \t Total Dis Loss : 0.002570509444922209\n",
      "Steps : 60500, \t Total Gen Loss : 29.00833511352539, \t Total Dis Loss : 0.0005698951426893473\n",
      "Steps : 60600, \t Total Gen Loss : 32.2396240234375, \t Total Dis Loss : 0.0004478107439354062\n",
      "Steps : 60700, \t Total Gen Loss : 31.720788955688477, \t Total Dis Loss : 0.0001220578997163102\n",
      "Time for epoch 9 is 345.28432989120483 sec\n",
      "Steps : 60800, \t Total Gen Loss : 29.83320426940918, \t Total Dis Loss : 0.011252093128859997\n",
      "Steps : 60900, \t Total Gen Loss : 30.06524658203125, \t Total Dis Loss : 0.0002912640920840204\n",
      "Steps : 61000, \t Total Gen Loss : 27.77386474609375, \t Total Dis Loss : 0.0007140099187381566\n",
      "Steps : 61100, \t Total Gen Loss : 29.6442928314209, \t Total Dis Loss : 0.0006557673332281411\n",
      "Steps : 61200, \t Total Gen Loss : 31.07492446899414, \t Total Dis Loss : 9.089722152566537e-05\n",
      "Steps : 61300, \t Total Gen Loss : 29.843605041503906, \t Total Dis Loss : 0.0008421698003076017\n",
      "Steps : 61400, \t Total Gen Loss : 28.589344024658203, \t Total Dis Loss : 0.00033926096512004733\n",
      "Steps : 61500, \t Total Gen Loss : 27.105716705322266, \t Total Dis Loss : 0.03665720298886299\n",
      "Steps : 61600, \t Total Gen Loss : 36.461639404296875, \t Total Dis Loss : 0.06232599541544914\n",
      "Steps : 61700, \t Total Gen Loss : 25.10443115234375, \t Total Dis Loss : 0.9886578321456909\n",
      "Steps : 61800, \t Total Gen Loss : 29.076900482177734, \t Total Dis Loss : 0.00826805830001831\n",
      "Steps : 61900, \t Total Gen Loss : 25.449207305908203, \t Total Dis Loss : 0.010036285035312176\n",
      "Steps : 62000, \t Total Gen Loss : 26.996109008789062, \t Total Dis Loss : 0.0048841023817658424\n",
      "Steps : 62100, \t Total Gen Loss : 27.275609970092773, \t Total Dis Loss : 0.00472728256136179\n",
      "Steps : 62200, \t Total Gen Loss : 26.133390426635742, \t Total Dis Loss : 0.003289694432169199\n",
      "Steps : 62300, \t Total Gen Loss : 27.537452697753906, \t Total Dis Loss : 0.003575559938326478\n",
      "Steps : 62400, \t Total Gen Loss : 29.313215255737305, \t Total Dis Loss : 0.0019205121789127588\n",
      "Steps : 62500, \t Total Gen Loss : 27.301795959472656, \t Total Dis Loss : 0.0062240120023489\n",
      "Steps : 62600, \t Total Gen Loss : 26.73944854736328, \t Total Dis Loss : 0.004176427144557238\n",
      "Steps : 62700, \t Total Gen Loss : 26.803009033203125, \t Total Dis Loss : 0.006355762016028166\n",
      "Steps : 62800, \t Total Gen Loss : 26.32830238342285, \t Total Dis Loss : 0.002896909136325121\n",
      "Steps : 62900, \t Total Gen Loss : 26.841148376464844, \t Total Dis Loss : 0.000983936944976449\n",
      "Steps : 63000, \t Total Gen Loss : 26.948612213134766, \t Total Dis Loss : 0.05112791061401367\n",
      "Steps : 63100, \t Total Gen Loss : 29.678892135620117, \t Total Dis Loss : 0.0025406195782124996\n",
      "Steps : 63200, \t Total Gen Loss : 27.317523956298828, \t Total Dis Loss : 0.004490590654313564\n",
      "Steps : 63300, \t Total Gen Loss : 28.20244026184082, \t Total Dis Loss : 0.000659692392218858\n",
      "Steps : 63400, \t Total Gen Loss : 31.1375732421875, \t Total Dis Loss : 0.0013290682109072804\n",
      "Steps : 63500, \t Total Gen Loss : 28.40235137939453, \t Total Dis Loss : 0.0002804849937092513\n",
      "Steps : 63600, \t Total Gen Loss : 28.764244079589844, \t Total Dis Loss : 0.0006519757444038987\n",
      "Steps : 63700, \t Total Gen Loss : 30.39739227294922, \t Total Dis Loss : 0.0003096635336987674\n",
      "Steps : 63800, \t Total Gen Loss : 28.473819732666016, \t Total Dis Loss : 0.0037203894462436438\n",
      "Steps : 63900, \t Total Gen Loss : 30.765533447265625, \t Total Dis Loss : 0.0004696973483078182\n",
      "Steps : 64000, \t Total Gen Loss : 30.67593765258789, \t Total Dis Loss : 0.00036112318048253655\n",
      "Steps : 64100, \t Total Gen Loss : 26.60731315612793, \t Total Dis Loss : 0.4730149209499359\n",
      "Steps : 64200, \t Total Gen Loss : 28.261396408081055, \t Total Dis Loss : 0.0012029256904497743\n",
      "Steps : 64300, \t Total Gen Loss : 26.128753662109375, \t Total Dis Loss : 0.0022274248767644167\n",
      "Steps : 64400, \t Total Gen Loss : 28.832374572753906, \t Total Dis Loss : 0.00047176709631457925\n",
      "Steps : 64500, \t Total Gen Loss : 30.736907958984375, \t Total Dis Loss : 0.0006387397297658026\n",
      "Steps : 64600, \t Total Gen Loss : 30.562326431274414, \t Total Dis Loss : 0.0005263196071609855\n",
      "Steps : 64700, \t Total Gen Loss : 28.09003448486328, \t Total Dis Loss : 0.0003616150643210858\n",
      "Steps : 64800, \t Total Gen Loss : 29.554668426513672, \t Total Dis Loss : 0.00036132021341472864\n",
      "Steps : 64900, \t Total Gen Loss : 29.01141357421875, \t Total Dis Loss : 0.00013790768571197987\n",
      "Steps : 65000, \t Total Gen Loss : 26.755067825317383, \t Total Dis Loss : 0.0011152566876262426\n",
      "Steps : 65100, \t Total Gen Loss : 30.13509750366211, \t Total Dis Loss : 0.0006279184017330408\n",
      "Steps : 65200, \t Total Gen Loss : 28.640981674194336, \t Total Dis Loss : 0.00046212709276005626\n",
      "Steps : 65300, \t Total Gen Loss : 30.854782104492188, \t Total Dis Loss : 0.00030538294231519103\n",
      "Steps : 65400, \t Total Gen Loss : 31.49635124206543, \t Total Dis Loss : 0.0004696112882811576\n",
      "Steps : 65500, \t Total Gen Loss : 28.516040802001953, \t Total Dis Loss : 0.0004882966168224812\n",
      "Steps : 65600, \t Total Gen Loss : 28.458154678344727, \t Total Dis Loss : 0.0008156515541486442\n",
      "Steps : 65700, \t Total Gen Loss : 31.396709442138672, \t Total Dis Loss : 0.00018193587311543524\n",
      "Steps : 65800, \t Total Gen Loss : 29.313093185424805, \t Total Dis Loss : 0.0008086182060651481\n",
      "Steps : 65900, \t Total Gen Loss : 26.11658477783203, \t Total Dis Loss : 0.004691711626946926\n",
      "Steps : 66000, \t Total Gen Loss : 29.258567810058594, \t Total Dis Loss : 0.001960141584277153\n",
      "Steps : 66100, \t Total Gen Loss : 30.276874542236328, \t Total Dis Loss : 0.0019887429662048817\n",
      "Steps : 66200, \t Total Gen Loss : 30.60767936706543, \t Total Dis Loss : 0.002093567978590727\n",
      "Steps : 66300, \t Total Gen Loss : 28.044540405273438, \t Total Dis Loss : 0.0014753364957869053\n",
      "Steps : 66400, \t Total Gen Loss : 30.54251480102539, \t Total Dis Loss : 0.0010888861725106835\n",
      "Steps : 66500, \t Total Gen Loss : 29.704872131347656, \t Total Dis Loss : 0.0007392051629722118\n",
      "Steps : 66600, \t Total Gen Loss : 27.71234893798828, \t Total Dis Loss : 0.002858574967831373\n",
      "Steps : 66700, \t Total Gen Loss : 31.120807647705078, \t Total Dis Loss : 0.0004805641365237534\n",
      "Steps : 66800, \t Total Gen Loss : 28.825916290283203, \t Total Dis Loss : 0.0007755111437290907\n",
      "Steps : 66900, \t Total Gen Loss : 27.35626792907715, \t Total Dis Loss : 0.00509137287735939\n",
      "Steps : 67000, \t Total Gen Loss : 30.10862922668457, \t Total Dis Loss : 0.00045744871022179723\n",
      "Steps : 67100, \t Total Gen Loss : 30.609046936035156, \t Total Dis Loss : 0.00043048072257079184\n",
      "Steps : 67200, \t Total Gen Loss : 30.9742488861084, \t Total Dis Loss : 0.0005488673923537135\n",
      "Steps : 67300, \t Total Gen Loss : 27.827863693237305, \t Total Dis Loss : 0.0007215965306386352\n",
      "Steps : 67400, \t Total Gen Loss : 31.379091262817383, \t Total Dis Loss : 0.01488109678030014\n",
      "Steps : 67500, \t Total Gen Loss : 27.885665893554688, \t Total Dis Loss : 0.0022546693217009306\n",
      "Time for epoch 10 is 344.84235548973083 sec\n",
      "Steps : 67600, \t Total Gen Loss : 29.42633819580078, \t Total Dis Loss : 0.0007194951758719981\n",
      "Steps : 67700, \t Total Gen Loss : 29.88077735900879, \t Total Dis Loss : 0.004762090742588043\n",
      "Steps : 67800, \t Total Gen Loss : 30.112041473388672, \t Total Dis Loss : 0.00045095395762473345\n",
      "Steps : 67900, \t Total Gen Loss : 28.436283111572266, \t Total Dis Loss : 0.002313581295311451\n",
      "Steps : 68000, \t Total Gen Loss : 29.634572982788086, \t Total Dis Loss : 0.0006479756557382643\n",
      "Steps : 68100, \t Total Gen Loss : 28.343521118164062, \t Total Dis Loss : 0.0009370142361149192\n",
      "Steps : 68200, \t Total Gen Loss : 32.3587646484375, \t Total Dis Loss : 4.360726597951725e-05\n",
      "Steps : 68300, \t Total Gen Loss : 31.695480346679688, \t Total Dis Loss : 0.000196104942006059\n",
      "Steps : 68400, \t Total Gen Loss : 28.736360549926758, \t Total Dis Loss : 0.0009796543745324016\n",
      "Steps : 68500, \t Total Gen Loss : 26.866683959960938, \t Total Dis Loss : 0.0008752748253755271\n",
      "Steps : 68600, \t Total Gen Loss : 28.61086654663086, \t Total Dis Loss : 0.02739473059773445\n",
      "Steps : 68700, \t Total Gen Loss : 29.556976318359375, \t Total Dis Loss : 0.0005596335395239294\n",
      "Steps : 68800, \t Total Gen Loss : 30.126630783081055, \t Total Dis Loss : 0.00020208893693052232\n",
      "Steps : 68900, \t Total Gen Loss : 27.894432067871094, \t Total Dis Loss : 0.0002800689544528723\n",
      "Steps : 69000, \t Total Gen Loss : 29.675752639770508, \t Total Dis Loss : 0.00028363597812131047\n",
      "Steps : 69100, \t Total Gen Loss : 30.646167755126953, \t Total Dis Loss : 0.0001651819475227967\n",
      "Steps : 69200, \t Total Gen Loss : 28.423288345336914, \t Total Dis Loss : 0.00026902620447799563\n",
      "Steps : 69300, \t Total Gen Loss : 24.05709457397461, \t Total Dis Loss : 0.048374637961387634\n",
      "Steps : 69400, \t Total Gen Loss : 29.935680389404297, \t Total Dis Loss : 0.0004123496182728559\n",
      "Steps : 69500, \t Total Gen Loss : 31.844539642333984, \t Total Dis Loss : 0.00017902777472045273\n",
      "Steps : 69600, \t Total Gen Loss : 28.93144989013672, \t Total Dis Loss : 0.00020541803678497672\n",
      "Steps : 69700, \t Total Gen Loss : 30.208450317382812, \t Total Dis Loss : 0.00019065962987951934\n",
      "Steps : 69800, \t Total Gen Loss : 27.930221557617188, \t Total Dis Loss : 0.0002330422430532053\n",
      "Steps : 69900, \t Total Gen Loss : 34.67137145996094, \t Total Dis Loss : 4.3441043089842424e-05\n",
      "Steps : 70000, \t Total Gen Loss : 29.881439208984375, \t Total Dis Loss : 0.00042533763917163014\n",
      "Steps : 70100, \t Total Gen Loss : 29.145915985107422, \t Total Dis Loss : 0.0005366679979488254\n",
      "Steps : 70200, \t Total Gen Loss : 29.493947982788086, \t Total Dis Loss : 0.0002893205382861197\n",
      "Steps : 70300, \t Total Gen Loss : 31.829490661621094, \t Total Dis Loss : 7.766658382024616e-05\n",
      "Steps : 70400, \t Total Gen Loss : 31.663896560668945, \t Total Dis Loss : 0.0002461883414071053\n",
      "Steps : 70500, \t Total Gen Loss : 30.885761260986328, \t Total Dis Loss : 0.002584196161478758\n",
      "Steps : 70600, \t Total Gen Loss : 27.947025299072266, \t Total Dis Loss : 0.00020478537771850824\n",
      "Steps : 70700, \t Total Gen Loss : 30.001941680908203, \t Total Dis Loss : 0.0001659932459006086\n",
      "Steps : 70800, \t Total Gen Loss : 32.85165023803711, \t Total Dis Loss : 0.0002985896426253021\n",
      "Steps : 70900, \t Total Gen Loss : 31.562854766845703, \t Total Dis Loss : 0.00012297632929403335\n",
      "Steps : 71000, \t Total Gen Loss : 31.61402702331543, \t Total Dis Loss : 8.804899698588997e-05\n",
      "Steps : 71100, \t Total Gen Loss : 30.608585357666016, \t Total Dis Loss : 0.00014104117872193456\n",
      "Steps : 71200, \t Total Gen Loss : 30.750391006469727, \t Total Dis Loss : 0.00019276380771771073\n",
      "Steps : 71300, \t Total Gen Loss : 30.396665573120117, \t Total Dis Loss : 0.00012550910469144583\n",
      "Steps : 71400, \t Total Gen Loss : 27.889665603637695, \t Total Dis Loss : 0.0005371461156755686\n",
      "Steps : 71500, \t Total Gen Loss : 30.13909912109375, \t Total Dis Loss : 0.00034825989860109985\n",
      "Steps : 71600, \t Total Gen Loss : 29.6937255859375, \t Total Dis Loss : 0.0006671133451163769\n",
      "Steps : 71700, \t Total Gen Loss : 28.704275131225586, \t Total Dis Loss : 0.0006865830509923398\n",
      "Steps : 71800, \t Total Gen Loss : 29.78437614440918, \t Total Dis Loss : 0.0003311405598651618\n",
      "Steps : 71900, \t Total Gen Loss : 29.012889862060547, \t Total Dis Loss : 0.0012494189431890845\n",
      "Steps : 72000, \t Total Gen Loss : 28.9215087890625, \t Total Dis Loss : 0.000530339078977704\n",
      "Steps : 72100, \t Total Gen Loss : 26.94110870361328, \t Total Dis Loss : 0.0021480759605765343\n",
      "Steps : 72200, \t Total Gen Loss : 28.33879852294922, \t Total Dis Loss : 0.0006067718495614827\n",
      "Steps : 72300, \t Total Gen Loss : 29.961490631103516, \t Total Dis Loss : 0.04480675980448723\n",
      "Steps : 72400, \t Total Gen Loss : 26.79083251953125, \t Total Dis Loss : 0.0037569915875792503\n",
      "Steps : 72500, \t Total Gen Loss : 28.6517333984375, \t Total Dis Loss : 0.00026150018675252795\n",
      "Steps : 72600, \t Total Gen Loss : 25.316301345825195, \t Total Dis Loss : 0.017533529549837112\n",
      "Steps : 72700, \t Total Gen Loss : 27.57708740234375, \t Total Dis Loss : 0.0003771625633817166\n",
      "Steps : 72800, \t Total Gen Loss : 27.481847763061523, \t Total Dis Loss : 0.0027023565489798784\n",
      "Steps : 72900, \t Total Gen Loss : 29.435810089111328, \t Total Dis Loss : 0.0014195676194503903\n",
      "Steps : 73000, \t Total Gen Loss : 29.219146728515625, \t Total Dis Loss : 0.0007649336475878954\n",
      "Steps : 73100, \t Total Gen Loss : 28.305530548095703, \t Total Dis Loss : 0.0007452963036485016\n",
      "Steps : 73200, \t Total Gen Loss : 27.326547622680664, \t Total Dis Loss : 0.0002791999722830951\n",
      "Steps : 73300, \t Total Gen Loss : 26.82596778869629, \t Total Dis Loss : 0.0008466072613373399\n",
      "Steps : 73400, \t Total Gen Loss : 29.913637161254883, \t Total Dis Loss : 0.00019101068028248847\n",
      "Steps : 73500, \t Total Gen Loss : 30.91152000427246, \t Total Dis Loss : 0.00019698055984918028\n",
      "Steps : 73600, \t Total Gen Loss : 29.001068115234375, \t Total Dis Loss : 0.00012157506716903299\n",
      "Steps : 73700, \t Total Gen Loss : 27.07282829284668, \t Total Dis Loss : 0.00016536629118490964\n",
      "Steps : 73800, \t Total Gen Loss : 27.446792602539062, \t Total Dis Loss : 0.002026910427957773\n",
      "Steps : 73900, \t Total Gen Loss : 35.02824783325195, \t Total Dis Loss : 0.00016955373575910926\n",
      "Steps : 74000, \t Total Gen Loss : 32.09551239013672, \t Total Dis Loss : 0.006321657449007034\n",
      "Steps : 74100, \t Total Gen Loss : 29.123397827148438, \t Total Dis Loss : 0.00017425073019694537\n",
      "Steps : 74200, \t Total Gen Loss : 28.06214714050293, \t Total Dis Loss : 0.000448393082479015\n",
      "Time for epoch 11 is 349.29412627220154 sec\n",
      "Steps : 74300, \t Total Gen Loss : 29.56052589416504, \t Total Dis Loss : 0.00022477543097920716\n",
      "Steps : 74400, \t Total Gen Loss : 29.95010757446289, \t Total Dis Loss : 0.0003272305184509605\n",
      "Steps : 74500, \t Total Gen Loss : 29.84004783630371, \t Total Dis Loss : 0.00017504968855064362\n",
      "Steps : 74600, \t Total Gen Loss : 30.86455726623535, \t Total Dis Loss : 8.829116268316284e-05\n",
      "Steps : 74700, \t Total Gen Loss : 29.630281448364258, \t Total Dis Loss : 0.0003411505895201117\n",
      "Steps : 74800, \t Total Gen Loss : 31.739456176757812, \t Total Dis Loss : 0.0006145787774585187\n",
      "Steps : 74900, \t Total Gen Loss : 26.433595657348633, \t Total Dis Loss : 0.0037796308752149343\n",
      "Steps : 75000, \t Total Gen Loss : 27.5433349609375, \t Total Dis Loss : 0.0003705247363541275\n",
      "Steps : 75100, \t Total Gen Loss : 31.002758026123047, \t Total Dis Loss : 0.000549514836166054\n",
      "Steps : 75200, \t Total Gen Loss : 31.22999382019043, \t Total Dis Loss : 6.778634269721806e-05\n",
      "Steps : 75300, \t Total Gen Loss : 31.295909881591797, \t Total Dis Loss : 0.0012030218495056033\n",
      "Steps : 75400, \t Total Gen Loss : 30.940053939819336, \t Total Dis Loss : 7.505893154302612e-05\n",
      "Steps : 75500, \t Total Gen Loss : 31.987442016601562, \t Total Dis Loss : 0.00010128319263458252\n",
      "Steps : 75600, \t Total Gen Loss : 30.925107955932617, \t Total Dis Loss : 0.002456856891512871\n",
      "Steps : 75700, \t Total Gen Loss : 28.17083168029785, \t Total Dis Loss : 0.0013061064528301358\n",
      "Steps : 75800, \t Total Gen Loss : 29.388961791992188, \t Total Dis Loss : 0.0004592015757225454\n",
      "Steps : 75900, \t Total Gen Loss : 29.128557205200195, \t Total Dis Loss : 0.0001891858846647665\n",
      "Steps : 76000, \t Total Gen Loss : 27.38093376159668, \t Total Dis Loss : 0.0024553099647164345\n",
      "Steps : 76100, \t Total Gen Loss : 28.320844650268555, \t Total Dis Loss : 0.00036341839586384594\n",
      "Steps : 76200, \t Total Gen Loss : 30.329442977905273, \t Total Dis Loss : 0.0004464106459636241\n",
      "Steps : 76300, \t Total Gen Loss : 29.51575469970703, \t Total Dis Loss : 0.0002819506044033915\n",
      "Steps : 76400, \t Total Gen Loss : 29.023971557617188, \t Total Dis Loss : 0.00016300406423397362\n",
      "Steps : 76500, \t Total Gen Loss : 31.724842071533203, \t Total Dis Loss : 6.820356793468818e-05\n",
      "Steps : 76600, \t Total Gen Loss : 29.289470672607422, \t Total Dis Loss : 0.00042995100375264883\n",
      "Steps : 76700, \t Total Gen Loss : 30.500560760498047, \t Total Dis Loss : 0.00020949661848135293\n",
      "Steps : 76800, \t Total Gen Loss : 30.81684684753418, \t Total Dis Loss : 0.0002418920921627432\n",
      "Steps : 76900, \t Total Gen Loss : 27.638145446777344, \t Total Dis Loss : 0.0003861459845211357\n",
      "Steps : 77000, \t Total Gen Loss : 27.244138717651367, \t Total Dis Loss : 0.00016424950445070863\n",
      "Steps : 77100, \t Total Gen Loss : 28.504135131835938, \t Total Dis Loss : 0.00016934530867729336\n",
      "Steps : 77200, \t Total Gen Loss : 29.629724502563477, \t Total Dis Loss : 0.0007337753195315599\n",
      "Steps : 77300, \t Total Gen Loss : 26.70903968811035, \t Total Dis Loss : 0.0010355307022109628\n",
      "Steps : 77400, \t Total Gen Loss : 32.3802375793457, \t Total Dis Loss : 5.765343303210102e-05\n",
      "Steps : 77500, \t Total Gen Loss : 30.627979278564453, \t Total Dis Loss : 0.00011905516294064\n",
      "Steps : 77600, \t Total Gen Loss : 27.529508590698242, \t Total Dis Loss : 0.0012203966034576297\n",
      "Steps : 77700, \t Total Gen Loss : 30.505678176879883, \t Total Dis Loss : 0.0009423040319234133\n",
      "Steps : 77800, \t Total Gen Loss : 29.61486053466797, \t Total Dis Loss : 0.00014059610839467496\n",
      "Steps : 77900, \t Total Gen Loss : 28.967985153198242, \t Total Dis Loss : 0.00023469998268410563\n",
      "Steps : 78000, \t Total Gen Loss : 30.570924758911133, \t Total Dis Loss : 0.001008148887194693\n",
      "Steps : 78100, \t Total Gen Loss : 29.688278198242188, \t Total Dis Loss : 0.00010168724111281335\n",
      "Steps : 78200, \t Total Gen Loss : 30.280019760131836, \t Total Dis Loss : 0.00019191199680790305\n",
      "Steps : 78300, \t Total Gen Loss : 31.742183685302734, \t Total Dis Loss : 4.976828859071247e-05\n",
      "Steps : 78400, \t Total Gen Loss : 30.79713249206543, \t Total Dis Loss : 0.0011139425914734602\n",
      "Steps : 78500, \t Total Gen Loss : 29.568660736083984, \t Total Dis Loss : 0.0018361874390393496\n",
      "Steps : 78600, \t Total Gen Loss : 31.853696823120117, \t Total Dis Loss : 0.0010951166041195393\n",
      "Steps : 78700, \t Total Gen Loss : 32.751731872558594, \t Total Dis Loss : 0.00026116479421034455\n",
      "Steps : 78800, \t Total Gen Loss : 31.189512252807617, \t Total Dis Loss : 0.00010476020543137565\n",
      "Steps : 78900, \t Total Gen Loss : 33.105224609375, \t Total Dis Loss : 0.005304212216287851\n",
      "Steps : 79000, \t Total Gen Loss : 32.344852447509766, \t Total Dis Loss : 0.0001113252219511196\n",
      "Steps : 79100, \t Total Gen Loss : 33.498268127441406, \t Total Dis Loss : 0.00022380604059435427\n",
      "Steps : 79200, \t Total Gen Loss : 32.30595397949219, \t Total Dis Loss : 9.229648276232183e-05\n",
      "Steps : 79300, \t Total Gen Loss : 29.39753532409668, \t Total Dis Loss : 0.0005825442494824529\n",
      "Steps : 79400, \t Total Gen Loss : 33.23822784423828, \t Total Dis Loss : 0.00011164980242028832\n",
      "Steps : 79500, \t Total Gen Loss : 30.004596710205078, \t Total Dis Loss : 0.0003191754803992808\n",
      "Steps : 79600, \t Total Gen Loss : 29.713022232055664, \t Total Dis Loss : 0.0005403888644650578\n",
      "Steps : 79700, \t Total Gen Loss : 30.54941177368164, \t Total Dis Loss : 0.001770734554156661\n",
      "Steps : 79800, \t Total Gen Loss : 30.33749771118164, \t Total Dis Loss : 0.09067311882972717\n",
      "Steps : 79900, \t Total Gen Loss : 33.95977783203125, \t Total Dis Loss : 0.000690433953423053\n",
      "Steps : 80000, \t Total Gen Loss : 34.549835205078125, \t Total Dis Loss : 0.026936043053865433\n",
      "Steps : 80100, \t Total Gen Loss : 36.63410949707031, \t Total Dis Loss : 6.590096018044278e-05\n",
      "Steps : 80200, \t Total Gen Loss : 33.624778747558594, \t Total Dis Loss : 0.0025263712741434574\n",
      "Steps : 80300, \t Total Gen Loss : 33.218589782714844, \t Total Dis Loss : 0.00035266904160380363\n",
      "Steps : 80400, \t Total Gen Loss : 33.83142852783203, \t Total Dis Loss : 0.00021008688781876117\n",
      "Steps : 80500, \t Total Gen Loss : 35.0907096862793, \t Total Dis Loss : 0.0002792453160509467\n",
      "Steps : 80600, \t Total Gen Loss : 31.397863388061523, \t Total Dis Loss : 0.0022235075011849403\n",
      "Steps : 80700, \t Total Gen Loss : 32.777164459228516, \t Total Dis Loss : 0.000771587947383523\n",
      "Steps : 80800, \t Total Gen Loss : 31.51586151123047, \t Total Dis Loss : 0.0089010214433074\n",
      "Steps : 80900, \t Total Gen Loss : 33.928794860839844, \t Total Dis Loss : 0.001433986471965909\n",
      "Steps : 81000, \t Total Gen Loss : 34.70964050292969, \t Total Dis Loss : 0.0002704174257814884\n",
      "Time for epoch 12 is 343.17110776901245 sec\n",
      "Steps : 81100, \t Total Gen Loss : 34.13136291503906, \t Total Dis Loss : 0.00020942841365467757\n",
      "Steps : 81200, \t Total Gen Loss : 34.57683181762695, \t Total Dis Loss : 0.00029698078287765384\n",
      "Steps : 81300, \t Total Gen Loss : 34.408546447753906, \t Total Dis Loss : 0.00010840639879461378\n",
      "Steps : 81400, \t Total Gen Loss : 35.35081481933594, \t Total Dis Loss : 0.0007372670224867761\n",
      "Steps : 81500, \t Total Gen Loss : 34.46503829956055, \t Total Dis Loss : 9.16806748136878e-05\n",
      "Steps : 81600, \t Total Gen Loss : 32.59708786010742, \t Total Dis Loss : 0.00014739470498170704\n",
      "Steps : 81700, \t Total Gen Loss : 33.18927764892578, \t Total Dis Loss : 0.0002119417185895145\n",
      "Steps : 81800, \t Total Gen Loss : 30.158035278320312, \t Total Dis Loss : 0.0010513667948544025\n",
      "Steps : 81900, \t Total Gen Loss : 29.721763610839844, \t Total Dis Loss : 0.0020595102105289698\n",
      "Steps : 82000, \t Total Gen Loss : 32.979923248291016, \t Total Dis Loss : 8.669323869980872e-05\n",
      "Steps : 82100, \t Total Gen Loss : 32.9008903503418, \t Total Dis Loss : 0.00015540547610726207\n",
      "Steps : 82200, \t Total Gen Loss : 32.43661880493164, \t Total Dis Loss : 0.0010621288092806935\n",
      "Steps : 82300, \t Total Gen Loss : 33.17639923095703, \t Total Dis Loss : 0.00014407845446839929\n",
      "Steps : 82400, \t Total Gen Loss : 29.109882354736328, \t Total Dis Loss : 0.0022474010474979877\n",
      "Steps : 82500, \t Total Gen Loss : 30.527755737304688, \t Total Dis Loss : 0.00026045070262625813\n",
      "Steps : 82600, \t Total Gen Loss : 32.106468200683594, \t Total Dis Loss : 0.00041485746623948216\n",
      "Steps : 82700, \t Total Gen Loss : 31.62521743774414, \t Total Dis Loss : 0.000114421833131928\n",
      "Steps : 82800, \t Total Gen Loss : 32.375450134277344, \t Total Dis Loss : 7.368205115199089e-05\n",
      "Steps : 82900, \t Total Gen Loss : 30.525619506835938, \t Total Dis Loss : 0.009952148422598839\n",
      "Steps : 83000, \t Total Gen Loss : 31.97473907470703, \t Total Dis Loss : 0.0005274617578834295\n",
      "Steps : 83100, \t Total Gen Loss : 33.86550521850586, \t Total Dis Loss : 0.0003520580066833645\n",
      "Steps : 83200, \t Total Gen Loss : 31.79789161682129, \t Total Dis Loss : 0.0006694333860650659\n",
      "Steps : 83300, \t Total Gen Loss : 33.63862609863281, \t Total Dis Loss : 0.00014462297258432955\n",
      "Steps : 83400, \t Total Gen Loss : 31.057422637939453, \t Total Dis Loss : 7.426810043398291e-05\n",
      "Steps : 83500, \t Total Gen Loss : 33.649173736572266, \t Total Dis Loss : 3.61008605977986e-05\n",
      "Steps : 83600, \t Total Gen Loss : 32.743553161621094, \t Total Dis Loss : 6.618566112592816e-05\n",
      "Steps : 83700, \t Total Gen Loss : 30.874338150024414, \t Total Dis Loss : 0.00012328448065090925\n",
      "Steps : 83800, \t Total Gen Loss : 31.053993225097656, \t Total Dis Loss : 0.00014576644753105938\n",
      "Steps : 83900, \t Total Gen Loss : 29.611244201660156, \t Total Dis Loss : 0.00044938866631127894\n",
      "Steps : 84000, \t Total Gen Loss : 27.801393508911133, \t Total Dis Loss : 0.00021989176457282156\n",
      "Steps : 84100, \t Total Gen Loss : 29.935882568359375, \t Total Dis Loss : 0.005254622083157301\n",
      "Steps : 84200, \t Total Gen Loss : 28.208349227905273, \t Total Dis Loss : 0.0003261567326262593\n",
      "Steps : 84300, \t Total Gen Loss : 28.484210968017578, \t Total Dis Loss : 0.00022379402071237564\n",
      "Steps : 84400, \t Total Gen Loss : 29.371858596801758, \t Total Dis Loss : 0.0001242433354491368\n",
      "Steps : 84500, \t Total Gen Loss : 30.79078483581543, \t Total Dis Loss : 0.00022916622401680797\n",
      "Steps : 84600, \t Total Gen Loss : 31.97873878479004, \t Total Dis Loss : 7.014589209575206e-05\n",
      "Steps : 84700, \t Total Gen Loss : 31.41668701171875, \t Total Dis Loss : 0.0002545053721405566\n",
      "Steps : 84800, \t Total Gen Loss : 30.16370964050293, \t Total Dis Loss : 0.0002295603626407683\n",
      "Steps : 84900, \t Total Gen Loss : 30.782167434692383, \t Total Dis Loss : 0.0003429034841246903\n",
      "Steps : 85000, \t Total Gen Loss : 29.22121810913086, \t Total Dis Loss : 9.860433056019247e-05\n",
      "Steps : 85100, \t Total Gen Loss : 29.61302947998047, \t Total Dis Loss : 6.965296051930636e-05\n",
      "Steps : 85200, \t Total Gen Loss : 30.393457412719727, \t Total Dis Loss : 6.744736310793087e-05\n",
      "Steps : 85300, \t Total Gen Loss : 29.627826690673828, \t Total Dis Loss : 0.000243450136622414\n",
      "Steps : 85400, \t Total Gen Loss : 31.977590560913086, \t Total Dis Loss : 0.0005038951640017331\n",
      "Steps : 85500, \t Total Gen Loss : 30.587379455566406, \t Total Dis Loss : 0.030145026743412018\n",
      "Steps : 85600, \t Total Gen Loss : 28.87544822692871, \t Total Dis Loss : 0.0005350022111088037\n",
      "Steps : 85700, \t Total Gen Loss : 28.457386016845703, \t Total Dis Loss : 0.00042338602361269295\n",
      "Steps : 85800, \t Total Gen Loss : 30.789663314819336, \t Total Dis Loss : 0.0023388771805912256\n",
      "Steps : 85900, \t Total Gen Loss : 29.84827423095703, \t Total Dis Loss : 0.00017659914738032967\n",
      "Steps : 86000, \t Total Gen Loss : 30.527780532836914, \t Total Dis Loss : 0.0001506020053057\n",
      "Steps : 86100, \t Total Gen Loss : 28.749422073364258, \t Total Dis Loss : 0.0002542872098274529\n",
      "Steps : 86200, \t Total Gen Loss : 28.974884033203125, \t Total Dis Loss : 0.00015014447853900492\n",
      "Steps : 86300, \t Total Gen Loss : 31.638898849487305, \t Total Dis Loss : 5.937838432146236e-05\n",
      "Steps : 86400, \t Total Gen Loss : 28.696744918823242, \t Total Dis Loss : 0.0006419767159968615\n",
      "Steps : 86500, \t Total Gen Loss : 28.4176025390625, \t Total Dis Loss : 0.00028178858337923884\n",
      "Steps : 86600, \t Total Gen Loss : 24.532087326049805, \t Total Dis Loss : 0.006355290301144123\n",
      "Steps : 86700, \t Total Gen Loss : 32.28627014160156, \t Total Dis Loss : 2.4192822820623405e-05\n",
      "Steps : 86800, \t Total Gen Loss : 27.995635986328125, \t Total Dis Loss : 0.00021678682242054492\n",
      "Steps : 86900, \t Total Gen Loss : 26.477218627929688, \t Total Dis Loss : 0.002837042324244976\n",
      "Steps : 87000, \t Total Gen Loss : 26.85983657836914, \t Total Dis Loss : 0.0004519639478530735\n",
      "Steps : 87100, \t Total Gen Loss : 29.94520378112793, \t Total Dis Loss : 0.0009970210958272219\n",
      "Steps : 87200, \t Total Gen Loss : 28.263900756835938, \t Total Dis Loss : 0.0008997591212391853\n",
      "Steps : 87300, \t Total Gen Loss : 28.61985969543457, \t Total Dis Loss : 0.0026843473315238953\n",
      "Steps : 87400, \t Total Gen Loss : 27.025630950927734, \t Total Dis Loss : 0.0011112649226561189\n",
      "Steps : 87500, \t Total Gen Loss : 29.454477310180664, \t Total Dis Loss : 0.00040614011231809855\n",
      "Steps : 87600, \t Total Gen Loss : 29.76282501220703, \t Total Dis Loss : 0.0003314782225061208\n",
      "Steps : 87700, \t Total Gen Loss : 29.64205551147461, \t Total Dis Loss : 0.0003251014568377286\n",
      "Time for epoch 13 is 342.72354674339294 sec\n",
      "Steps : 87800, \t Total Gen Loss : 29.063613891601562, \t Total Dis Loss : 0.00034892247640527785\n",
      "Steps : 87900, \t Total Gen Loss : 27.236099243164062, \t Total Dis Loss : 0.0012071533128619194\n",
      "Steps : 88000, \t Total Gen Loss : 29.644798278808594, \t Total Dis Loss : 0.00012899900320917368\n",
      "Steps : 88100, \t Total Gen Loss : 34.16867446899414, \t Total Dis Loss : 3.4713630157057196e-05\n",
      "Steps : 88200, \t Total Gen Loss : 29.8780460357666, \t Total Dis Loss : 0.0004283320449758321\n",
      "Steps : 88300, \t Total Gen Loss : 29.476253509521484, \t Total Dis Loss : 0.0005423762486316264\n",
      "Steps : 88400, \t Total Gen Loss : 31.754919052124023, \t Total Dis Loss : 0.00010316532279830426\n",
      "Steps : 88500, \t Total Gen Loss : 29.833467483520508, \t Total Dis Loss : 0.00018405384616926312\n",
      "Steps : 88600, \t Total Gen Loss : 32.34775161743164, \t Total Dis Loss : 7.858817116357386e-05\n",
      "Steps : 88700, \t Total Gen Loss : 30.776630401611328, \t Total Dis Loss : 0.0005079201073385775\n",
      "Steps : 88800, \t Total Gen Loss : 26.99542808532715, \t Total Dis Loss : 0.0004275140818208456\n",
      "Steps : 88900, \t Total Gen Loss : 29.301471710205078, \t Total Dis Loss : 0.0006301489775069058\n",
      "Steps : 89000, \t Total Gen Loss : 32.819976806640625, \t Total Dis Loss : 9.956209396477789e-05\n",
      "Steps : 89100, \t Total Gen Loss : 29.764551162719727, \t Total Dis Loss : 0.0007708000484853983\n",
      "Steps : 89200, \t Total Gen Loss : 28.2966365814209, \t Total Dis Loss : 0.0003925572382286191\n",
      "Steps : 89300, \t Total Gen Loss : 31.269298553466797, \t Total Dis Loss : 0.00033938055275939405\n",
      "Steps : 89400, \t Total Gen Loss : 30.307466506958008, \t Total Dis Loss : 0.0012144080828875303\n",
      "Steps : 89500, \t Total Gen Loss : 30.034955978393555, \t Total Dis Loss : 0.00017783406656235456\n",
      "Steps : 89600, \t Total Gen Loss : 27.724708557128906, \t Total Dis Loss : 0.00020119306282140315\n",
      "Steps : 89700, \t Total Gen Loss : 30.15850830078125, \t Total Dis Loss : 0.016135305166244507\n",
      "Steps : 89800, \t Total Gen Loss : 32.59625244140625, \t Total Dis Loss : 0.00010340297740185633\n",
      "Steps : 89900, \t Total Gen Loss : 29.49663734436035, \t Total Dis Loss : 0.0003491016977932304\n",
      "Steps : 90000, \t Total Gen Loss : 29.558408737182617, \t Total Dis Loss : 0.0005873506888747215\n",
      "Steps : 90100, \t Total Gen Loss : 29.208005905151367, \t Total Dis Loss : 0.0002621417515911162\n",
      "Steps : 90200, \t Total Gen Loss : 27.071773529052734, \t Total Dis Loss : 0.0006566147785633802\n",
      "Steps : 90300, \t Total Gen Loss : 27.506858825683594, \t Total Dis Loss : 0.00014702253974974155\n",
      "Steps : 90400, \t Total Gen Loss : 32.173248291015625, \t Total Dis Loss : 5.052677443018183e-05\n",
      "Steps : 90500, \t Total Gen Loss : 31.271469116210938, \t Total Dis Loss : 0.0002286694070789963\n",
      "Steps : 90600, \t Total Gen Loss : 31.85873031616211, \t Total Dis Loss : 2.0977717213099822e-05\n",
      "Steps : 90700, \t Total Gen Loss : 32.336082458496094, \t Total Dis Loss : 0.0002890451578423381\n",
      "Steps : 90800, \t Total Gen Loss : 29.873828887939453, \t Total Dis Loss : 0.00012341026740614325\n",
      "Steps : 90900, \t Total Gen Loss : 33.090946197509766, \t Total Dis Loss : 3.919105074601248e-05\n",
      "Steps : 91000, \t Total Gen Loss : 32.25352478027344, \t Total Dis Loss : 0.00017573607328813523\n",
      "Steps : 91100, \t Total Gen Loss : 30.52104949951172, \t Total Dis Loss : 0.00019837389118038118\n",
      "Steps : 91200, \t Total Gen Loss : 30.529701232910156, \t Total Dis Loss : 0.00013529640273191035\n",
      "Steps : 91300, \t Total Gen Loss : 28.983686447143555, \t Total Dis Loss : 0.0003007331397384405\n",
      "Steps : 91400, \t Total Gen Loss : 30.19078826904297, \t Total Dis Loss : 0.00010885779920499772\n",
      "Steps : 91500, \t Total Gen Loss : 30.100521087646484, \t Total Dis Loss : 0.0034159603528678417\n",
      "Steps : 91600, \t Total Gen Loss : 31.914804458618164, \t Total Dis Loss : 0.00015088103828020394\n",
      "Steps : 91700, \t Total Gen Loss : 32.292999267578125, \t Total Dis Loss : 0.00011774527956731617\n",
      "Steps : 91800, \t Total Gen Loss : 32.2508659362793, \t Total Dis Loss : 6.852793740108609e-05\n",
      "Steps : 91900, \t Total Gen Loss : 31.19904899597168, \t Total Dis Loss : 4.86365424876567e-05\n",
      "Steps : 92000, \t Total Gen Loss : 32.980682373046875, \t Total Dis Loss : 3.861419827444479e-05\n",
      "Steps : 92100, \t Total Gen Loss : 29.427677154541016, \t Total Dis Loss : 0.00017468631267547607\n",
      "Steps : 92200, \t Total Gen Loss : 30.409992218017578, \t Total Dis Loss : 6.89220178173855e-05\n",
      "Steps : 92300, \t Total Gen Loss : 27.744230270385742, \t Total Dis Loss : 0.0008614914841018617\n",
      "Steps : 92400, \t Total Gen Loss : 28.60336685180664, \t Total Dis Loss : 0.00021474694949574769\n",
      "Steps : 92500, \t Total Gen Loss : 28.956499099731445, \t Total Dis Loss : 0.0003786870220210403\n",
      "Steps : 92600, \t Total Gen Loss : 29.661731719970703, \t Total Dis Loss : 5.853502807440236e-05\n",
      "Steps : 92700, \t Total Gen Loss : 28.851207733154297, \t Total Dis Loss : 0.0004026670940220356\n",
      "Steps : 92800, \t Total Gen Loss : 30.338102340698242, \t Total Dis Loss : 6.203410885063931e-05\n",
      "Steps : 92900, \t Total Gen Loss : 30.444345474243164, \t Total Dis Loss : 0.00024948155623860657\n",
      "Steps : 93000, \t Total Gen Loss : 30.027015686035156, \t Total Dis Loss : 9.125254291575402e-05\n",
      "Steps : 93100, \t Total Gen Loss : 30.380006790161133, \t Total Dis Loss : 0.00014018411457072943\n",
      "Steps : 93200, \t Total Gen Loss : 30.762914657592773, \t Total Dis Loss : 0.00010088054841617122\n",
      "Steps : 93300, \t Total Gen Loss : 29.236631393432617, \t Total Dis Loss : 0.00018944487965200096\n",
      "Steps : 93400, \t Total Gen Loss : 27.76430892944336, \t Total Dis Loss : 0.00164318410679698\n",
      "Steps : 93500, \t Total Gen Loss : 28.257450103759766, \t Total Dis Loss : 0.0010085755493491888\n",
      "Steps : 93600, \t Total Gen Loss : 31.402692794799805, \t Total Dis Loss : 0.0001327417412539944\n",
      "Steps : 93700, \t Total Gen Loss : 29.954317092895508, \t Total Dis Loss : 8.499649993609637e-05\n",
      "Steps : 93800, \t Total Gen Loss : 30.7159423828125, \t Total Dis Loss : 0.0001286232436541468\n",
      "Steps : 93900, \t Total Gen Loss : 31.791362762451172, \t Total Dis Loss : 4.966832420905121e-05\n",
      "Steps : 94000, \t Total Gen Loss : 30.550058364868164, \t Total Dis Loss : 4.703041122411378e-05\n",
      "Steps : 94100, \t Total Gen Loss : 31.05031967163086, \t Total Dis Loss : 3.579007170628756e-05\n",
      "Steps : 94200, \t Total Gen Loss : 30.54007339477539, \t Total Dis Loss : 6.11876675975509e-05\n",
      "Steps : 94300, \t Total Gen Loss : 27.09115219116211, \t Total Dis Loss : 0.00019706250168383121\n",
      "Steps : 94400, \t Total Gen Loss : 30.006818771362305, \t Total Dis Loss : 5.8320019888924435e-05\n",
      "Steps : 94500, \t Total Gen Loss : 29.971012115478516, \t Total Dis Loss : 0.00013708672486245632\n",
      "Time for epoch 14 is 342.6361858844757 sec\n",
      "Steps : 94600, \t Total Gen Loss : 31.05394744873047, \t Total Dis Loss : 0.00021426343300845474\n",
      "Steps : 94700, \t Total Gen Loss : 30.444128036499023, \t Total Dis Loss : 5.001815952709876e-05\n",
      "Steps : 94800, \t Total Gen Loss : 31.63616943359375, \t Total Dis Loss : 2.8928479878231883e-05\n",
      "Steps : 94900, \t Total Gen Loss : 34.00334167480469, \t Total Dis Loss : 3.3243235520785674e-05\n",
      "Steps : 95000, \t Total Gen Loss : 31.123851776123047, \t Total Dis Loss : 6.465450132964179e-05\n",
      "Steps : 95100, \t Total Gen Loss : 31.45348358154297, \t Total Dis Loss : 0.0010035417508333921\n",
      "Steps : 95200, \t Total Gen Loss : 29.56280517578125, \t Total Dis Loss : 0.0030756318010389805\n",
      "Steps : 95300, \t Total Gen Loss : 30.582542419433594, \t Total Dis Loss : 0.00013302125444170088\n",
      "Steps : 95400, \t Total Gen Loss : 29.964881896972656, \t Total Dis Loss : 8.287920354632661e-05\n",
      "Steps : 95500, \t Total Gen Loss : 33.336490631103516, \t Total Dis Loss : 2.909153772634454e-05\n",
      "Steps : 95600, \t Total Gen Loss : 31.814184188842773, \t Total Dis Loss : 3.377267057658173e-05\n",
      "Steps : 95700, \t Total Gen Loss : 30.917827606201172, \t Total Dis Loss : 0.00048189624794758856\n",
      "Steps : 95800, \t Total Gen Loss : 28.999887466430664, \t Total Dis Loss : 0.0006441407604143023\n",
      "Steps : 95900, \t Total Gen Loss : 28.829687118530273, \t Total Dis Loss : 0.00016184878768399358\n",
      "Steps : 96000, \t Total Gen Loss : 29.719091415405273, \t Total Dis Loss : 0.00040055220597423613\n",
      "Steps : 96100, \t Total Gen Loss : 30.561594009399414, \t Total Dis Loss : 0.0003520784084685147\n",
      "Steps : 96200, \t Total Gen Loss : 28.687101364135742, \t Total Dis Loss : 0.0002114672097377479\n",
      "Steps : 96300, \t Total Gen Loss : 30.262331008911133, \t Total Dis Loss : 0.00015645561506971717\n",
      "Steps : 96400, \t Total Gen Loss : 29.03331184387207, \t Total Dis Loss : 0.00018992451077792794\n",
      "Steps : 96500, \t Total Gen Loss : 28.818937301635742, \t Total Dis Loss : 0.0001558213698444888\n",
      "Steps : 96600, \t Total Gen Loss : 30.258148193359375, \t Total Dis Loss : 0.0037310312036424875\n",
      "Steps : 96700, \t Total Gen Loss : 29.564815521240234, \t Total Dis Loss : 7.430781988659874e-05\n",
      "Steps : 96800, \t Total Gen Loss : 30.437007904052734, \t Total Dis Loss : 7.897846808191389e-05\n",
      "Steps : 96900, \t Total Gen Loss : 30.139013290405273, \t Total Dis Loss : 6.510803359560668e-05\n",
      "Steps : 97000, \t Total Gen Loss : 30.610403060913086, \t Total Dis Loss : 0.00011198844731552526\n",
      "Steps : 97100, \t Total Gen Loss : 30.190017700195312, \t Total Dis Loss : 0.00011915089271496981\n",
      "Steps : 97200, \t Total Gen Loss : 31.142887115478516, \t Total Dis Loss : 7.158978405641392e-05\n",
      "Steps : 97300, \t Total Gen Loss : 30.330530166625977, \t Total Dis Loss : 4.055591853102669e-05\n",
      "Steps : 97400, \t Total Gen Loss : 29.83438491821289, \t Total Dis Loss : 0.0002009508607443422\n",
      "Steps : 97500, \t Total Gen Loss : 30.174020767211914, \t Total Dis Loss : 4.495167377172038e-05\n",
      "Steps : 97600, \t Total Gen Loss : 30.657508850097656, \t Total Dis Loss : 8.073851495282724e-05\n",
      "Steps : 97700, \t Total Gen Loss : 30.31622886657715, \t Total Dis Loss : 7.675202505197376e-05\n",
      "Steps : 97800, \t Total Gen Loss : 28.481632232666016, \t Total Dis Loss : 0.00015320822421927005\n",
      "Steps : 97900, \t Total Gen Loss : 31.466657638549805, \t Total Dis Loss : 2.098503682645969e-05\n",
      "Steps : 98000, \t Total Gen Loss : 31.872390747070312, \t Total Dis Loss : 3.09246861434076e-05\n",
      "Steps : 98100, \t Total Gen Loss : 31.173397064208984, \t Total Dis Loss : 7.092798477970064e-05\n",
      "Steps : 98200, \t Total Gen Loss : 31.582195281982422, \t Total Dis Loss : 4.3744683352997527e-05\n",
      "Steps : 98300, \t Total Gen Loss : 29.935754776000977, \t Total Dis Loss : 5.244859858066775e-05\n",
      "Steps : 98400, \t Total Gen Loss : 29.885578155517578, \t Total Dis Loss : 2.2207039364730008e-05\n",
      "Steps : 98500, \t Total Gen Loss : 29.914649963378906, \t Total Dis Loss : 2.4554010451538488e-05\n",
      "Steps : 98600, \t Total Gen Loss : 31.344816207885742, \t Total Dis Loss : 3.781340637942776e-05\n",
      "Steps : 98700, \t Total Gen Loss : 30.40416717529297, \t Total Dis Loss : 3.288463267381303e-05\n",
      "Steps : 98800, \t Total Gen Loss : 30.768848419189453, \t Total Dis Loss : 2.81490520137595e-05\n",
      "Steps : 98900, \t Total Gen Loss : 32.32080841064453, \t Total Dis Loss : 2.417412179056555e-05\n",
      "Steps : 99000, \t Total Gen Loss : 33.085533142089844, \t Total Dis Loss : 1.8310220184503123e-05\n",
      "Steps : 99100, \t Total Gen Loss : 31.330392837524414, \t Total Dis Loss : 1.8664159142645076e-05\n",
      "Steps : 99200, \t Total Gen Loss : 32.788787841796875, \t Total Dis Loss : 2.5437038857489824e-05\n",
      "Steps : 99300, \t Total Gen Loss : 30.490215301513672, \t Total Dis Loss : 0.00021655746968463063\n",
      "Steps : 99400, \t Total Gen Loss : 30.518138885498047, \t Total Dis Loss : 0.00018460022693034261\n",
      "Steps : 99500, \t Total Gen Loss : 27.937427520751953, \t Total Dis Loss : 0.0019142169039696455\n",
      "Steps : 99600, \t Total Gen Loss : 30.913612365722656, \t Total Dis Loss : 0.00014646512863691896\n",
      "Steps : 99700, \t Total Gen Loss : 30.01529312133789, \t Total Dis Loss : 0.0001527106505818665\n",
      "Steps : 99800, \t Total Gen Loss : 29.8687744140625, \t Total Dis Loss : 0.00017227644275408238\n",
      "Steps : 99900, \t Total Gen Loss : 30.735055923461914, \t Total Dis Loss : 0.00010135309275938198\n",
      "Steps : 100000, \t Total Gen Loss : 29.245361328125, \t Total Dis Loss : 0.0004884109366685152\n",
      "Steps : 100100, \t Total Gen Loss : 30.048194885253906, \t Total Dis Loss : 0.0003096691216342151\n",
      "Steps : 100200, \t Total Gen Loss : 29.430505752563477, \t Total Dis Loss : 6.969680543988943e-05\n",
      "Steps : 100300, \t Total Gen Loss : 30.81521987915039, \t Total Dis Loss : 4.692385846283287e-05\n",
      "Steps : 100400, \t Total Gen Loss : 32.179019927978516, \t Total Dis Loss : 0.0008217136492021382\n",
      "Steps : 100500, \t Total Gen Loss : 33.87931442260742, \t Total Dis Loss : 0.0007293063681572676\n",
      "Steps : 100600, \t Total Gen Loss : 33.76103973388672, \t Total Dis Loss : 0.004230513237416744\n",
      "Steps : 100700, \t Total Gen Loss : 29.793331146240234, \t Total Dis Loss : 0.0011649824446067214\n",
      "Steps : 100800, \t Total Gen Loss : 29.02051544189453, \t Total Dis Loss : 0.002916166791692376\n",
      "Steps : 100900, \t Total Gen Loss : 36.69257736206055, \t Total Dis Loss : 0.0004324685432948172\n",
      "Steps : 101000, \t Total Gen Loss : 32.63553237915039, \t Total Dis Loss : 0.00015684112440794706\n",
      "Steps : 101100, \t Total Gen Loss : 31.48302459716797, \t Total Dis Loss : 0.0007867764215916395\n",
      "Steps : 101200, \t Total Gen Loss : 31.379430770874023, \t Total Dis Loss : 0.0001434023433830589\n",
      "Time for epoch 15 is 344.2064039707184 sec\n",
      "Steps : 101300, \t Total Gen Loss : 32.997764587402344, \t Total Dis Loss : 0.00012468555360101163\n",
      "Steps : 101400, \t Total Gen Loss : 30.836952209472656, \t Total Dis Loss : 0.00015575058932881802\n",
      "Steps : 101500, \t Total Gen Loss : 30.029104232788086, \t Total Dis Loss : 0.000137569586513564\n",
      "Steps : 101600, \t Total Gen Loss : 32.03601837158203, \t Total Dis Loss : 0.00021018412371631712\n",
      "Steps : 101700, \t Total Gen Loss : 31.369281768798828, \t Total Dis Loss : 0.00010130137525266036\n",
      "Steps : 101800, \t Total Gen Loss : 30.626319885253906, \t Total Dis Loss : 0.00011509400064824149\n",
      "Steps : 101900, \t Total Gen Loss : 32.08185958862305, \t Total Dis Loss : 9.728704753797501e-05\n",
      "Steps : 102000, \t Total Gen Loss : 31.12502670288086, \t Total Dis Loss : 0.0001384764036629349\n",
      "Steps : 102100, \t Total Gen Loss : 30.991256713867188, \t Total Dis Loss : 9.039138240041211e-05\n",
      "Steps : 102200, \t Total Gen Loss : 33.808895111083984, \t Total Dis Loss : 2.317924918315839e-05\n",
      "Steps : 102300, \t Total Gen Loss : 31.060636520385742, \t Total Dis Loss : 6.441854202421382e-05\n",
      "Steps : 102400, \t Total Gen Loss : 31.623781204223633, \t Total Dis Loss : 5.120659261592664e-05\n",
      "Steps : 102500, \t Total Gen Loss : 32.25148391723633, \t Total Dis Loss : 4.948168498231098e-05\n",
      "Steps : 102600, \t Total Gen Loss : 31.14249610900879, \t Total Dis Loss : 1.2256526133569423e-05\n",
      "Steps : 102700, \t Total Gen Loss : 31.715084075927734, \t Total Dis Loss : 3.0235281883506104e-05\n",
      "Steps : 102800, \t Total Gen Loss : 34.313377380371094, \t Total Dis Loss : 0.00017632190429139882\n",
      "Steps : 102900, \t Total Gen Loss : 33.48462677001953, \t Total Dis Loss : 4.614395948010497e-05\n",
      "Steps : 103000, \t Total Gen Loss : 34.25180435180664, \t Total Dis Loss : 0.0005668867379426956\n",
      "Steps : 103100, \t Total Gen Loss : 35.220027923583984, \t Total Dis Loss : 0.00029950778116472065\n",
      "Steps : 103200, \t Total Gen Loss : 36.04938507080078, \t Total Dis Loss : 0.000945531646721065\n",
      "Steps : 103300, \t Total Gen Loss : 35.101139068603516, \t Total Dis Loss : 0.00033072728547267616\n",
      "Steps : 103400, \t Total Gen Loss : 34.56997299194336, \t Total Dis Loss : 0.0004018896142952144\n",
      "Steps : 103500, \t Total Gen Loss : 33.75323486328125, \t Total Dis Loss : 0.0010466530220583081\n",
      "Steps : 103600, \t Total Gen Loss : 33.631126403808594, \t Total Dis Loss : 0.00021112112153787166\n",
      "Steps : 103700, \t Total Gen Loss : 36.177650451660156, \t Total Dis Loss : 0.00017346095410175622\n",
      "Steps : 103800, \t Total Gen Loss : 34.05584716796875, \t Total Dis Loss : 0.00027146912179887295\n",
      "Steps : 103900, \t Total Gen Loss : 36.78154373168945, \t Total Dis Loss : 0.0002220801980001852\n",
      "Steps : 104000, \t Total Gen Loss : 35.325355529785156, \t Total Dis Loss : 0.005690539721399546\n",
      "Steps : 104100, \t Total Gen Loss : 34.94698715209961, \t Total Dis Loss : 0.00020331818086560816\n",
      "Steps : 104200, \t Total Gen Loss : 31.97208595275879, \t Total Dis Loss : 0.0005491503397934139\n",
      "Steps : 104300, \t Total Gen Loss : 29.44923973083496, \t Total Dis Loss : 0.0005986200994811952\n",
      "Steps : 104400, \t Total Gen Loss : 32.32004165649414, \t Total Dis Loss : 0.00012650011922232807\n",
      "Steps : 104500, \t Total Gen Loss : 30.29400062561035, \t Total Dis Loss : 0.0021476910915225744\n",
      "Steps : 104600, \t Total Gen Loss : 33.281280517578125, \t Total Dis Loss : 0.0019494943553581834\n",
      "Steps : 104700, \t Total Gen Loss : 30.5380802154541, \t Total Dis Loss : 0.00017832515004556626\n",
      "Steps : 104800, \t Total Gen Loss : 32.02492904663086, \t Total Dis Loss : 0.00033816444920375943\n",
      "Steps : 104900, \t Total Gen Loss : 31.719507217407227, \t Total Dis Loss : 0.0005675029824487865\n",
      "Steps : 105000, \t Total Gen Loss : 32.90293884277344, \t Total Dis Loss : 0.000294018245767802\n",
      "Steps : 105100, \t Total Gen Loss : 33.358802795410156, \t Total Dis Loss : 0.00025145866675302386\n",
      "Steps : 105200, \t Total Gen Loss : 32.472755432128906, \t Total Dis Loss : 0.0003752677876036614\n",
      "Steps : 105300, \t Total Gen Loss : 36.8132438659668, \t Total Dis Loss : 9.91340129985474e-05\n",
      "Steps : 105400, \t Total Gen Loss : 32.65580749511719, \t Total Dis Loss : 0.0003671832964755595\n",
      "Steps : 105500, \t Total Gen Loss : 31.77684783935547, \t Total Dis Loss : 0.00015330215683206916\n",
      "Steps : 105600, \t Total Gen Loss : 32.903892517089844, \t Total Dis Loss : 5.967771721770987e-05\n",
      "Steps : 105700, \t Total Gen Loss : 33.40495681762695, \t Total Dis Loss : 4.106048436369747e-05\n",
      "Steps : 105800, \t Total Gen Loss : 32.22251892089844, \t Total Dis Loss : 0.007706921547651291\n",
      "Steps : 105900, \t Total Gen Loss : 30.579479217529297, \t Total Dis Loss : 7.75267108110711e-05\n",
      "Steps : 106000, \t Total Gen Loss : 30.840993881225586, \t Total Dis Loss : 4.9970531108556315e-05\n",
      "Steps : 106100, \t Total Gen Loss : 32.58473205566406, \t Total Dis Loss : 0.0004444172082003206\n",
      "Steps : 106200, \t Total Gen Loss : 33.45950698852539, \t Total Dis Loss : 5.8777324738912284e-05\n",
      "Steps : 106300, \t Total Gen Loss : 33.453346252441406, \t Total Dis Loss : 1.6887088349903934e-05\n",
      "Steps : 106400, \t Total Gen Loss : 35.0886116027832, \t Total Dis Loss : 3.380084672244266e-05\n",
      "Steps : 106500, \t Total Gen Loss : 33.220943450927734, \t Total Dis Loss : 9.259663784177974e-05\n",
      "Steps : 106600, \t Total Gen Loss : 32.50353240966797, \t Total Dis Loss : 0.0006034884136170149\n",
      "Steps : 106700, \t Total Gen Loss : 33.36785125732422, \t Total Dis Loss : 8.567568875150755e-05\n",
      "Steps : 106800, \t Total Gen Loss : 34.881954193115234, \t Total Dis Loss : 7.283002560143359e-06\n",
      "Steps : 106900, \t Total Gen Loss : 33.196617126464844, \t Total Dis Loss : 6.28559646429494e-05\n",
      "Steps : 107000, \t Total Gen Loss : 32.750831604003906, \t Total Dis Loss : 0.0001994770427700132\n",
      "Steps : 107100, \t Total Gen Loss : 30.2911434173584, \t Total Dis Loss : 0.0003078391309827566\n",
      "Steps : 107200, \t Total Gen Loss : 34.0908203125, \t Total Dis Loss : 9.458326530875638e-05\n",
      "Steps : 107300, \t Total Gen Loss : 31.929140090942383, \t Total Dis Loss : 5.200106534175575e-05\n",
      "Steps : 107400, \t Total Gen Loss : 34.223777770996094, \t Total Dis Loss : 0.0002840557135641575\n",
      "Steps : 107500, \t Total Gen Loss : 33.32254409790039, \t Total Dis Loss : 0.00022473953140433878\n",
      "Steps : 107600, \t Total Gen Loss : 33.36603546142578, \t Total Dis Loss : 0.000297546066576615\n",
      "Steps : 107700, \t Total Gen Loss : 33.972782135009766, \t Total Dis Loss : 0.00018604681827127934\n",
      "Steps : 107800, \t Total Gen Loss : 32.29880142211914, \t Total Dis Loss : 0.00027369079180061817\n",
      "Steps : 107900, \t Total Gen Loss : 31.883268356323242, \t Total Dis Loss : 4.9340163968736306e-05\n",
      "Steps : 108000, \t Total Gen Loss : 31.05265235900879, \t Total Dis Loss : 0.0074958172626793385\n",
      "Time for epoch 16 is 344.2772424221039 sec\n",
      "Steps : 108100, \t Total Gen Loss : 31.353492736816406, \t Total Dis Loss : 0.0001888331607915461\n",
      "Steps : 108200, \t Total Gen Loss : 31.392480850219727, \t Total Dis Loss : 0.00025740181445144117\n",
      "Steps : 108300, \t Total Gen Loss : 32.782291412353516, \t Total Dis Loss : 7.25876961951144e-05\n",
      "Steps : 108400, \t Total Gen Loss : 31.675992965698242, \t Total Dis Loss : 0.00011012159666279331\n",
      "Steps : 108500, \t Total Gen Loss : 32.54930877685547, \t Total Dis Loss : 2.4725552066229284e-05\n",
      "Steps : 108600, \t Total Gen Loss : 31.527645111083984, \t Total Dis Loss : 0.00021485860634129494\n",
      "Steps : 108700, \t Total Gen Loss : 32.624603271484375, \t Total Dis Loss : 1.5587089364998974e-05\n",
      "Steps : 108800, \t Total Gen Loss : 31.917335510253906, \t Total Dis Loss : 9.701885574031621e-05\n",
      "Steps : 108900, \t Total Gen Loss : 32.53213882446289, \t Total Dis Loss : 0.00012355661601759493\n",
      "Steps : 109000, \t Total Gen Loss : 30.568279266357422, \t Total Dis Loss : 0.0002352099836571142\n",
      "Steps : 109100, \t Total Gen Loss : 32.90532302856445, \t Total Dis Loss : 4.667089524446055e-05\n",
      "Steps : 109200, \t Total Gen Loss : 30.49274444580078, \t Total Dis Loss : 0.002726916456595063\n",
      "Steps : 109300, \t Total Gen Loss : 32.89421081542969, \t Total Dis Loss : 4.908889241050929e-05\n",
      "Steps : 109400, \t Total Gen Loss : 31.5301513671875, \t Total Dis Loss : 0.011585055850446224\n",
      "Steps : 109500, \t Total Gen Loss : 33.07145690917969, \t Total Dis Loss : 0.000263331166934222\n",
      "Steps : 109600, \t Total Gen Loss : 35.17755889892578, \t Total Dis Loss : 0.0016308934427797794\n",
      "Steps : 109700, \t Total Gen Loss : 34.045921325683594, \t Total Dis Loss : 0.0014818997588008642\n",
      "Steps : 109800, \t Total Gen Loss : 34.722843170166016, \t Total Dis Loss : 0.003278844989836216\n",
      "Steps : 109900, \t Total Gen Loss : 32.58595657348633, \t Total Dis Loss : 0.0006850153440609574\n",
      "Steps : 110000, \t Total Gen Loss : 31.853866577148438, \t Total Dis Loss : 4.1440151107963175e-05\n",
      "Steps : 110100, \t Total Gen Loss : 33.439727783203125, \t Total Dis Loss : 0.0002593737153802067\n",
      "Steps : 110200, \t Total Gen Loss : 33.343719482421875, \t Total Dis Loss : 0.0002489946782588959\n",
      "Steps : 110300, \t Total Gen Loss : 35.08045959472656, \t Total Dis Loss : 7.189644384197891e-05\n",
      "Steps : 110400, \t Total Gen Loss : 32.640625, \t Total Dis Loss : 0.001084582065232098\n",
      "Steps : 110500, \t Total Gen Loss : 32.773319244384766, \t Total Dis Loss : 0.00018056738190352917\n",
      "Steps : 110600, \t Total Gen Loss : 33.275691986083984, \t Total Dis Loss : 7.716350955888629e-05\n",
      "Steps : 110700, \t Total Gen Loss : 31.11429214477539, \t Total Dis Loss : 0.00023827081895433366\n",
      "Steps : 110800, \t Total Gen Loss : 34.26097869873047, \t Total Dis Loss : 7.175780046964064e-05\n",
      "Steps : 110900, \t Total Gen Loss : 32.65046691894531, \t Total Dis Loss : 5.7861099776346236e-05\n",
      "Steps : 111000, \t Total Gen Loss : 32.3636474609375, \t Total Dis Loss : 0.00015876760880928487\n",
      "Steps : 111100, \t Total Gen Loss : 33.719512939453125, \t Total Dis Loss : 0.0001324215845670551\n",
      "Steps : 111200, \t Total Gen Loss : 29.679428100585938, \t Total Dis Loss : 0.001780521939508617\n",
      "Steps : 111300, \t Total Gen Loss : 30.256404876708984, \t Total Dis Loss : 0.00035518419463187456\n",
      "Steps : 111400, \t Total Gen Loss : 31.23638343811035, \t Total Dis Loss : 0.005089647602289915\n",
      "Steps : 111500, \t Total Gen Loss : 32.2166862487793, \t Total Dis Loss : 0.000876431236974895\n",
      "Steps : 111600, \t Total Gen Loss : 30.983530044555664, \t Total Dis Loss : 0.00054658338194713\n",
      "Steps : 111700, \t Total Gen Loss : 32.16250991821289, \t Total Dis Loss : 0.000974794093053788\n",
      "Steps : 111800, \t Total Gen Loss : 29.823144912719727, \t Total Dis Loss : 0.0008364984532818198\n",
      "Steps : 111900, \t Total Gen Loss : 31.24093246459961, \t Total Dis Loss : 0.0001309386861976236\n",
      "Steps : 112000, \t Total Gen Loss : 30.73715591430664, \t Total Dis Loss : 0.002858443884178996\n",
      "Steps : 112100, \t Total Gen Loss : 28.725112915039062, \t Total Dis Loss : 0.001309767016209662\n",
      "Steps : 112200, \t Total Gen Loss : 31.885955810546875, \t Total Dis Loss : 0.0001536442432552576\n",
      "Steps : 112300, \t Total Gen Loss : 30.834487915039062, \t Total Dis Loss : 0.0002704876533243805\n",
      "Steps : 112400, \t Total Gen Loss : 30.76300811767578, \t Total Dis Loss : 9.872588270809501e-05\n",
      "Steps : 112500, \t Total Gen Loss : 33.18427658081055, \t Total Dis Loss : 7.83285140641965e-05\n",
      "Steps : 112600, \t Total Gen Loss : 32.325767517089844, \t Total Dis Loss : 0.00013129027502145618\n",
      "Steps : 112700, \t Total Gen Loss : 32.29330062866211, \t Total Dis Loss : 0.0015254607424139977\n",
      "Steps : 112800, \t Total Gen Loss : 32.629722595214844, \t Total Dis Loss : 4.683699080487713e-05\n",
      "Steps : 112900, \t Total Gen Loss : 32.857444763183594, \t Total Dis Loss : 5.0198006647406146e-05\n",
      "Steps : 113000, \t Total Gen Loss : 28.84355926513672, \t Total Dis Loss : 0.00024728025891818106\n",
      "Steps : 113100, \t Total Gen Loss : 30.197933197021484, \t Total Dis Loss : 0.00021697842748835683\n",
      "Steps : 113200, \t Total Gen Loss : 31.11379623413086, \t Total Dis Loss : 0.0002440746611682698\n",
      "Steps : 113300, \t Total Gen Loss : 31.148239135742188, \t Total Dis Loss : 0.00021616976300720125\n",
      "Steps : 113400, \t Total Gen Loss : 30.757862091064453, \t Total Dis Loss : 0.00010717654367908835\n",
      "Steps : 113500, \t Total Gen Loss : 31.720409393310547, \t Total Dis Loss : 0.00011476725194370374\n",
      "Steps : 113600, \t Total Gen Loss : 29.586132049560547, \t Total Dis Loss : 8.138997509377077e-05\n",
      "Steps : 113700, \t Total Gen Loss : 31.374061584472656, \t Total Dis Loss : 0.00022797581914346665\n",
      "Steps : 113800, \t Total Gen Loss : 32.19453048706055, \t Total Dis Loss : 0.00017438476788811386\n",
      "Steps : 113900, \t Total Gen Loss : 33.25999069213867, \t Total Dis Loss : 6.159461190691218e-05\n",
      "Steps : 114000, \t Total Gen Loss : 29.87523651123047, \t Total Dis Loss : 0.00016681122360751033\n",
      "Steps : 114100, \t Total Gen Loss : 29.864267349243164, \t Total Dis Loss : 0.00018926168559119105\n",
      "Steps : 114200, \t Total Gen Loss : 30.673587799072266, \t Total Dis Loss : 0.00015548615192528814\n",
      "Steps : 114300, \t Total Gen Loss : 31.750415802001953, \t Total Dis Loss : 8.864064875524491e-05\n",
      "Steps : 114400, \t Total Gen Loss : 30.475671768188477, \t Total Dis Loss : 2.8905251383548602e-05\n",
      "Steps : 114500, \t Total Gen Loss : 30.935714721679688, \t Total Dis Loss : 5.347888873075135e-05\n",
      "Steps : 114600, \t Total Gen Loss : 31.995853424072266, \t Total Dis Loss : 3.974705032305792e-05\n",
      "Steps : 114700, \t Total Gen Loss : 30.96310043334961, \t Total Dis Loss : 5.2674367907457054e-05\n",
      "Time for epoch 17 is 344.56674313545227 sec\n",
      "Steps : 114800, \t Total Gen Loss : 31.31911849975586, \t Total Dis Loss : 4.1290593799203634e-05\n",
      "Steps : 114900, \t Total Gen Loss : 31.505508422851562, \t Total Dis Loss : 0.00020106877491343766\n",
      "Steps : 115000, \t Total Gen Loss : 31.664552688598633, \t Total Dis Loss : 5.2675241022370756e-05\n",
      "Steps : 115100, \t Total Gen Loss : 33.44686508178711, \t Total Dis Loss : 7.999816443771124e-05\n",
      "Steps : 115200, \t Total Gen Loss : 30.89289093017578, \t Total Dis Loss : 0.002190191764384508\n",
      "Steps : 115300, \t Total Gen Loss : 33.3738899230957, \t Total Dis Loss : 0.00012577165034599602\n",
      "Steps : 115400, \t Total Gen Loss : 32.97636413574219, \t Total Dis Loss : 0.0001602566771907732\n",
      "Steps : 115500, \t Total Gen Loss : 34.59453201293945, \t Total Dis Loss : 7.566289423266426e-05\n",
      "Steps : 115600, \t Total Gen Loss : 33.30727005004883, \t Total Dis Loss : 8.004752453416586e-05\n",
      "Steps : 115700, \t Total Gen Loss : 33.38547897338867, \t Total Dis Loss : 0.0005769855342805386\n",
      "Steps : 115800, \t Total Gen Loss : 30.646692276000977, \t Total Dis Loss : 0.00047647213796153665\n",
      "Steps : 115900, \t Total Gen Loss : 31.989837646484375, \t Total Dis Loss : 2.4084602046059445e-05\n",
      "Steps : 116000, \t Total Gen Loss : 34.284027099609375, \t Total Dis Loss : 6.503912300104275e-05\n",
      "Steps : 116100, \t Total Gen Loss : 33.145240783691406, \t Total Dis Loss : 0.001274287118576467\n",
      "Steps : 116200, \t Total Gen Loss : 33.720245361328125, \t Total Dis Loss : 0.0002853694895748049\n",
      "Steps : 116300, \t Total Gen Loss : 31.652400970458984, \t Total Dis Loss : 0.0005974596133455634\n",
      "Steps : 116400, \t Total Gen Loss : 35.72952651977539, \t Total Dis Loss : 0.000650012050755322\n",
      "Steps : 116500, \t Total Gen Loss : 31.897306442260742, \t Total Dis Loss : 0.001214061863720417\n",
      "Steps : 116600, \t Total Gen Loss : 34.55378723144531, \t Total Dis Loss : 3.7020767194917426e-05\n",
      "Steps : 116700, \t Total Gen Loss : 31.85765266418457, \t Total Dis Loss : 0.00014285215002018958\n",
      "Steps : 116800, \t Total Gen Loss : 33.527076721191406, \t Total Dis Loss : 0.004816507454961538\n",
      "Steps : 116900, \t Total Gen Loss : 34.439361572265625, \t Total Dis Loss : 0.0002034952922258526\n",
      "Steps : 117000, \t Total Gen Loss : 31.731464385986328, \t Total Dis Loss : 0.0002616900310385972\n",
      "Steps : 117100, \t Total Gen Loss : 31.50654411315918, \t Total Dis Loss : 0.00017692736582830548\n",
      "Steps : 117200, \t Total Gen Loss : 30.93926239013672, \t Total Dis Loss : 0.08196840435266495\n",
      "Steps : 117300, \t Total Gen Loss : 35.0936164855957, \t Total Dis Loss : 4.0957791497930884e-05\n",
      "Steps : 117400, \t Total Gen Loss : 33.89977264404297, \t Total Dis Loss : 0.0009979543974623084\n",
      "Steps : 117500, \t Total Gen Loss : 35.265655517578125, \t Total Dis Loss : 0.0001384959468850866\n",
      "Steps : 117600, \t Total Gen Loss : 35.740867614746094, \t Total Dis Loss : 0.0003930962411686778\n",
      "Steps : 117700, \t Total Gen Loss : 32.047027587890625, \t Total Dis Loss : 0.0005045365542173386\n",
      "Steps : 117800, \t Total Gen Loss : 31.185144424438477, \t Total Dis Loss : 0.00021796004148200154\n",
      "Steps : 117900, \t Total Gen Loss : 31.250335693359375, \t Total Dis Loss : 0.0002180040319217369\n",
      "Steps : 118000, \t Total Gen Loss : 32.184993743896484, \t Total Dis Loss : 0.00012908194912597537\n",
      "Steps : 118100, \t Total Gen Loss : 29.89906883239746, \t Total Dis Loss : 0.0002657182631082833\n",
      "Steps : 118200, \t Total Gen Loss : 32.54658889770508, \t Total Dis Loss : 6.254501204239205e-05\n",
      "Steps : 118300, \t Total Gen Loss : 31.99317741394043, \t Total Dis Loss : 0.00035394547739997506\n",
      "Steps : 118400, \t Total Gen Loss : 32.486793518066406, \t Total Dis Loss : 4.824248026125133e-05\n",
      "Steps : 118500, \t Total Gen Loss : 34.0524787902832, \t Total Dis Loss : 0.000353617942892015\n",
      "Steps : 118600, \t Total Gen Loss : 32.39778137207031, \t Total Dis Loss : 4.745104524772614e-05\n",
      "Steps : 118700, \t Total Gen Loss : 29.391822814941406, \t Total Dis Loss : 0.0004458767652977258\n",
      "Steps : 118800, \t Total Gen Loss : 30.519351959228516, \t Total Dis Loss : 8.156259718816727e-05\n",
      "Steps : 118900, \t Total Gen Loss : 29.83965492248535, \t Total Dis Loss : 0.0008737785392440856\n",
      "Steps : 119000, \t Total Gen Loss : 29.82980728149414, \t Total Dis Loss : 0.0013541949447244406\n",
      "Steps : 119100, \t Total Gen Loss : 30.462011337280273, \t Total Dis Loss : 0.00019665912259370089\n",
      "Steps : 119200, \t Total Gen Loss : 30.42481803894043, \t Total Dis Loss : 8.330248965648934e-05\n",
      "Steps : 119300, \t Total Gen Loss : 28.109405517578125, \t Total Dis Loss : 0.00014353066217154264\n",
      "Steps : 119400, \t Total Gen Loss : 31.795665740966797, \t Total Dis Loss : 2.452864828228485e-05\n",
      "Steps : 119500, \t Total Gen Loss : 30.47747802734375, \t Total Dis Loss : 4.9590231355978176e-05\n",
      "Steps : 119600, \t Total Gen Loss : 27.473133087158203, \t Total Dis Loss : 0.0012804989237338305\n",
      "Steps : 119700, \t Total Gen Loss : 28.9223575592041, \t Total Dis Loss : 0.00015335444186348468\n",
      "Steps : 119800, \t Total Gen Loss : 29.050308227539062, \t Total Dis Loss : 0.00016162033716682345\n",
      "Steps : 119900, \t Total Gen Loss : 30.63117790222168, \t Total Dis Loss : 8.068111492320895e-05\n",
      "Steps : 120000, \t Total Gen Loss : 30.67559051513672, \t Total Dis Loss : 7.593857299070805e-05\n",
      "Steps : 120100, \t Total Gen Loss : 30.035768508911133, \t Total Dis Loss : 0.0003502154431771487\n",
      "Steps : 120200, \t Total Gen Loss : 30.721698760986328, \t Total Dis Loss : 9.442622103961185e-05\n",
      "Steps : 120300, \t Total Gen Loss : 33.706817626953125, \t Total Dis Loss : 2.090732778015081e-05\n",
      "Steps : 120400, \t Total Gen Loss : 31.522151947021484, \t Total Dis Loss : 0.00015555258141830564\n",
      "Steps : 120500, \t Total Gen Loss : 34.775386810302734, \t Total Dis Loss : 3.0774419428780675e-05\n",
      "Steps : 120600, \t Total Gen Loss : 30.962753295898438, \t Total Dis Loss : 8.414508920395747e-05\n",
      "Steps : 120700, \t Total Gen Loss : 31.70741844177246, \t Total Dis Loss : 0.00010473123984411359\n",
      "Steps : 120800, \t Total Gen Loss : 32.983951568603516, \t Total Dis Loss : 2.691253394004889e-05\n",
      "Steps : 120900, \t Total Gen Loss : 33.04290771484375, \t Total Dis Loss : 3.081416798522696e-05\n",
      "Steps : 121000, \t Total Gen Loss : 32.396278381347656, \t Total Dis Loss : 4.529068974079564e-05\n",
      "Steps : 121100, \t Total Gen Loss : 36.37534713745117, \t Total Dis Loss : 3.89294882552349e-06\n",
      "Steps : 121200, \t Total Gen Loss : 31.705913543701172, \t Total Dis Loss : 3.607755934353918e-05\n",
      "Steps : 121300, \t Total Gen Loss : 32.064762115478516, \t Total Dis Loss : 1.8529945009504445e-05\n",
      "Steps : 121400, \t Total Gen Loss : 31.515689849853516, \t Total Dis Loss : 8.086625166470185e-05\n",
      "Steps : 121500, \t Total Gen Loss : 31.153278350830078, \t Total Dis Loss : 0.00021101237507537007\n",
      "Time for epoch 18 is 343.70144152641296 sec\n",
      "Steps : 121600, \t Total Gen Loss : 30.19778823852539, \t Total Dis Loss : 0.00010357845894759521\n",
      "Steps : 121700, \t Total Gen Loss : 30.88861083984375, \t Total Dis Loss : 0.0008970339549705386\n",
      "Steps : 121800, \t Total Gen Loss : 29.845821380615234, \t Total Dis Loss : 0.00036189091042615473\n",
      "Steps : 121900, \t Total Gen Loss : 29.204214096069336, \t Total Dis Loss : 0.00033623463241383433\n",
      "Steps : 122000, \t Total Gen Loss : 30.18953514099121, \t Total Dis Loss : 3.970250691054389e-05\n",
      "Steps : 122100, \t Total Gen Loss : 32.35380935668945, \t Total Dis Loss : 4.628214082913473e-05\n",
      "Steps : 122200, \t Total Gen Loss : 32.22267150878906, \t Total Dis Loss : 0.00010889137047342956\n",
      "Steps : 122300, \t Total Gen Loss : 30.086057662963867, \t Total Dis Loss : 0.0002414165501249954\n",
      "Steps : 122400, \t Total Gen Loss : 31.90581512451172, \t Total Dis Loss : 1.6522077203262597e-05\n",
      "Steps : 122500, \t Total Gen Loss : 32.56171417236328, \t Total Dis Loss : 5.100821363157593e-05\n",
      "Steps : 122600, \t Total Gen Loss : 31.032203674316406, \t Total Dis Loss : 0.000143040218972601\n",
      "Steps : 122700, \t Total Gen Loss : 33.345375061035156, \t Total Dis Loss : 1.135867387347389e-05\n",
      "Steps : 122800, \t Total Gen Loss : 29.888668060302734, \t Total Dis Loss : 0.00022536813048645854\n",
      "Steps : 122900, \t Total Gen Loss : 30.95379638671875, \t Total Dis Loss : 0.00037265539867803454\n",
      "Steps : 123000, \t Total Gen Loss : 30.20441246032715, \t Total Dis Loss : 0.0006071722600609064\n",
      "Steps : 123100, \t Total Gen Loss : 31.175086975097656, \t Total Dis Loss : 9.116143337450922e-05\n",
      "Steps : 123200, \t Total Gen Loss : 29.418964385986328, \t Total Dis Loss : 0.000509956618770957\n",
      "Steps : 123300, \t Total Gen Loss : 29.183429718017578, \t Total Dis Loss : 0.00026754633290693164\n",
      "Steps : 123400, \t Total Gen Loss : 32.47370910644531, \t Total Dis Loss : 0.00011971504136454314\n",
      "Steps : 123500, \t Total Gen Loss : 30.14990997314453, \t Total Dis Loss : 0.0006425160681828856\n",
      "Steps : 123600, \t Total Gen Loss : 32.655113220214844, \t Total Dis Loss : 5.5022621381795034e-05\n",
      "Steps : 123700, \t Total Gen Loss : 31.548681259155273, \t Total Dis Loss : 7.949521386763081e-05\n",
      "Steps : 123800, \t Total Gen Loss : 30.972166061401367, \t Total Dis Loss : 5.492070340551436e-05\n",
      "Steps : 123900, \t Total Gen Loss : 31.471967697143555, \t Total Dis Loss : 0.00010939231287920848\n",
      "Steps : 124000, \t Total Gen Loss : 29.58372688293457, \t Total Dis Loss : 0.00016812501417007297\n",
      "Steps : 124100, \t Total Gen Loss : 29.736371994018555, \t Total Dis Loss : 3.2999832910718396e-05\n",
      "Steps : 124200, \t Total Gen Loss : 32.532039642333984, \t Total Dis Loss : 8.969756891019642e-05\n",
      "Steps : 124300, \t Total Gen Loss : 30.631973266601562, \t Total Dis Loss : 0.00032899738289415836\n",
      "Steps : 124400, \t Total Gen Loss : 30.895862579345703, \t Total Dis Loss : 7.398353045573458e-05\n",
      "Steps : 124500, \t Total Gen Loss : 30.244461059570312, \t Total Dis Loss : 5.884438724024221e-05\n",
      "Steps : 124600, \t Total Gen Loss : 32.72489547729492, \t Total Dis Loss : 0.00019337776757311076\n",
      "Steps : 124700, \t Total Gen Loss : 31.8311824798584, \t Total Dis Loss : 2.527345350245014e-05\n",
      "Steps : 124800, \t Total Gen Loss : 30.812297821044922, \t Total Dis Loss : 5.871852408745326e-05\n",
      "Steps : 124900, \t Total Gen Loss : 32.539154052734375, \t Total Dis Loss : 3.460563311818987e-05\n",
      "Steps : 125000, \t Total Gen Loss : 31.650821685791016, \t Total Dis Loss : 7.167632429627702e-05\n",
      "Steps : 125100, \t Total Gen Loss : 28.68775177001953, \t Total Dis Loss : 0.0015077749267220497\n",
      "Steps : 125200, \t Total Gen Loss : 31.034440994262695, \t Total Dis Loss : 0.0001953048340510577\n",
      "Steps : 125300, \t Total Gen Loss : 30.896318435668945, \t Total Dis Loss : 6.299111555563286e-05\n",
      "Steps : 125400, \t Total Gen Loss : 31.292137145996094, \t Total Dis Loss : 4.599148815032095e-05\n",
      "Steps : 125500, \t Total Gen Loss : 31.223360061645508, \t Total Dis Loss : 6.425435276469216e-05\n",
      "Steps : 125600, \t Total Gen Loss : 28.9613094329834, \t Total Dis Loss : 0.0004982005339115858\n",
      "Steps : 125700, \t Total Gen Loss : 30.16131591796875, \t Total Dis Loss : 0.00012507881911005825\n",
      "Steps : 125800, \t Total Gen Loss : 28.170589447021484, \t Total Dis Loss : 0.00013450886763166636\n",
      "Steps : 125900, \t Total Gen Loss : 29.825746536254883, \t Total Dis Loss : 0.0006655646720901132\n",
      "Steps : 126000, \t Total Gen Loss : 29.892662048339844, \t Total Dis Loss : 9.841255086939782e-05\n",
      "Steps : 126100, \t Total Gen Loss : 29.426353454589844, \t Total Dis Loss : 7.08243896951899e-05\n",
      "Steps : 126200, \t Total Gen Loss : 29.712200164794922, \t Total Dis Loss : 2.387975837336853e-05\n",
      "Steps : 126300, \t Total Gen Loss : 31.14075469970703, \t Total Dis Loss : 1.9029310351470485e-05\n",
      "Steps : 126400, \t Total Gen Loss : 32.44733810424805, \t Total Dis Loss : 5.59541422262555e-06\n",
      "Steps : 126500, \t Total Gen Loss : 32.64152908325195, \t Total Dis Loss : 8.963121217675507e-06\n",
      "Steps : 126600, \t Total Gen Loss : 31.07192611694336, \t Total Dis Loss : 3.980538895120844e-05\n",
      "Steps : 126700, \t Total Gen Loss : 30.87411880493164, \t Total Dis Loss : 7.748686584818643e-06\n",
      "Steps : 126800, \t Total Gen Loss : 33.645267486572266, \t Total Dis Loss : 1.5967039871611632e-05\n",
      "Steps : 126900, \t Total Gen Loss : 31.06928825378418, \t Total Dis Loss : 0.0001248954504262656\n",
      "Steps : 127000, \t Total Gen Loss : 30.19448471069336, \t Total Dis Loss : 0.0008387849084101617\n",
      "Steps : 127100, \t Total Gen Loss : 31.47441291809082, \t Total Dis Loss : 0.00015180943591985852\n",
      "Steps : 127200, \t Total Gen Loss : 32.39921188354492, \t Total Dis Loss : 6.300363747868687e-05\n",
      "Steps : 127300, \t Total Gen Loss : 30.92788314819336, \t Total Dis Loss : 0.0022604127880185843\n",
      "Steps : 127400, \t Total Gen Loss : 28.56499481201172, \t Total Dis Loss : 0.0025262064300477505\n",
      "Steps : 127500, \t Total Gen Loss : 32.796546936035156, \t Total Dis Loss : 3.1366507755592465e-05\n",
      "Steps : 127600, \t Total Gen Loss : 30.63238525390625, \t Total Dis Loss : 0.00010053603909909725\n",
      "Steps : 127700, \t Total Gen Loss : 31.30243492126465, \t Total Dis Loss : 0.00046110048424452543\n",
      "Steps : 127800, \t Total Gen Loss : 33.300018310546875, \t Total Dis Loss : 0.0003224126703571528\n",
      "Steps : 127900, \t Total Gen Loss : 31.703289031982422, \t Total Dis Loss : 0.0006613291334360838\n",
      "Steps : 128000, \t Total Gen Loss : 31.32475471496582, \t Total Dis Loss : 0.00031455798307433724\n",
      "Steps : 128100, \t Total Gen Loss : 30.365833282470703, \t Total Dis Loss : 0.0002638153382577002\n",
      "Steps : 128200, \t Total Gen Loss : 31.7434139251709, \t Total Dis Loss : 0.00010677152022253722\n",
      "Time for epoch 19 is 341.2351746559143 sec\n",
      "Steps : 128300, \t Total Gen Loss : 33.52521896362305, \t Total Dis Loss : 7.899089541751891e-05\n",
      "Steps : 128400, \t Total Gen Loss : 32.08025360107422, \t Total Dis Loss : 0.002549627097323537\n",
      "Steps : 128500, \t Total Gen Loss : 32.65224075317383, \t Total Dis Loss : 0.00013971103180665523\n",
      "Steps : 128600, \t Total Gen Loss : 34.18413543701172, \t Total Dis Loss : 9.832669456955045e-05\n",
      "Steps : 128700, \t Total Gen Loss : 30.3594970703125, \t Total Dis Loss : 0.00016719875566195697\n",
      "Steps : 128800, \t Total Gen Loss : 33.02773666381836, \t Total Dis Loss : 0.00011326547246426344\n",
      "Steps : 128900, \t Total Gen Loss : 33.23854064941406, \t Total Dis Loss : 0.0002089631016133353\n",
      "Steps : 129000, \t Total Gen Loss : 31.06961441040039, \t Total Dis Loss : 5.755926395067945e-05\n",
      "Steps : 129100, \t Total Gen Loss : 32.706993103027344, \t Total Dis Loss : 0.0001376019645249471\n",
      "Steps : 129200, \t Total Gen Loss : 31.361534118652344, \t Total Dis Loss : 6.854382081655785e-05\n",
      "Steps : 129300, \t Total Gen Loss : 30.599971771240234, \t Total Dis Loss : 9.77864910964854e-05\n",
      "Steps : 129400, \t Total Gen Loss : 29.49360466003418, \t Total Dis Loss : 8.610107033746317e-05\n",
      "Steps : 129500, \t Total Gen Loss : 30.950376510620117, \t Total Dis Loss : 0.0023845527321100235\n",
      "Steps : 129600, \t Total Gen Loss : 32.480010986328125, \t Total Dis Loss : 7.175389328040183e-05\n",
      "Steps : 129700, \t Total Gen Loss : 32.36568832397461, \t Total Dis Loss : 1.3922042853664607e-05\n",
      "Steps : 129800, \t Total Gen Loss : 30.934343338012695, \t Total Dis Loss : 0.00045423320261761546\n",
      "Steps : 129900, \t Total Gen Loss : 33.46405792236328, \t Total Dis Loss : 8.203439938370138e-05\n",
      "Steps : 130000, \t Total Gen Loss : 31.613744735717773, \t Total Dis Loss : 0.00016024301294237375\n",
      "Steps : 130100, \t Total Gen Loss : 30.20157814025879, \t Total Dis Loss : 0.0005229749367572367\n",
      "Steps : 130200, \t Total Gen Loss : 30.29429817199707, \t Total Dis Loss : 0.0014885985292494297\n",
      "Steps : 130300, \t Total Gen Loss : 31.19520378112793, \t Total Dis Loss : 0.0005838460638187826\n",
      "Steps : 130400, \t Total Gen Loss : 33.22813415527344, \t Total Dis Loss : 0.00021550017117988318\n",
      "Steps : 130500, \t Total Gen Loss : 30.383163452148438, \t Total Dis Loss : 0.00017274443234782666\n",
      "Steps : 130600, \t Total Gen Loss : 32.18619918823242, \t Total Dis Loss : 4.03214362449944e-05\n",
      "Steps : 130700, \t Total Gen Loss : 31.97417640686035, \t Total Dis Loss : 5.0370108510833234e-05\n",
      "Steps : 130800, \t Total Gen Loss : 27.661495208740234, \t Total Dis Loss : 0.0006538231973536313\n",
      "Steps : 130900, \t Total Gen Loss : 33.36174392700195, \t Total Dis Loss : 8.353632438229397e-05\n",
      "Steps : 131000, \t Total Gen Loss : 32.21345901489258, \t Total Dis Loss : 8.340075146406889e-05\n",
      "Steps : 131100, \t Total Gen Loss : 32.287349700927734, \t Total Dis Loss : 4.358077421784401e-05\n",
      "Steps : 131200, \t Total Gen Loss : 31.019079208374023, \t Total Dis Loss : 8.819755748845637e-05\n",
      "Steps : 131300, \t Total Gen Loss : 31.8521671295166, \t Total Dis Loss : 6.275043415371329e-05\n",
      "Steps : 131400, \t Total Gen Loss : 32.3438835144043, \t Total Dis Loss : 4.184756107861176e-05\n",
      "Steps : 131500, \t Total Gen Loss : 31.49250030517578, \t Total Dis Loss : 8.708846871741116e-05\n",
      "Steps : 131600, \t Total Gen Loss : 31.32940673828125, \t Total Dis Loss : 5.517953832168132e-05\n",
      "Steps : 131700, \t Total Gen Loss : 33.755855560302734, \t Total Dis Loss : 5.985770985716954e-05\n",
      "Steps : 131800, \t Total Gen Loss : 33.059871673583984, \t Total Dis Loss : 6.522693729493767e-05\n",
      "Steps : 131900, \t Total Gen Loss : 30.54922866821289, \t Total Dis Loss : 2.308229340997059e-05\n",
      "Steps : 132000, \t Total Gen Loss : 29.98996925354004, \t Total Dis Loss : 0.0002664641651790589\n",
      "Steps : 132100, \t Total Gen Loss : 29.748069763183594, \t Total Dis Loss : 0.0003051600360777229\n",
      "Steps : 132200, \t Total Gen Loss : 30.991365432739258, \t Total Dis Loss : 0.0006572762504220009\n",
      "Steps : 132300, \t Total Gen Loss : 30.027690887451172, \t Total Dis Loss : 0.00019355038239154965\n",
      "Steps : 132400, \t Total Gen Loss : 31.007415771484375, \t Total Dis Loss : 0.00047956116031855345\n",
      "Steps : 132500, \t Total Gen Loss : 30.73322296142578, \t Total Dis Loss : 0.0001174437566078268\n",
      "Steps : 132600, \t Total Gen Loss : 31.414947509765625, \t Total Dis Loss : 0.00015367390005849302\n",
      "Steps : 132700, \t Total Gen Loss : 29.87581443786621, \t Total Dis Loss : 6.507118814624846e-05\n",
      "Steps : 132800, \t Total Gen Loss : 31.24077033996582, \t Total Dis Loss : 4.8501184210181236e-05\n",
      "Steps : 132900, \t Total Gen Loss : 31.59970474243164, \t Total Dis Loss : 1.7330441551166587e-05\n",
      "Steps : 133000, \t Total Gen Loss : 29.27278709411621, \t Total Dis Loss : 0.006534396205097437\n",
      "Steps : 133100, \t Total Gen Loss : 29.5178165435791, \t Total Dis Loss : 0.0008233597036451101\n",
      "Steps : 133200, \t Total Gen Loss : 30.930042266845703, \t Total Dis Loss : 0.000338064186507836\n",
      "Steps : 133300, \t Total Gen Loss : 31.449254989624023, \t Total Dis Loss : 0.00017687528452370316\n",
      "Steps : 133400, \t Total Gen Loss : 30.44533348083496, \t Total Dis Loss : 8.340987551491708e-05\n",
      "Steps : 133500, \t Total Gen Loss : 30.268402099609375, \t Total Dis Loss : 7.453634316334501e-05\n",
      "Steps : 133600, \t Total Gen Loss : 32.6707763671875, \t Total Dis Loss : 7.36858492018655e-05\n",
      "Steps : 133700, \t Total Gen Loss : 32.537742614746094, \t Total Dis Loss : 9.447028423892334e-05\n",
      "Steps : 133800, \t Total Gen Loss : 28.605030059814453, \t Total Dis Loss : 9.07778594410047e-05\n",
      "Steps : 133900, \t Total Gen Loss : 34.22886276245117, \t Total Dis Loss : 3.349737380631268e-05\n",
      "Steps : 134000, \t Total Gen Loss : 31.96367073059082, \t Total Dis Loss : 0.00011014995106961578\n",
      "Steps : 134100, \t Total Gen Loss : 31.447927474975586, \t Total Dis Loss : 2.21585578401573e-05\n",
      "Steps : 134200, \t Total Gen Loss : 30.35466766357422, \t Total Dis Loss : 2.6726285796030425e-05\n",
      "Steps : 134300, \t Total Gen Loss : 33.20428466796875, \t Total Dis Loss : 2.1920193830737844e-05\n",
      "Steps : 134400, \t Total Gen Loss : 30.801603317260742, \t Total Dis Loss : 0.00023049730225466192\n",
      "Steps : 134500, \t Total Gen Loss : 30.77530860900879, \t Total Dis Loss : 7.862558413762599e-05\n",
      "Steps : 134600, \t Total Gen Loss : 31.265823364257812, \t Total Dis Loss : 8.806037658359855e-05\n",
      "Steps : 134700, \t Total Gen Loss : 33.06962585449219, \t Total Dis Loss : 2.7564454285311513e-05\n",
      "Steps : 134800, \t Total Gen Loss : 33.833282470703125, \t Total Dis Loss : 2.3145687009673566e-05\n",
      "Steps : 134900, \t Total Gen Loss : 32.33639907836914, \t Total Dis Loss : 1.4715244105900638e-05\n",
      "Steps : 135000, \t Total Gen Loss : 33.22322463989258, \t Total Dis Loss : 9.354278518003412e-06\n",
      "Time for epoch 20 is 344.36448526382446 sec\n",
      "Steps : 135100, \t Total Gen Loss : 28.803470611572266, \t Total Dis Loss : 0.0006323371781036258\n",
      "Steps : 135200, \t Total Gen Loss : 30.6213321685791, \t Total Dis Loss : 0.00024014960217755288\n",
      "Steps : 135300, \t Total Gen Loss : 31.751970291137695, \t Total Dis Loss : 0.00010532661690376699\n",
      "Steps : 135400, \t Total Gen Loss : 31.337209701538086, \t Total Dis Loss : 0.00046224455581977963\n",
      "Steps : 135500, \t Total Gen Loss : 30.129039764404297, \t Total Dis Loss : 0.00020928142475895584\n",
      "Steps : 135600, \t Total Gen Loss : 30.176279067993164, \t Total Dis Loss : 0.0003601413336582482\n",
      "Steps : 135700, \t Total Gen Loss : 31.0963191986084, \t Total Dis Loss : 8.87807400431484e-05\n",
      "Steps : 135800, \t Total Gen Loss : 27.34282112121582, \t Total Dis Loss : 0.0006473333342000842\n",
      "Steps : 135900, \t Total Gen Loss : 28.66002082824707, \t Total Dis Loss : 0.00015390625048894435\n",
      "Steps : 136000, \t Total Gen Loss : 30.527589797973633, \t Total Dis Loss : 0.00014909132733009756\n",
      "Steps : 136100, \t Total Gen Loss : 26.803346633911133, \t Total Dis Loss : 0.0002711024135351181\n",
      "Steps : 136200, \t Total Gen Loss : 32.70673751831055, \t Total Dis Loss : 4.56094930996187e-05\n",
      "Steps : 136300, \t Total Gen Loss : 32.5065803527832, \t Total Dis Loss : 5.923060598433949e-05\n",
      "Steps : 136400, \t Total Gen Loss : 31.42336654663086, \t Total Dis Loss : 8.068403985816985e-05\n",
      "Steps : 136500, \t Total Gen Loss : 32.59562301635742, \t Total Dis Loss : 0.00017146939353551716\n",
      "Steps : 136600, \t Total Gen Loss : 38.652374267578125, \t Total Dis Loss : 0.003132564714178443\n",
      "Steps : 136700, \t Total Gen Loss : 32.64579772949219, \t Total Dis Loss : 0.0010526599362492561\n",
      "Steps : 136800, \t Total Gen Loss : 33.495994567871094, \t Total Dis Loss : 0.002349680056795478\n",
      "Steps : 136900, \t Total Gen Loss : 36.61650466918945, \t Total Dis Loss : 0.0002521932474337518\n",
      "Steps : 137000, \t Total Gen Loss : 33.82627487182617, \t Total Dis Loss : 0.0003101436304859817\n",
      "Steps : 137100, \t Total Gen Loss : 31.418819427490234, \t Total Dis Loss : 3.567200474208221e-05\n",
      "Steps : 137200, \t Total Gen Loss : 30.181631088256836, \t Total Dis Loss : 0.0001578728697495535\n",
      "Steps : 137300, \t Total Gen Loss : 34.251190185546875, \t Total Dis Loss : 3.474779077805579e-05\n",
      "Steps : 137400, \t Total Gen Loss : 33.08662414550781, \t Total Dis Loss : 0.00011040177196264267\n",
      "Steps : 137500, \t Total Gen Loss : 29.3869686126709, \t Total Dis Loss : 0.0002747199032455683\n",
      "Steps : 137600, \t Total Gen Loss : 30.34947395324707, \t Total Dis Loss : 0.0001248550834134221\n",
      "Steps : 137700, \t Total Gen Loss : 33.708457946777344, \t Total Dis Loss : 5.3434137953445315e-05\n",
      "Steps : 137800, \t Total Gen Loss : 31.909896850585938, \t Total Dis Loss : 9.774202771950513e-05\n",
      "Steps : 137900, \t Total Gen Loss : 31.350112915039062, \t Total Dis Loss : 0.00036322191590443254\n",
      "Steps : 138000, \t Total Gen Loss : 32.42389678955078, \t Total Dis Loss : 9.286671411246061e-05\n",
      "Steps : 138100, \t Total Gen Loss : 30.757970809936523, \t Total Dis Loss : 9.779158426681533e-05\n",
      "Steps : 138200, \t Total Gen Loss : 30.586334228515625, \t Total Dis Loss : 0.00013664443395100534\n",
      "Steps : 138300, \t Total Gen Loss : 34.64404296875, \t Total Dis Loss : 0.006624745205044746\n",
      "Steps : 138400, \t Total Gen Loss : 34.97580337524414, \t Total Dis Loss : 1.5210758647299372e-05\n",
      "Steps : 138500, \t Total Gen Loss : 30.74676513671875, \t Total Dis Loss : 7.165722490753978e-05\n",
      "Steps : 138600, \t Total Gen Loss : 30.173961639404297, \t Total Dis Loss : 7.753631507512182e-05\n",
      "Steps : 138700, \t Total Gen Loss : 29.704687118530273, \t Total Dis Loss : 0.0001985454000532627\n",
      "Steps : 138800, \t Total Gen Loss : 29.383413314819336, \t Total Dis Loss : 0.00013447135279420763\n",
      "Steps : 138900, \t Total Gen Loss : 32.091949462890625, \t Total Dis Loss : 0.0001555593335069716\n",
      "Steps : 139000, \t Total Gen Loss : 32.17158889770508, \t Total Dis Loss : 0.00010779120202641934\n",
      "Steps : 139100, \t Total Gen Loss : 32.57862854003906, \t Total Dis Loss : 5.524553853319958e-05\n",
      "Steps : 139200, \t Total Gen Loss : 30.82393455505371, \t Total Dis Loss : 7.587512664031237e-05\n",
      "Steps : 139300, \t Total Gen Loss : 31.185962677001953, \t Total Dis Loss : 9.633573063183576e-05\n",
      "Steps : 139400, \t Total Gen Loss : 29.579631805419922, \t Total Dis Loss : 0.00012278252688702196\n",
      "Steps : 139500, \t Total Gen Loss : 31.999549865722656, \t Total Dis Loss : 8.756082388572395e-05\n",
      "Steps : 139600, \t Total Gen Loss : 31.16037368774414, \t Total Dis Loss : 6.721420504618436e-05\n",
      "Steps : 139700, \t Total Gen Loss : 31.492759704589844, \t Total Dis Loss : 5.426195275504142e-05\n",
      "Steps : 139800, \t Total Gen Loss : 32.01218032836914, \t Total Dis Loss : 4.966809501638636e-05\n",
      "Steps : 139900, \t Total Gen Loss : 29.94048500061035, \t Total Dis Loss : 7.170612661866471e-05\n",
      "Steps : 140000, \t Total Gen Loss : 30.998395919799805, \t Total Dis Loss : 5.594959293375723e-05\n",
      "Steps : 140100, \t Total Gen Loss : 31.267181396484375, \t Total Dis Loss : 4.5272161514731124e-05\n",
      "Steps : 140200, \t Total Gen Loss : 31.527179718017578, \t Total Dis Loss : 5.345702447812073e-05\n",
      "Steps : 140300, \t Total Gen Loss : 31.702871322631836, \t Total Dis Loss : 4.008938049082644e-05\n",
      "Steps : 140400, \t Total Gen Loss : 30.68225860595703, \t Total Dis Loss : 4.138581789447926e-05\n",
      "Steps : 140500, \t Total Gen Loss : 28.803749084472656, \t Total Dis Loss : 3.3350061130477116e-05\n",
      "Steps : 140600, \t Total Gen Loss : 32.03999710083008, \t Total Dis Loss : 3.0596667784266174e-05\n",
      "Steps : 140700, \t Total Gen Loss : 32.284664154052734, \t Total Dis Loss : 2.7497095288708806e-05\n",
      "Steps : 140800, \t Total Gen Loss : 32.492855072021484, \t Total Dis Loss : 1.9334653188707307e-05\n",
      "Steps : 140900, \t Total Gen Loss : 32.4190559387207, \t Total Dis Loss : 2.373816641920712e-05\n",
      "Steps : 141000, \t Total Gen Loss : 31.48505973815918, \t Total Dis Loss : 3.447131530265324e-05\n",
      "Steps : 141100, \t Total Gen Loss : 31.74163055419922, \t Total Dis Loss : 3.161384302075021e-05\n",
      "Steps : 141200, \t Total Gen Loss : 31.323810577392578, \t Total Dis Loss : 4.2537179979262874e-05\n",
      "Steps : 141300, \t Total Gen Loss : 29.091785430908203, \t Total Dis Loss : 0.0002670236281119287\n",
      "Steps : 141400, \t Total Gen Loss : 29.680343627929688, \t Total Dis Loss : 0.00023105541185941547\n",
      "Steps : 141500, \t Total Gen Loss : 31.418704986572266, \t Total Dis Loss : 0.00012181622150819749\n",
      "Steps : 141600, \t Total Gen Loss : 30.593196868896484, \t Total Dis Loss : 5.782035441370681e-05\n",
      "Steps : 141700, \t Total Gen Loss : 31.289960861206055, \t Total Dis Loss : 0.00011403634562157094\n",
      "Time for epoch 21 is 344.6422772407532 sec\n",
      "Steps : 141800, \t Total Gen Loss : 29.398578643798828, \t Total Dis Loss : 0.00013094415771774948\n",
      "Steps : 141900, \t Total Gen Loss : 30.563526153564453, \t Total Dis Loss : 0.00022538845951203257\n",
      "Steps : 142000, \t Total Gen Loss : 31.552528381347656, \t Total Dis Loss : 6.278751243371516e-05\n",
      "Steps : 142100, \t Total Gen Loss : 31.276817321777344, \t Total Dis Loss : 4.216131128487177e-05\n",
      "Steps : 142200, \t Total Gen Loss : 29.328105926513672, \t Total Dis Loss : 0.0004029925330542028\n",
      "Steps : 142300, \t Total Gen Loss : 29.468521118164062, \t Total Dis Loss : 0.0001128773728851229\n",
      "Steps : 142400, \t Total Gen Loss : 31.16164779663086, \t Total Dis Loss : 7.785834168316796e-05\n",
      "Steps : 142500, \t Total Gen Loss : 31.7989501953125, \t Total Dis Loss : 6.469367508543655e-05\n",
      "Steps : 142600, \t Total Gen Loss : 31.117334365844727, \t Total Dis Loss : 6.266799755394459e-05\n",
      "Steps : 142700, \t Total Gen Loss : 30.850723266601562, \t Total Dis Loss : 6.831991777289659e-05\n",
      "Steps : 142800, \t Total Gen Loss : 30.084562301635742, \t Total Dis Loss : 5.981074718874879e-05\n",
      "Steps : 142900, \t Total Gen Loss : 32.82856369018555, \t Total Dis Loss : 3.278378426330164e-05\n",
      "Steps : 143000, \t Total Gen Loss : 31.387189865112305, \t Total Dis Loss : 8.860866364557296e-05\n",
      "Steps : 143100, \t Total Gen Loss : 32.18006134033203, \t Total Dis Loss : 0.00014301038754638284\n",
      "Steps : 143200, \t Total Gen Loss : 30.255630493164062, \t Total Dis Loss : 9.495514677837491e-05\n",
      "Steps : 143300, \t Total Gen Loss : 32.89513397216797, \t Total Dis Loss : 0.0006546784425154328\n",
      "Steps : 143400, \t Total Gen Loss : 31.8640193939209, \t Total Dis Loss : 9.888861677609384e-05\n",
      "Steps : 143500, \t Total Gen Loss : 32.621482849121094, \t Total Dis Loss : 0.00011841729428851977\n",
      "Steps : 143600, \t Total Gen Loss : 31.20052719116211, \t Total Dis Loss : 0.0003019850410055369\n",
      "Steps : 143700, \t Total Gen Loss : 26.051326751708984, \t Total Dis Loss : 0.02204052358865738\n",
      "Steps : 143800, \t Total Gen Loss : 30.310733795166016, \t Total Dis Loss : 0.0067991772666573524\n",
      "Steps : 143900, \t Total Gen Loss : 29.116172790527344, \t Total Dis Loss : 0.006169935688376427\n",
      "Steps : 144000, \t Total Gen Loss : 27.403823852539062, \t Total Dis Loss : 0.002125635277479887\n",
      "Steps : 144100, \t Total Gen Loss : 28.508108139038086, \t Total Dis Loss : 0.0077360207214951515\n",
      "Steps : 144200, \t Total Gen Loss : 28.45039176940918, \t Total Dis Loss : 0.0018307685386389494\n",
      "Steps : 144300, \t Total Gen Loss : 25.70384979248047, \t Total Dis Loss : 0.009881741367280483\n",
      "Steps : 144400, \t Total Gen Loss : 28.674606323242188, \t Total Dis Loss : 0.0021596276201307774\n",
      "Steps : 144500, \t Total Gen Loss : 28.769813537597656, \t Total Dis Loss : 0.0018142107874155045\n",
      "Steps : 144600, \t Total Gen Loss : 28.81635856628418, \t Total Dis Loss : 0.0014175699325278401\n",
      "Steps : 144700, \t Total Gen Loss : 26.67726707458496, \t Total Dis Loss : 0.0016772020608186722\n",
      "Steps : 144800, \t Total Gen Loss : 30.4267578125, \t Total Dis Loss : 0.0010978755308315158\n",
      "Steps : 144900, \t Total Gen Loss : 26.957216262817383, \t Total Dis Loss : 0.0016133013414219022\n",
      "Steps : 145000, \t Total Gen Loss : 26.447265625, \t Total Dis Loss : 0.0009253231692127883\n",
      "Steps : 145100, \t Total Gen Loss : 28.682004928588867, \t Total Dis Loss : 0.0006305038114078343\n",
      "Steps : 145200, \t Total Gen Loss : 27.75208282470703, \t Total Dis Loss : 0.0012191857676953077\n",
      "Steps : 145300, \t Total Gen Loss : 32.03009033203125, \t Total Dis Loss : 0.010127517394721508\n",
      "Steps : 145400, \t Total Gen Loss : 29.027559280395508, \t Total Dis Loss : 0.0007702992297708988\n",
      "Steps : 145500, \t Total Gen Loss : 30.7814998626709, \t Total Dis Loss : 0.0012661932269111276\n",
      "Steps : 145600, \t Total Gen Loss : 30.31745147705078, \t Total Dis Loss : 0.004847923293709755\n",
      "Steps : 145700, \t Total Gen Loss : 27.67498016357422, \t Total Dis Loss : 0.00037915754364803433\n",
      "Steps : 145800, \t Total Gen Loss : 29.551761627197266, \t Total Dis Loss : 0.00033964315662160516\n",
      "Steps : 145900, \t Total Gen Loss : 30.113197326660156, \t Total Dis Loss : 0.0005430834135040641\n",
      "Steps : 146000, \t Total Gen Loss : 27.708709716796875, \t Total Dis Loss : 0.0005652591935358942\n",
      "Steps : 146100, \t Total Gen Loss : 30.47410774230957, \t Total Dis Loss : 0.00015683285892009735\n",
      "Steps : 146200, \t Total Gen Loss : 29.167781829833984, \t Total Dis Loss : 0.00026903682737611234\n",
      "Steps : 146300, \t Total Gen Loss : 28.187698364257812, \t Total Dis Loss : 0.0012570309918373823\n",
      "Steps : 146400, \t Total Gen Loss : 29.58582878112793, \t Total Dis Loss : 0.00032776675652712584\n",
      "Steps : 146500, \t Total Gen Loss : 29.25820541381836, \t Total Dis Loss : 0.0005655365530401468\n",
      "Steps : 146600, \t Total Gen Loss : 27.65360450744629, \t Total Dis Loss : 0.0014965836890041828\n",
      "Steps : 146700, \t Total Gen Loss : 29.499008178710938, \t Total Dis Loss : 0.0003936575376428664\n",
      "Steps : 146800, \t Total Gen Loss : 29.910533905029297, \t Total Dis Loss : 0.0004872175049968064\n",
      "Steps : 146900, \t Total Gen Loss : 30.011402130126953, \t Total Dis Loss : 0.0013260423438623548\n",
      "Steps : 147000, \t Total Gen Loss : 30.814115524291992, \t Total Dis Loss : 0.0007699317065998912\n",
      "Steps : 147100, \t Total Gen Loss : 32.39301300048828, \t Total Dis Loss : 0.00020912144100293517\n",
      "Steps : 147200, \t Total Gen Loss : 31.20170021057129, \t Total Dis Loss : 0.0004546880954876542\n",
      "Steps : 147300, \t Total Gen Loss : 29.91115951538086, \t Total Dis Loss : 0.0010440779151394963\n",
      "Steps : 147400, \t Total Gen Loss : 33.8056640625, \t Total Dis Loss : 0.07058104872703552\n",
      "Steps : 147500, \t Total Gen Loss : 27.442888259887695, \t Total Dis Loss : 0.0005205041379667819\n",
      "Steps : 147600, \t Total Gen Loss : 29.61232566833496, \t Total Dis Loss : 0.00028914230642840266\n",
      "Steps : 147700, \t Total Gen Loss : 30.063064575195312, \t Total Dis Loss : 0.00012888916535302997\n",
      "Steps : 147800, \t Total Gen Loss : 29.80569839477539, \t Total Dis Loss : 0.00024286100233439356\n",
      "Steps : 147900, \t Total Gen Loss : 30.97251319885254, \t Total Dis Loss : 0.0002061559644062072\n",
      "Steps : 148000, \t Total Gen Loss : 30.246963500976562, \t Total Dis Loss : 0.000612939998973161\n",
      "Steps : 148100, \t Total Gen Loss : 30.87110710144043, \t Total Dis Loss : 0.00029982085106894374\n",
      "Steps : 148200, \t Total Gen Loss : 31.990928649902344, \t Total Dis Loss : 0.00019587672431953251\n",
      "Steps : 148300, \t Total Gen Loss : 29.441617965698242, \t Total Dis Loss : 0.0003289980813860893\n",
      "Steps : 148400, \t Total Gen Loss : 29.515945434570312, \t Total Dis Loss : 8.854683255776763e-05\n",
      "Steps : 148500, \t Total Gen Loss : 30.58823013305664, \t Total Dis Loss : 0.0001339732261840254\n",
      "Time for epoch 22 is 338.3309597969055 sec\n",
      "Steps : 148600, \t Total Gen Loss : 29.889310836791992, \t Total Dis Loss : 0.00020053135813213885\n",
      "Steps : 148700, \t Total Gen Loss : 33.17219161987305, \t Total Dis Loss : 0.00011219348380109295\n",
      "Steps : 148800, \t Total Gen Loss : 32.07769775390625, \t Total Dis Loss : 0.0004894120502285659\n",
      "Steps : 148900, \t Total Gen Loss : 37.947425842285156, \t Total Dis Loss : 0.004415825009346008\n",
      "Steps : 149000, \t Total Gen Loss : 33.21160888671875, \t Total Dis Loss : 0.0013300678692758083\n",
      "Steps : 149100, \t Total Gen Loss : 30.944795608520508, \t Total Dis Loss : 0.003939340356737375\n",
      "Steps : 149200, \t Total Gen Loss : 32.13116455078125, \t Total Dis Loss : 0.00035414728336036205\n",
      "Steps : 149300, \t Total Gen Loss : 31.805744171142578, \t Total Dis Loss : 0.0003964729839935899\n",
      "Steps : 149400, \t Total Gen Loss : 31.64716911315918, \t Total Dis Loss : 0.0010146457934752107\n",
      "Steps : 149500, \t Total Gen Loss : 33.73984909057617, \t Total Dis Loss : 0.0005392323364503682\n",
      "Steps : 149600, \t Total Gen Loss : 33.2135124206543, \t Total Dis Loss : 0.00030837953090667725\n",
      "Steps : 149700, \t Total Gen Loss : 36.610626220703125, \t Total Dis Loss : 0.00019386544590815902\n",
      "Steps : 149800, \t Total Gen Loss : 34.2733154296875, \t Total Dis Loss : 0.00026404959498904645\n",
      "Steps : 149900, \t Total Gen Loss : 35.922523498535156, \t Total Dis Loss : 9.967942605726421e-05\n",
      "Steps : 150000, \t Total Gen Loss : 31.574352264404297, \t Total Dis Loss : 0.0003303533303551376\n",
      "Steps : 150100, \t Total Gen Loss : 29.171831130981445, \t Total Dis Loss : 0.0037596463225781918\n",
      "Steps : 150200, \t Total Gen Loss : 35.137428283691406, \t Total Dis Loss : 8.354461897397414e-05\n",
      "Steps : 150300, \t Total Gen Loss : 32.98896026611328, \t Total Dis Loss : 0.0004237879766151309\n",
      "Steps : 150400, \t Total Gen Loss : 32.956111907958984, \t Total Dis Loss : 0.0004268368938937783\n",
      "Steps : 150500, \t Total Gen Loss : 32.689964294433594, \t Total Dis Loss : 0.00019934249576181173\n",
      "Steps : 150600, \t Total Gen Loss : 30.541629791259766, \t Total Dis Loss : 0.00020787458925042301\n",
      "Steps : 150700, \t Total Gen Loss : 32.57382583618164, \t Total Dis Loss : 6.184038647916168e-05\n",
      "Steps : 150800, \t Total Gen Loss : 31.188140869140625, \t Total Dis Loss : 0.0001393261773046106\n",
      "Steps : 150900, \t Total Gen Loss : 32.057044982910156, \t Total Dis Loss : 0.00027928626514039934\n",
      "Steps : 151000, \t Total Gen Loss : 30.781095504760742, \t Total Dis Loss : 0.0004017970059067011\n",
      "Steps : 151100, \t Total Gen Loss : 28.707008361816406, \t Total Dis Loss : 0.0006614638841710985\n",
      "Steps : 151200, \t Total Gen Loss : 29.62174415588379, \t Total Dis Loss : 0.0005778010236099362\n",
      "Steps : 151300, \t Total Gen Loss : 35.86738204956055, \t Total Dis Loss : 0.00017267774092033505\n",
      "Steps : 151400, \t Total Gen Loss : 32.396705627441406, \t Total Dis Loss : 0.0003389393677935004\n",
      "Steps : 151500, \t Total Gen Loss : 32.796417236328125, \t Total Dis Loss : 4.050050483783707e-05\n",
      "Steps : 151600, \t Total Gen Loss : 33.54938888549805, \t Total Dis Loss : 0.000781111535616219\n",
      "Steps : 151700, \t Total Gen Loss : 32.32056427001953, \t Total Dis Loss : 0.0010255746310576797\n",
      "Steps : 151800, \t Total Gen Loss : 32.03963088989258, \t Total Dis Loss : 0.00012664309178944677\n",
      "Steps : 151900, \t Total Gen Loss : 35.02739334106445, \t Total Dis Loss : 0.00013034010771661997\n",
      "Steps : 152000, \t Total Gen Loss : 30.248287200927734, \t Total Dis Loss : 0.002161980839446187\n",
      "Steps : 152100, \t Total Gen Loss : 31.148460388183594, \t Total Dis Loss : 0.0006045810878276825\n",
      "Steps : 152200, \t Total Gen Loss : 28.579591751098633, \t Total Dis Loss : 0.00251084310002625\n",
      "Steps : 152300, \t Total Gen Loss : 32.92286682128906, \t Total Dis Loss : 0.00048532828805036843\n",
      "Steps : 152400, \t Total Gen Loss : 34.02912139892578, \t Total Dis Loss : 0.00012815666559617966\n",
      "Steps : 152500, \t Total Gen Loss : 32.36444091796875, \t Total Dis Loss : 8.979978156276047e-05\n",
      "Steps : 152600, \t Total Gen Loss : 33.66494369506836, \t Total Dis Loss : 6.873748498037457e-05\n",
      "Steps : 152700, \t Total Gen Loss : 35.022666931152344, \t Total Dis Loss : 1.0229765393887646e-05\n",
      "Steps : 152800, \t Total Gen Loss : 33.50384521484375, \t Total Dis Loss : 0.0001569596497574821\n",
      "Steps : 152900, \t Total Gen Loss : 32.29175567626953, \t Total Dis Loss : 0.0003156074963044375\n",
      "Steps : 153000, \t Total Gen Loss : 32.80230712890625, \t Total Dis Loss : 2.8857533834525384e-05\n",
      "Steps : 153100, \t Total Gen Loss : 36.428035736083984, \t Total Dis Loss : 3.7508456443902105e-05\n",
      "Steps : 153200, \t Total Gen Loss : 37.35784912109375, \t Total Dis Loss : 3.282250327174552e-05\n",
      "Steps : 153300, \t Total Gen Loss : 34.250492095947266, \t Total Dis Loss : 0.00047016789903864264\n",
      "Steps : 153400, \t Total Gen Loss : 34.41252899169922, \t Total Dis Loss : 0.0001522769161965698\n",
      "Steps : 153500, \t Total Gen Loss : 33.049659729003906, \t Total Dis Loss : 0.0002440432144794613\n",
      "Steps : 153600, \t Total Gen Loss : 31.677139282226562, \t Total Dis Loss : 0.0008136239484883845\n",
      "Steps : 153700, \t Total Gen Loss : 33.222801208496094, \t Total Dis Loss : 0.00015183247160166502\n",
      "Steps : 153800, \t Total Gen Loss : 33.92859649658203, \t Total Dis Loss : 0.0003257470962125808\n",
      "Steps : 153900, \t Total Gen Loss : 34.633872985839844, \t Total Dis Loss : 0.0003530264366418123\n",
      "Steps : 154000, \t Total Gen Loss : 32.87641143798828, \t Total Dis Loss : 0.001312473090365529\n",
      "Steps : 154100, \t Total Gen Loss : 33.62266159057617, \t Total Dis Loss : 0.0018809979083016515\n",
      "Steps : 154200, \t Total Gen Loss : 35.93149948120117, \t Total Dis Loss : 0.00013364112237468362\n",
      "Steps : 154300, \t Total Gen Loss : 34.96028137207031, \t Total Dis Loss : 0.0005588633357547224\n",
      "Steps : 154400, \t Total Gen Loss : 37.06817626953125, \t Total Dis Loss : 1.3601473256130703e-05\n",
      "Steps : 154500, \t Total Gen Loss : 35.484500885009766, \t Total Dis Loss : 0.00010850765829673037\n",
      "Steps : 154600, \t Total Gen Loss : 35.5471076965332, \t Total Dis Loss : 0.00015668594278395176\n",
      "Steps : 154700, \t Total Gen Loss : 37.98543167114258, \t Total Dis Loss : 1.4398633538803551e-05\n",
      "Steps : 154800, \t Total Gen Loss : 32.022586822509766, \t Total Dis Loss : 0.0012114085257053375\n",
      "Steps : 154900, \t Total Gen Loss : 35.73740005493164, \t Total Dis Loss : 0.00024211501295212656\n",
      "Steps : 155000, \t Total Gen Loss : 35.548892974853516, \t Total Dis Loss : 0.00027741497615352273\n",
      "Steps : 155100, \t Total Gen Loss : 38.19019317626953, \t Total Dis Loss : 0.000147679602378048\n",
      "Steps : 155200, \t Total Gen Loss : 37.5820426940918, \t Total Dis Loss : 7.175314385676757e-05\n",
      "Time for epoch 23 is 336.79826617240906 sec\n",
      "Steps : 155300, \t Total Gen Loss : 32.85214614868164, \t Total Dis Loss : 0.00014890191960148513\n",
      "Steps : 155400, \t Total Gen Loss : 36.362823486328125, \t Total Dis Loss : 0.0005313641158863902\n",
      "Steps : 155500, \t Total Gen Loss : 33.560264587402344, \t Total Dis Loss : 0.0006925152847543359\n",
      "Steps : 155600, \t Total Gen Loss : 34.62334442138672, \t Total Dis Loss : 0.0006254284526221454\n",
      "Steps : 155700, \t Total Gen Loss : 36.05276870727539, \t Total Dis Loss : 0.0004845979856327176\n",
      "Steps : 155800, \t Total Gen Loss : 35.159324645996094, \t Total Dis Loss : 0.0002121743164025247\n",
      "Steps : 155900, \t Total Gen Loss : 36.22002410888672, \t Total Dis Loss : 4.470550629775971e-05\n",
      "Steps : 156000, \t Total Gen Loss : 35.1823844909668, \t Total Dis Loss : 6.410334026440978e-05\n",
      "Steps : 156100, \t Total Gen Loss : 32.38389205932617, \t Total Dis Loss : 0.0013904429506510496\n",
      "Steps : 156200, \t Total Gen Loss : 32.574462890625, \t Total Dis Loss : 0.0002767122641671449\n",
      "Steps : 156300, \t Total Gen Loss : 32.599483489990234, \t Total Dis Loss : 0.00016028026584535837\n",
      "Steps : 156400, \t Total Gen Loss : 32.82252502441406, \t Total Dis Loss : 0.00036510260542854667\n",
      "Steps : 156500, \t Total Gen Loss : 32.54175567626953, \t Total Dis Loss : 8.419061487074941e-05\n",
      "Steps : 156600, \t Total Gen Loss : 32.570167541503906, \t Total Dis Loss : 9.283049439545721e-05\n",
      "Steps : 156700, \t Total Gen Loss : 37.202178955078125, \t Total Dis Loss : 0.00043698324589058757\n",
      "Steps : 156800, \t Total Gen Loss : 36.36014938354492, \t Total Dis Loss : 0.00011916236690012738\n",
      "Steps : 156900, \t Total Gen Loss : 37.85916519165039, \t Total Dis Loss : 0.0007393076084554195\n",
      "Steps : 157000, \t Total Gen Loss : 33.9599609375, \t Total Dis Loss : 0.00020948523888364434\n",
      "Steps : 157100, \t Total Gen Loss : 32.14516830444336, \t Total Dis Loss : 0.00013610949099529535\n",
      "Steps : 157200, \t Total Gen Loss : 33.73765182495117, \t Total Dis Loss : 0.00016485956439282745\n",
      "Steps : 157300, \t Total Gen Loss : 34.56256103515625, \t Total Dis Loss : 3.387965261936188e-05\n",
      "Steps : 157400, \t Total Gen Loss : 35.3008918762207, \t Total Dis Loss : 0.0016069747507572174\n",
      "Steps : 157500, \t Total Gen Loss : 34.48094177246094, \t Total Dis Loss : 0.0002704764483496547\n",
      "Steps : 157600, \t Total Gen Loss : 32.784637451171875, \t Total Dis Loss : 0.00019819478620775044\n",
      "Steps : 157700, \t Total Gen Loss : 33.22331237792969, \t Total Dis Loss : 6.407655746443197e-05\n",
      "Steps : 157800, \t Total Gen Loss : 34.44693374633789, \t Total Dis Loss : 0.0001994119957089424\n",
      "Steps : 157900, \t Total Gen Loss : 35.108909606933594, \t Total Dis Loss : 1.4886506505717989e-05\n",
      "Steps : 158000, \t Total Gen Loss : 34.529808044433594, \t Total Dis Loss : 0.0002574099344201386\n",
      "Steps : 158100, \t Total Gen Loss : 34.453372955322266, \t Total Dis Loss : 0.0028265640139579773\n",
      "Steps : 158200, \t Total Gen Loss : 32.53004837036133, \t Total Dis Loss : 0.0005291984416544437\n",
      "Steps : 158300, \t Total Gen Loss : 34.45946502685547, \t Total Dis Loss : 0.00010062397632282227\n",
      "Steps : 158400, \t Total Gen Loss : 34.58918380737305, \t Total Dis Loss : 7.619693496963009e-05\n",
      "Steps : 158500, \t Total Gen Loss : 35.04787826538086, \t Total Dis Loss : 0.0005755330785177648\n",
      "Steps : 158600, \t Total Gen Loss : 33.36378479003906, \t Total Dis Loss : 0.00024272961309179664\n",
      "Steps : 158700, \t Total Gen Loss : 34.44321060180664, \t Total Dis Loss : 0.00034983528894372284\n",
      "Steps : 158800, \t Total Gen Loss : 32.16941833496094, \t Total Dis Loss : 0.0016084382077679038\n",
      "Steps : 158900, \t Total Gen Loss : 34.63844680786133, \t Total Dis Loss : 0.0002635371347423643\n",
      "Steps : 159000, \t Total Gen Loss : 32.75954818725586, \t Total Dis Loss : 0.0003326250007376075\n",
      "Steps : 159100, \t Total Gen Loss : 35.49679183959961, \t Total Dis Loss : 9.102150215767324e-05\n",
      "Steps : 159200, \t Total Gen Loss : 34.91566848754883, \t Total Dis Loss : 0.0005243218620307744\n",
      "Steps : 159300, \t Total Gen Loss : 35.38097381591797, \t Total Dis Loss : 0.0015932287788018584\n",
      "Steps : 159400, \t Total Gen Loss : 31.443777084350586, \t Total Dis Loss : 0.0024923395831137896\n",
      "Steps : 159500, \t Total Gen Loss : 33.23350524902344, \t Total Dis Loss : 0.0004892019787803292\n",
      "Steps : 159600, \t Total Gen Loss : 35.241737365722656, \t Total Dis Loss : 0.000447231053840369\n",
      "Steps : 159700, \t Total Gen Loss : 38.950199127197266, \t Total Dis Loss : 0.0003307107253931463\n",
      "Steps : 159800, \t Total Gen Loss : 32.65513610839844, \t Total Dis Loss : 0.001075593987479806\n",
      "Steps : 159900, \t Total Gen Loss : 33.116024017333984, \t Total Dis Loss : 0.0001266797917196527\n",
      "Steps : 160000, \t Total Gen Loss : 34.03617477416992, \t Total Dis Loss : 0.0005764725501649082\n",
      "Steps : 160100, \t Total Gen Loss : 35.98078918457031, \t Total Dis Loss : 5.204160333960317e-05\n",
      "Steps : 160200, \t Total Gen Loss : 35.59701919555664, \t Total Dis Loss : 0.0005975992535240948\n",
      "Steps : 160300, \t Total Gen Loss : 32.05979537963867, \t Total Dis Loss : 0.00043077053851448\n",
      "Steps : 160400, \t Total Gen Loss : 33.97138977050781, \t Total Dis Loss : 0.0017828041454777122\n",
      "Steps : 160500, \t Total Gen Loss : 37.853919982910156, \t Total Dis Loss : 0.00015984950005076826\n",
      "Steps : 160600, \t Total Gen Loss : 34.874080657958984, \t Total Dis Loss : 0.00015640773926861584\n",
      "Steps : 160700, \t Total Gen Loss : 33.57537078857422, \t Total Dis Loss : 0.00020079012028872967\n",
      "Steps : 160800, \t Total Gen Loss : 34.46063232421875, \t Total Dis Loss : 7.917352195363492e-05\n",
      "Steps : 160900, \t Total Gen Loss : 34.05305480957031, \t Total Dis Loss : 0.00021478325652424246\n",
      "Steps : 161000, \t Total Gen Loss : 33.626895904541016, \t Total Dis Loss : 0.00043342020944692194\n",
      "Steps : 161100, \t Total Gen Loss : 33.85887145996094, \t Total Dis Loss : 0.0008421667735092342\n",
      "Steps : 161200, \t Total Gen Loss : 34.19740295410156, \t Total Dis Loss : 8.856921340338886e-05\n",
      "Steps : 161300, \t Total Gen Loss : 34.913394927978516, \t Total Dis Loss : 0.00014267436927184463\n",
      "Steps : 161400, \t Total Gen Loss : 35.6250114440918, \t Total Dis Loss : 3.565047518350184e-05\n",
      "Steps : 161500, \t Total Gen Loss : 32.862457275390625, \t Total Dis Loss : 7.981224916875362e-05\n",
      "Steps : 161600, \t Total Gen Loss : 31.757753372192383, \t Total Dis Loss : 0.00030753109604120255\n",
      "Steps : 161700, \t Total Gen Loss : 36.604679107666016, \t Total Dis Loss : 0.00014330074191093445\n",
      "Steps : 161800, \t Total Gen Loss : 33.14003372192383, \t Total Dis Loss : 0.0005147032788954675\n",
      "Steps : 161900, \t Total Gen Loss : 33.059722900390625, \t Total Dis Loss : 0.0007232351927086711\n",
      "Steps : 162000, \t Total Gen Loss : 36.78042984008789, \t Total Dis Loss : 0.000630069465842098\n",
      "Time for epoch 24 is 345.07217717170715 sec\n",
      "Steps : 162100, \t Total Gen Loss : 32.24161911010742, \t Total Dis Loss : 0.000495601212605834\n",
      "Steps : 162200, \t Total Gen Loss : 34.17575454711914, \t Total Dis Loss : 0.00012622668873518705\n",
      "Steps : 162300, \t Total Gen Loss : 34.522857666015625, \t Total Dis Loss : 0.05295068398118019\n",
      "Steps : 162400, \t Total Gen Loss : 35.42361831665039, \t Total Dis Loss : 0.0003925002529285848\n",
      "Steps : 162500, \t Total Gen Loss : 35.499305725097656, \t Total Dis Loss : 0.00044293972314335406\n",
      "Steps : 162600, \t Total Gen Loss : 36.45465850830078, \t Total Dis Loss : 0.00022265584266278893\n",
      "Steps : 162700, \t Total Gen Loss : 32.332584381103516, \t Total Dis Loss : 0.0015110122039914131\n",
      "Steps : 162800, \t Total Gen Loss : 32.00870895385742, \t Total Dis Loss : 0.000340390601195395\n",
      "Steps : 162900, \t Total Gen Loss : 34.21979904174805, \t Total Dis Loss : 0.0002010645403061062\n",
      "Steps : 163000, \t Total Gen Loss : 33.62507247924805, \t Total Dis Loss : 0.00021039688726887107\n",
      "Steps : 163100, \t Total Gen Loss : 33.02473068237305, \t Total Dis Loss : 0.0044265748001635075\n",
      "Steps : 163200, \t Total Gen Loss : 36.53398895263672, \t Total Dis Loss : 1.758007419994101e-05\n",
      "Steps : 163300, \t Total Gen Loss : 35.612937927246094, \t Total Dis Loss : 0.0001304734469158575\n",
      "Steps : 163400, \t Total Gen Loss : 34.83155822753906, \t Total Dis Loss : 0.00010215785005129874\n",
      "Steps : 163500, \t Total Gen Loss : 31.64664077758789, \t Total Dis Loss : 0.0005559608107432723\n",
      "Steps : 163600, \t Total Gen Loss : 34.80261993408203, \t Total Dis Loss : 3.7120513297850266e-05\n",
      "Steps : 163700, \t Total Gen Loss : 33.18453598022461, \t Total Dis Loss : 0.00010564172407612205\n",
      "Steps : 163800, \t Total Gen Loss : 30.542593002319336, \t Total Dis Loss : 0.0012542471522465348\n",
      "Steps : 163900, \t Total Gen Loss : 34.31493377685547, \t Total Dis Loss : 0.0005437746876850724\n",
      "Steps : 164000, \t Total Gen Loss : 36.71282196044922, \t Total Dis Loss : 0.00011486589210107923\n",
      "Steps : 164100, \t Total Gen Loss : 36.51581954956055, \t Total Dis Loss : 0.00013071217108517885\n",
      "Steps : 164200, \t Total Gen Loss : 33.892364501953125, \t Total Dis Loss : 0.0003030116204172373\n",
      "Steps : 164300, \t Total Gen Loss : 32.7586555480957, \t Total Dis Loss : 0.002196383895352483\n",
      "Steps : 164400, \t Total Gen Loss : 36.26025390625, \t Total Dis Loss : 0.00018164223001804203\n",
      "Steps : 164500, \t Total Gen Loss : 33.39500045776367, \t Total Dis Loss : 0.00030556099954992533\n",
      "Steps : 164600, \t Total Gen Loss : 35.1429328918457, \t Total Dis Loss : 0.00030381546821445227\n",
      "Steps : 164700, \t Total Gen Loss : 32.18992233276367, \t Total Dis Loss : 0.00013338823919184506\n",
      "Steps : 164800, \t Total Gen Loss : 31.945919036865234, \t Total Dis Loss : 0.007921416312456131\n",
      "Steps : 164900, \t Total Gen Loss : 32.540992736816406, \t Total Dis Loss : 0.563527524471283\n",
      "Steps : 165000, \t Total Gen Loss : 31.679603576660156, \t Total Dis Loss : 0.0016695798840373755\n",
      "Steps : 165100, \t Total Gen Loss : 33.02748489379883, \t Total Dis Loss : 0.00023032951867207885\n",
      "Steps : 165200, \t Total Gen Loss : 34.9237060546875, \t Total Dis Loss : 0.004132293164730072\n",
      "Steps : 165300, \t Total Gen Loss : 36.73131561279297, \t Total Dis Loss : 0.0012604300864040852\n",
      "Steps : 165400, \t Total Gen Loss : 36.34101104736328, \t Total Dis Loss : 0.0013604783453047276\n",
      "Steps : 165500, \t Total Gen Loss : 37.648216247558594, \t Total Dis Loss : 0.0010106012923642993\n",
      "Steps : 165600, \t Total Gen Loss : 31.894147872924805, \t Total Dis Loss : 0.001034404500387609\n",
      "Steps : 165700, \t Total Gen Loss : 35.08686447143555, \t Total Dis Loss : 8.797197369858623e-05\n",
      "Steps : 165800, \t Total Gen Loss : 33.26973342895508, \t Total Dis Loss : 0.0009588354732841253\n",
      "Steps : 165900, \t Total Gen Loss : 36.522560119628906, \t Total Dis Loss : 0.0010015293955802917\n",
      "Steps : 166000, \t Total Gen Loss : 32.5711555480957, \t Total Dis Loss : 0.0002910713665187359\n",
      "Steps : 166100, \t Total Gen Loss : 35.252052307128906, \t Total Dis Loss : 0.00036155793350189924\n",
      "Steps : 166200, \t Total Gen Loss : 34.30642318725586, \t Total Dis Loss : 0.0004966476699337363\n",
      "Steps : 166300, \t Total Gen Loss : 35.3023567199707, \t Total Dis Loss : 0.0002626332570798695\n",
      "Steps : 166400, \t Total Gen Loss : 32.119667053222656, \t Total Dis Loss : 0.0005277435993775725\n",
      "Steps : 166500, \t Total Gen Loss : 35.02582550048828, \t Total Dis Loss : 0.00023431867884937674\n",
      "Steps : 166600, \t Total Gen Loss : 36.81114196777344, \t Total Dis Loss : 8.101556886686012e-05\n",
      "Steps : 166700, \t Total Gen Loss : 32.78307342529297, \t Total Dis Loss : 0.00036971818190068007\n",
      "Steps : 166800, \t Total Gen Loss : 35.72065734863281, \t Total Dis Loss : 0.0003940615861210972\n",
      "Steps : 166900, \t Total Gen Loss : 35.781105041503906, \t Total Dis Loss : 0.0005287165986374021\n",
      "Steps : 167000, \t Total Gen Loss : 34.462547302246094, \t Total Dis Loss : 0.0008473103516735137\n",
      "Steps : 167100, \t Total Gen Loss : 32.315345764160156, \t Total Dis Loss : 0.0009651347645558417\n",
      "Steps : 167200, \t Total Gen Loss : 31.941200256347656, \t Total Dis Loss : 0.00010820994793903083\n",
      "Steps : 167300, \t Total Gen Loss : 30.539363861083984, \t Total Dis Loss : 0.0002723158395383507\n",
      "Steps : 167400, \t Total Gen Loss : 33.5389404296875, \t Total Dis Loss : 0.0004934846656396985\n",
      "Steps : 167500, \t Total Gen Loss : 35.151615142822266, \t Total Dis Loss : 0.00024438221589662135\n",
      "Steps : 167600, \t Total Gen Loss : 37.886837005615234, \t Total Dis Loss : 2.336597208341118e-05\n",
      "Steps : 167700, \t Total Gen Loss : 31.119123458862305, \t Total Dis Loss : 0.0023458872456103563\n",
      "Steps : 167800, \t Total Gen Loss : 37.950164794921875, \t Total Dis Loss : 0.000249934324529022\n",
      "Steps : 167900, \t Total Gen Loss : 33.467254638671875, \t Total Dis Loss : 0.00021080582519061863\n",
      "Steps : 168000, \t Total Gen Loss : 35.3655891418457, \t Total Dis Loss : 4.989139415556565e-05\n",
      "Steps : 168100, \t Total Gen Loss : 36.33835220336914, \t Total Dis Loss : 4.6322114940267056e-05\n",
      "Steps : 168200, \t Total Gen Loss : 33.776302337646484, \t Total Dis Loss : 0.0008996615069918334\n",
      "Steps : 168300, \t Total Gen Loss : 31.878986358642578, \t Total Dis Loss : 0.00022398566943593323\n",
      "Steps : 168400, \t Total Gen Loss : 41.17629623413086, \t Total Dis Loss : 0.0024618017487227917\n",
      "Steps : 168500, \t Total Gen Loss : 36.14726257324219, \t Total Dis Loss : 0.0016385269118472934\n",
      "Steps : 168600, \t Total Gen Loss : 34.01776885986328, \t Total Dis Loss : 0.00014018006913829595\n",
      "Steps : 168700, \t Total Gen Loss : 33.81886672973633, \t Total Dis Loss : 0.00010434899013489485\n",
      "Time for epoch 25 is 344.14678859710693 sec\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 25\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_path)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f97c7fcb1d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(x_batch_train, training=True)\n",
    "        _, feat_real = discriminator(x_batch_train, training=True)\n",
    "        _, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()        \n",
    "\n",
    "        rec = abs(x_batch_train - generated_images)\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "\n",
    "        rec = tf.reduce_sum(rec, [1,2,3])\n",
    "        lat = tf.reduce_sum(lat, [1,2,3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float32)) + ((1 - set_lambda) * tf.cast(lat, tf.float32))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 16000\n"
     ]
    }
   ],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(len(an_scores), len(gt_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000,)\n",
      "(7000,)\n"
     ]
    }
   ],
   "source": [
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "\n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4570909 0.45215932\n",
      "0.15733778 0.15981226\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o')\n",
    "\n",
    "print(np.mean(normal), np.mean(anormaly))\n",
    "print(np.std(normal), np.std(anormaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
